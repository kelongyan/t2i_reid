import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from utils.loss_logger import LossLogger


class SymmetricReconstructionLoss(nn.Module):
    """
    ç®€åŒ–çš„é‡æ„æŸå¤± - ä»…å…³æ³¨ä¿¡æ¯å®Œæ•´æ€§

    æ ¸å¿ƒæ€æƒ³ï¼šF_input â‰ˆ F_id + F_attr
    ç¡®ä¿è§£è€¦åçš„ä¸¤ä¸ªç‰¹å¾èƒ½å¤Ÿé‡å»ºåŸå§‹è¾“å…¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±

    é‡æ„è¯´æ˜ï¼š
    - ç§»é™¤å¤æ‚çš„Cosineã€Diversityã€EnergyæŸå¤±
    - ä»…ä¿ç•™MSEæŸå¤±ï¼Œè®©æ¨¡å‹è‡ªç„¶å­¦ä¹ é‡æ„
    - ä¸orthogonal_lossé…åˆï¼Œé¿å…ç‰¹å¾é‡å 
    """
    def __init__(self):
        super().__init__()
        self.mse_loss = nn.MSELoss()

    def forward(self, id_feat, attr_feat, original_feat):
        """
        Args:
            id_feat: IDç‰¹å¾ [B, dim]
            attr_feat: Attrç‰¹å¾ [B, dim]
            original_feat: åŸå§‹ç‰¹å¾ï¼ˆè§£è€¦å‰çš„å…¨å±€ç‰¹å¾ï¼‰[B, dim]

        Returns:
            loss: é‡æ„æŸå¤±ï¼ˆMSEï¼‰
        """
        # ç®€å•åŠ æ³•é‡å»º
        reconstructed = id_feat + attr_feat  # [B, dim]

        # ä»…ä½¿ç”¨MSEæŸå¤±
        mse_loss = self.mse_loss(reconstructed, original_feat)

        return mse_loss


class EnhancedOrthogonalLoss(nn.Module):
    """
    å¢å¼ºæ­£äº¤æŸå¤± (Enhanced Orthogonal Loss)

    æ”¹è¿›ï¼šå¢åŠ äº¤å‰æ‰¹æ¬¡çº¦æŸï¼Œè®©ä¸åŒæ ·æœ¬çš„IDå’ŒAttrç‰¹å¾ç©ºé—´ä¹Ÿè¶‹å‘æ­£äº¤
    """
    def __init__(self):
        super().__init__()

    def forward(self, id_embeds, attr_embeds, cross_batch=True):
        """
        Args:
            id_embeds: IDç‰¹å¾ [B, dim]
            attr_embeds: Attrç‰¹å¾ [B, dim]
            cross_batch: æ˜¯å¦å¯ç”¨äº¤å‰æ‰¹æ¬¡æ­£äº¤çº¦æŸ

        Returns:
            loss: æ­£äº¤æŸå¤±
        """
        # å½’ä¸€åŒ–
        id_norm = F.normalize(id_embeds, dim=-1, eps=1e-8)     # [B, dim]
        attr_norm = F.normalize(attr_embeds, dim=-1, eps=1e-8) # [B, dim]

        # === æ‰¹æ¬¡å†…æ­£äº¤çº¦æŸï¼ˆæ ·æœ¬è‡ªå·±çš„IDå’ŒAttræ­£äº¤ï¼‰===
        # ä½™å¼¦ç›¸ä¼¼åº¦ï¼šåº”è¯¥æ¥è¿‘0
        intra_cosine = (id_norm * attr_norm).sum(dim=-1)  # [B]
        intra_cosine = torch.clamp(intra_cosine, min=-1.0, max=1.0)
        intra_loss = intra_cosine.pow(2).mean()

        # === äº¤å‰æ‰¹æ¬¡æ­£äº¤çº¦æŸï¼ˆæ‰€æœ‰ID vs æ‰€æœ‰Attrï¼‰===
        if cross_batch and id_embeds.size(0) > 1:
            # è®¡ç®—å…¨å±€ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
            cross_sim = torch.matmul(id_norm, attr_norm.t())
            # æœ€å°åŒ–æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼ˆè®©æ•´ä¸ªçŸ©é˜µè¶‹å‘0ï¼‰
            cross_loss = cross_sim.pow(2).mean()

            return intra_loss + 0.5 * cross_loss
        else:
            return intra_loss


class Loss(nn.Module):
    """
    === FSHDæŸå¤±å‡½æ•°æ¨¡å— (é‡æ„ç‰ˆ - Phase 3: åšå‡æ³•) ===

    æ ¸å¿ƒæ”¹è¿› (åŸºäºæ—¥å¿—è¯Šæ–­)ï¼š
    1. å½»åº•ç§»é™¤é‡æ„æŸå¤± (Reconstruction Loss)ï¼šæ¶ˆé™¤"çº¿æ€§é‡æ„"ä¸"è¯­ä¹‰æµå½¢"çš„æ•°å­¦å†²çªã€‚
    2. é™çº§æ­£äº¤çº¦æŸ (Orthogonal Loss)ï¼šæƒé‡é™è‡³ 0.05ï¼Œé¿å…ç ´åç‰¹å¾çš„å†…åœ¨è¯­ä¹‰è”ç³»ã€‚
    3. ç¼©æ”¾åˆ†ç±»æŸå¤± (CLS Scaling)ï¼šLogits / 20.0ï¼Œè§£å†³åˆ†ç±»æŸå¤±æ•°å€¼è¿‡å¤§(8.0+)ä¸»å¯¼æ¢¯åº¦çš„é—®é¢˜ã€‚
    4. æ¿€æ´»æœè£…è¯­ä¹‰ (Cloth Semantic)ï¼šæƒé‡æå‡è‡³ 0.5ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¹ å±æ€§å¯¹é½ã€‚

    ä¿ç•™çš„5ä¸ªæ ¸å¿ƒæŸå¤±ï¼š
    - InfoNCE (1.0): ä¸»ä»»åŠ¡
    - IdTriplet (1.0): èº«ä»½ä¸€è‡´æ€§
    - ClothSemantic (0.5): å±æ€§å¯¹é½
    - Orthogonal (0.05): å¼±è§£è€¦çº¦æŸ
    - AntiCollapse (1.0): åŸºç¡€æ­£åˆ™
    - Cls (0.05): å¼±åˆ†ç±»è¾…åŠ©
    """

    def __init__(self, temperature=0.1, weights=None, num_classes=None, logger=None):
        super().__init__()
        self.temperature = temperature
        self.num_classes = num_classes
        self.logger = logger

        # Label Smoothing
        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.2)

        # === æ ¸å¿ƒæŸå¤±æ¨¡å— ===
        # ç§»é™¤ SymmetricReconstructionLoss
        self._orthogonal_loss_module = EnhancedOrthogonalLoss()

        # === åˆå§‹åŒ–LossLogger ===
        self.loss_logger = LossLogger(logger.debug_logger) if logger else None

        # === Phase 3 æ¨èæƒé‡é…ç½® ===
        self.weights = weights if weights is not None else {
            # === æ ¸å¿ƒä»»åŠ¡ ===
            'info_nce': 1.0,              # ä¸»ä»»åŠ¡
            'id_triplet': 1.0,            # èº«ä»½ä¸€è‡´æ€§ (å¢å¼º)
            'cloth_semantic': 0.5,        # ğŸ”¥ æ¿€æ´»ï¼šå±æ€§å¯¹é½ (å¤§å¹…æå‡)

            # === çº¦æŸä¸æ­£åˆ™ ===
            'cls': 0.05,                  # ğŸ”¥ é™ä½ï¼šé…åˆLogit Scalingä½¿ç”¨
            'orthogonal': 0.05,           # ğŸ”¥ é™çº§ï¼šå¼±çº¦æŸï¼Œé¿å…ç ´åè¯­ä¹‰
        }

        # åŠ¨æ€æƒé‡è°ƒæ•´å‚æ•°
        self.current_epoch = 0
        self.enable_dynamic_weights = True

        # æ³¨å†Œdummyå‚æ•°ç”¨äºè·å–è®¾å¤‡
        self.register_buffer('_dummy', torch.zeros(1))

        # è°ƒè¯•è®¡æ•°å™¨
        if logger:
            self.debug_logger = logger.debug_logger
            self._batch_counter = 0

    def set_semantic_guidance(self, semantic_guidance_module):
        pass

    def _get_device(self):
        return self._dummy.device

    def update_epoch(self, epoch):
        """
        === Phase 3: ç®€åŒ–çš„ä¸¤é˜¶æ®µç­–ç•¥ ===
        ä¸å†è¿›è¡Œæ¿€è¿›çš„æƒé‡æ³¢åŠ¨ï¼Œä¿æŒç¨³å®šçš„ä¼˜åŒ–ç›®æ ‡ã€‚
        """
        self.current_epoch = epoch

        if not self.enable_dynamic_weights:
            return

        # åŠ¨æ€ç­–ç•¥ä»…å¾®è°ƒï¼Œä¸å†æ”¹å˜ä¸»æ¬¡å…³ç³»
        if epoch <= 5:
            # Warmup: ç¨å¾®é™ä½ cloth_semanticï¼Œè®© ID ç‰¹å¾å…ˆæˆå‹
            self.weights['cloth_semantic'] = 0.2
            self.weights['orthogonal'] = 0.0  # å‰5ä¸ªepochå®Œå…¨å…³é—­æ­£äº¤ï¼Œå…ˆå­¦ç‰¹å¾
        else:
            # Full Regime
            self.weights['cloth_semantic'] = 0.5
            self.weights['orthogonal'] = 0.05 # å¼€å¯å¼±æ­£äº¤

        # è®°å½•æƒé‡å˜åŒ–
        if self.logger and epoch in [1, 6]:
            self.debug_logger.info(f"ğŸ”¥ Loss weights updated at epoch {epoch}:")
            for k, v in self.weights.items():
                if v > 0:
                    self.debug_logger.info(f"   - {k}: {v:.4f}")

    def info_nce_loss(self, image_embeds, text_embeds, fused_embeds=None):
        """InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤±"""
        if image_embeds is None or text_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        visual_embeds = fused_embeds if fused_embeds is not None else image_embeds

        bsz = visual_embeds.size(0)
        visual_embeds = F.normalize(visual_embeds, dim=-1, eps=1e-8)
        text_embeds = F.normalize(text_embeds, dim=-1, eps=1e-8)

        sim = torch.matmul(visual_embeds, text_embeds.t()) / self.temperature
        sim = torch.clamp(sim, min=-50, max=50)

        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)

        total_loss = (loss_i2t + loss_t2i) / 2

        if self.logger and self.loss_logger and self.loss_logger.should_log('info_nce'):
            self.loss_logger.log_info_nce_stats(visual_embeds, text_embeds, total_loss, self.temperature)

        return total_loss

    def id_classification_loss(self, id_logits, pids):
        """
        èº«ä»½åˆ†ç±»æŸå¤± (Fixed: Remove Logit Scaling)
        
        Logits / 20.0 å¯¼è‡´ Softmax åˆ†å¸ƒå¹³å¦ï¼ŒCE Loss ç»´æŒåœ¨ ln(C) ~ 8.2ã€‚
        ç§»é™¤ç¼©æ”¾ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ å°–å³°åˆ†å¸ƒä»¥é™ä½æŸå¤±ã€‚
        """
        if id_logits is None or pids is None:
            return torch.tensor(0.0, device=self._get_device())

        # ç§»é™¤æ‰‹åŠ¨ç¼©æ”¾ï¼Œä»…ä¿ç•™æ•°å€¼ç¨³å®šæ€§çš„è£å‰ª
        scaled_logits = torch.clamp(id_logits, min=-50, max=50)

        ce_loss = self.ce_loss(scaled_logits, pids)

        if self.logger and self.loss_logger and self.loss_logger.should_log('cls'):
            self.loss_logger.log_cls_stats(scaled_logits, pids, ce_loss)

        return ce_loss

    def cloth_semantic_loss(self, cloth_image_embeds, cloth_text_embeds):
        """æœè£…è¯­ä¹‰æŸå¤±"""
        if cloth_image_embeds is None or cloth_text_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        bsz = cloth_image_embeds.size(0)
        cloth_image_norm = F.normalize(cloth_image_embeds, dim=-1, eps=1e-8)
        cloth_text_norm = F.normalize(cloth_text_embeds, dim=-1, eps=1e-8)

        sim = torch.matmul(cloth_image_norm, cloth_text_norm.t()) / self.temperature
        sim = torch.clamp(sim, min=-50, max=50)

        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)

        total_loss = (loss_i2t + loss_t2i) / 2

        if self.logger and self.loss_logger and self.loss_logger.should_log('cloth_semantic'):
            self.loss_logger.log_cloth_semantic_stats(cloth_image_norm, cloth_text_norm, total_loss, self.temperature)

        return total_loss

    def orthogonal_loss(self, id_embeds, cloth_embeds):
        """æ­£äº¤çº¦æŸæŸå¤±"""
        if id_embeds is None or cloth_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        ortho_loss = self._orthogonal_loss_module(id_embeds, cloth_embeds, cross_batch=True)

        if self.logger and self.loss_logger and self.loss_logger.should_log('orthogonal'):
            self.loss_logger.log_orthogonality_stats(id_embeds, cloth_embeds, ortho_loss)

        return ortho_loss

    def triplet_loss(self, embeds, pids, margin=0.3):
        """ID ä¸€è‡´æ€§ Triplet Loss"""
        if embeds is None or pids is None:
            return torch.tensor(0.0, device=self._get_device())

        n = embeds.size(0)
        dist = torch.pow(embeds, 2).sum(dim=1, keepdim=True).expand(n, n)
        dist = dist + dist.t()
        dist.addmm_(embeds, embeds.t(), beta=1, alpha=-2)
        dist = dist.clamp(min=1e-12).sqrt()

        mask = pids.expand(n, n).eq(pids.expand(n, n).t())
        dist_ap, _ = torch.max(dist * mask.float(), dim=1)
        dist_an, _ = torch.min(dist * (1. - mask.float()) + 1e6 * mask.float(), dim=1)

        loss = F.relu(dist_ap - dist_an + margin).mean()

        if self.logger and self.loss_logger and self.loss_logger.should_log('id_triplet'):
            self.loss_logger.log_triplet_stats(embeds, pids, loss, margin)

        return loss

    def forward(self, image_embeds, id_text_embeds, fused_embeds, id_logits, id_embeds,
                cloth_embeds, cloth_text_embeds, cloth_image_embeds, pids,
                is_matched=None, epoch=None, gate=None,
                id_seq_features=None, cloth_seq_features=None, saliency_score=None,
                id_cls_features=None, original_feat=None, freq_info=None):
        """å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰æŸå¤± (Phase 3)"""
        losses = {}

        if epoch is not None:
            self.update_epoch(epoch)

        # 1. InfoNCE (ä¸»ä»»åŠ¡)
        losses['info_nce'] = self.info_nce_loss(image_embeds, id_text_embeds, fused_embeds)

        # 2. Classification (ç¼©æ”¾å)
        losses['cls'] = self.id_classification_loss(id_logits, pids)

        # 3. Cloth Semantic (æ¿€æ´»)
        losses['cloth_semantic'] = self.cloth_semantic_loss(cloth_image_embeds, cloth_text_embeds)

        # 4. Orthogonal (å¼±çº¦æŸ)
        losses['orthogonal'] = self.orthogonal_loss(id_embeds, cloth_embeds)

        # 5. Triplet (IDä¸€è‡´æ€§)
        losses['id_triplet'] = self.triplet_loss(id_embeds, pids)

        # === å…¼å®¹æ€§å ä½ç¬¦ (å·²åˆ é™¤çš„æŸå¤±è¿”å›0ï¼Œä½†ä¸åŠ å…¥total) ===
        losses['gate_adaptive'] = torch.tensor(0.0, device=self._get_device())
        losses['semantic_alignment'] = torch.tensor(0.0, device=self._get_device())
        losses['freq_consistency'] = torch.tensor(0.0, device=self._get_device())
        losses['freq_separation'] = torch.tensor(0.0, device=self._get_device())
        losses['anti_collapse'] = torch.tensor(0.0, device=self._get_device())
        losses['reconstruction'] = torch.tensor(0.0, device=self._get_device())

        # === NaNæ£€æµ‹ä¸æ±‚å’Œ ===
        total_loss = torch.tensor(0.0, device=self._get_device())
        for key, value in losses.items():
            if key == 'total': continue
            if torch.isnan(value).any() or torch.isinf(value).any():
                if self.logger:
                    self.debug_logger.warning(f"âš ï¸  Loss '{key}' is NaN/Inf! Resetting to 0.")
                losses[key] = torch.tensor(0.0, device=self._get_device(), requires_grad=True)
            
            weight = self.weights.get(key, 0.0)
            if weight > 0:
                total_loss += weight * losses[key]

        losses['total'] = total_loss

        # æ—¥å¿—è®°å½•
        if self.logger and self.loss_logger:
            self._batch_counter += 1
            if self._batch_counter % 100 == 0:
                self.loss_logger.log_weighted_loss_summary(losses, self.weights)

        return losses
