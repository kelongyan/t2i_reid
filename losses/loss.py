import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from utils.loss_logger import LossLogger


class SymmetricReconstructionLoss(nn.Module):
    """
    ç®€åŒ–çš„é‡æž„æŸå¤± - ä»…å…³æ³¨ä¿¡æ¯å®Œæ•´æ€§

    æ ¸å¿ƒæ€æƒ³ï¼šF_input â‰ˆ F_id + F_attr
    ç¡®ä¿è§£è€¦åŽçš„ä¸¤ä¸ªç‰¹å¾èƒ½å¤Ÿé‡å»ºåŽŸå§‹è¾“å…¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±

    é‡æž„è¯´æ˜Žï¼š
    - ç§»é™¤å¤æ‚çš„Cosineã€Diversityã€EnergyæŸå¤±
    - ä»…ä¿ç•™MSEæŸå¤±ï¼Œè®©æ¨¡åž‹è‡ªç„¶å­¦ä¹ é‡æž„
    - ä¸Žorthogonal_lossé…åˆï¼Œé¿å…ç‰¹å¾é‡å 
    """
    def __init__(self):
        super().__init__()
        self.mse_loss = nn.MSELoss()

    def forward(self, id_feat, attr_feat, original_feat):
        """
        Args:
            id_feat: IDç‰¹å¾ [B, dim]
            attr_feat: Attrç‰¹å¾ [B, dim]
            original_feat: åŽŸå§‹ç‰¹å¾ï¼ˆè§£è€¦å‰çš„å…¨å±€ç‰¹å¾ï¼‰[B, dim]

        Returns:
            loss: é‡æž„æŸå¤±ï¼ˆMSEï¼‰
        """
        # ç®€å•åŠ æ³•é‡å»º
        reconstructed = id_feat + attr_feat  # [B, dim]

        # ä»…ä½¿ç”¨MSEæŸå¤±
        mse_loss = self.mse_loss(reconstructed, original_feat)

        return mse_loss


class EnhancedOrthogonalLoss(nn.Module):
    """
    å¢žå¼ºæ­£äº¤æŸå¤± (Enhanced Orthogonal Loss)

    æ”¹è¿›ï¼šå¢žåŠ äº¤å‰æ‰¹æ¬¡çº¦æŸï¼Œè®©ä¸åŒæ ·æœ¬çš„IDå’ŒAttrç‰¹å¾ç©ºé—´ä¹Ÿè¶‹å‘æ­£äº¤
    """
    def __init__(self):
        super().__init__()

    def forward(self, id_embeds, attr_embeds, cross_batch=True):
        """
        Args:
            id_embeds: IDç‰¹å¾ [B, dim]
            attr_embeds: Attrç‰¹å¾ [B, dim]
            cross_batch: æ˜¯å¦å¯ç”¨äº¤å‰æ‰¹æ¬¡æ­£äº¤çº¦æŸ

        Returns:
            loss: æ­£äº¤æŸå¤±
        """
        # å½’ä¸€åŒ–
        id_norm = F.normalize(id_embeds, dim=-1, eps=1e-8)     # [B, dim]
        attr_norm = F.normalize(attr_embeds, dim=-1, eps=1e-8) # [B, dim]

        # === æ‰¹æ¬¡å†…æ­£äº¤çº¦æŸï¼ˆæ ·æœ¬è‡ªå·±çš„IDå’ŒAttræ­£äº¤ï¼‰===
        # ä½™å¼¦ç›¸ä¼¼åº¦ï¼šåº”è¯¥æŽ¥è¿‘0
        intra_cosine = (id_norm * attr_norm).sum(dim=-1)  # [B]
        intra_cosine = torch.clamp(intra_cosine, min=-1.0, max=1.0)
        intra_loss = intra_cosine.pow(2).mean()

        # === äº¤å‰æ‰¹æ¬¡æ­£äº¤çº¦æŸï¼ˆæ‰€æœ‰ID vs æ‰€æœ‰Attrï¼‰===
        if cross_batch and id_embeds.size(0) > 1:
            # è®¡ç®—å…¨å±€ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
            cross_sim = torch.matmul(id_norm, attr_norm.t())
            # æœ€å°åŒ–æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼ˆè®©æ•´ä¸ªçŸ©é˜µè¶‹å‘0ï¼‰
            cross_loss = cross_sim.pow(2).mean()

            return intra_loss + 0.5 * cross_loss
        else:
            return intra_loss


class FrequencyAlignmentLoss(nn.Module):
    """
    é¢‘åŸŸå¯¹é½æŸå¤± (Frequency Alignment Loss) - æ–¹æ¡ˆB

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. IDç‰¹å¾åº”è¯¥ä¸Žä½Žé¢‘æˆåˆ†é«˜åº¦ç›¸å…³
    2. Attrç‰¹å¾åº”è¯¥ä¸Žé«˜é¢‘æˆåˆ†é«˜åº¦ç›¸å…³
    3. é¿å…é¢‘åŸŸæ··å å¯¼è‡´çš„èº«ä»½ä¿¡æ¯æ³„æ¼

    è®¾è®¡ç†å¿µï¼š
    - å……åˆ†åˆ©ç”¨FSHDæ¨¡å—çš„é¢‘åŸŸåˆ†è§£èƒ½åŠ›
    - å¼ºåŒ–é¢‘åŸŸ-ç©ºåŸŸè”åˆå»ºæ¨¡çš„æœ‰æ•ˆæ€§
    - é˜²æ­¢IDå’ŒAttrç‰¹å¾åœ¨é¢‘åŸŸä¸Šæ··å 
    - ä¸Žæ£€ç´¢ä»»åŠ¡å®Œå…¨ä¸€è‡´ï¼Œä½¿ç”¨L2å½’ä¸€åŒ–ç‰¹å¾
    """
    def __init__(self):
        super().__init__()
    
    def forward(self, id_feat, attr_feat, freq_info):
        """
        Args:
            id_feat: [B, dim] - IDç‰¹å¾
            attr_feat: [B, dim] - Attrç‰¹å¾
            freq_info: dict - åŒ…å«é¢‘åŸŸä¿¡æ¯ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
                - low_freq_energy: [B] - ä½Žé¢‘èƒ½é‡ï¼ˆå¯é€‰ï¼‰
                - high_freq_energy: [B] - é«˜é¢‘èƒ½é‡ï¼ˆå¯é€‰ï¼‰
                - energy_ratio: [B] - é«˜é¢‘èƒ½é‡æ¯”çŽ‡ï¼ˆhigh_freq / totalï¼‰
                - freq_magnitude: [B] - DCTç³»æ•°å¹…åº¦
                - freq_coeff: [B, D, H, W] - DCTç³»æ•°å¼ é‡ï¼ˆç”¨äºŽé«˜çº§åˆ†æžï¼‰

        Returns:
            loss: é¢‘åŸŸå¯¹é½æŸå¤±
        """
        B = id_feat.shape[0]
        device = id_feat.device
        
        # === ðŸ”¥ ä¿®å¤ï¼šå¦‚æžœfreq_infoä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼ ===
        if freq_info is None:
            freq_info = {}  # åˆ›å»ºç©ºå­—å…¸ï¼ŒåŽç»­ä¼šä½¿ç”¨é»˜è®¤å€¼
        
        # å½’ä¸€åŒ–ç‰¹å¾ï¼ˆä½¿ç”¨L2å½’ä¸€åŒ–ï¼Œä¸Žæ£€ç´¢ä»»åŠ¡ä¸€è‡´ï¼‰
        id_norm = F.normalize(id_feat, dim=-1, eps=1e-8)     # [B, dim]
        attr_norm = F.normalize(attr_feat, dim=-1, eps=1e-8) # [B, dim]
        
        # æå–é¢‘åŸŸä¿¡æ¯ï¼ˆæä¾›é»˜è®¤å€¼ä»¥é˜²ç¼ºå¤±ï¼‰
        energy_ratio = freq_info.get('energy_ratio', torch.ones(B, device=device) * 0.5)
        freq_magnitude = freq_info.get('freq_magnitude', torch.ones(B, device=device))
        freq_coeff = freq_info.get('freq_coeff', None)
        
        # ===== æŸå¤±1ï¼šç‰¹å¾èƒ½é‡ä¸€è‡´æ€§ =====
        # IDç‰¹å¾åº”è¯¥æœ‰æ›´å¤§çš„èƒ½é‡ï¼ˆèº«ä»½ä¿¡æ¯æ›´ä¸°å¯Œï¼‰
        id_energy = torch.sum(id_norm.pow(2), dim=-1)  # [B]
        attr_energy = torch.sum(attr_norm.pow(2), dim=-1)  # [B]
        
        # IDèƒ½é‡åº”è¯¥å¤§äºŽAttrèƒ½é‡
        energy_gap_loss = F.relu(attr_energy - id_energy).mean() * 0.3
        
        # ===== æŸå¤±2ï¼šé¢‘åŸŸèƒ½é‡ç›¸å…³æ€§ =====
        # IDç‰¹å¾åº”è¯¥ä¸»å¯¼ä½Žé¢‘ï¼ŒAttrç‰¹å¾åº”è¯¥ä¸»å¯¼é«˜é¢‘
        # ä½¿ç”¨energy_ratioä½œä¸ºæŒ‡å¯¼ï¼šä½Žratioè¡¨ç¤ºæ›´å¤šä½Žé¢‘ï¼Œé«˜ratioè¡¨ç¤ºæ›´å¤šé«˜é¢‘
        
        # IDç‰¹å¾åº”è¯¥ä¸Žä½Žé¢‘ä¸»å¯¼çš„æ ·æœ¬ï¼ˆenergy_ratioä½Žï¼‰æœ‰æ›´é«˜çš„èƒ½é‡
        id_energy_weighted = id_energy * (1.0 - energy_ratio)
        # Attrç‰¹å¾åº”è¯¥ä¸Žé«˜é¢‘ä¸»å¯¼çš„æ ·æœ¬ï¼ˆenergy_ratioé«˜ï¼‰æœ‰æ›´é«˜çš„èƒ½é‡
        attr_energy_weighted = attr_energy * energy_ratio
        
        # æœ€å¤§åŒ–åŠ æƒèƒ½é‡
        freq_energy_loss = (1.0 - id_energy_weighted.mean()).abs() * 0.5
        freq_energy_loss += (1.0 - attr_energy_weighted.mean()).abs() * 0.5
        
        # ===== æŸå¤±3ï¼šé¢‘åŸŸç»“æž„ä¿æŒ =====
        # å¦‚æžœæä¾›äº†DCTç³»æ•°ï¼Œè®¡ç®—é¢‘åŸŸç»“æž„çš„ç›¸ä¼¼æ€§
        freq_structure_loss = torch.tensor(0.0, device=device)
        if freq_coeff is not None:
            # freq_coeff: [B, D, H, W]
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é¢‘åŸŸç»“æž„ï¼ˆæ²¿é€šé“ç»´åº¦çš„æ–¹å·®ï¼‰
            freq_structure = torch.var(freq_coeff, dim=1).mean(dim=(1, 2))  # [B]
            
            # IDç‰¹å¾åº”è¯¥ä¸Žæ›´å¹³æ»‘çš„é¢‘åŸŸç»“æž„ç›¸å…³ï¼ˆä½Žé¢‘ä¸»å¯¼ï¼‰
            # Attrç‰¹å¾åº”è¯¥ä¸Žæ›´å¤æ‚çš„é¢‘åŸŸç»“æž„ç›¸å…³ï¼ˆé«˜é¢‘ä¸»å¯¼ï¼‰
            
            # ä½¿ç”¨é¢‘åŸŸå¹…åº¦ä½œä¸ºå‚è€ƒ
            freq_structure_loss = (freq_magnitude - freq_structure).abs().mean() * 0.2
        
        # ===== æŸå¤±4ï¼šID-Attré¢‘åŸŸåˆ†ç¦» =====
        # IDå’ŒAttrç‰¹å¾åœ¨é¢‘åŸŸä¸Šçš„æŠ•å½±åº”è¯¥æ­£äº¤
        # è®¡ç®—IDå’ŒAttrç‰¹å¾ä¸Ž"è™šæ‹Ÿ"ä½Žé¢‘/é«˜é¢‘å‘é‡çš„ç›¸ä¼¼åº¦
        
        # è™šæ‹Ÿä½Žé¢‘å‘é‡ï¼šå‡è®¾ä½Žé¢‘çš„ç‰¹å¾ï¼ˆå¹³æ»‘ã€ç¨³å®šï¼‰
        # æˆ‘ä»¬ä½¿ç”¨ç‰¹å¾å‡å€¼ä½œä¸º"å…¨å±€ä½Žé¢‘"çš„ä»£ç†
        id_mean = id_norm.mean(dim=0, keepdim=True)  # [1, dim]
        attr_mean = attr_norm.mean(dim=0, keepdim=True)  # [1, dim]
        
        # IDç‰¹å¾åº”è¯¥æ›´æŽ¥è¿‘å…¨å±€IDç‰¹å¾ï¼ˆè¡¨ç¤ºèº«ä»½çš„ä¸€è‡´æ€§ï¼‰
        id_consistency = (id_norm * id_mean).sum(dim=-1)  # [B]
        id_consistency_loss = (1.0 - id_consistency).mean() * 0.4
        
        # Attrç‰¹å¾åº”è¯¥æ›´æŽ¥è¿‘å…¨å±€Attrç‰¹å¾ï¼ˆè¡¨ç¤ºå±žæ€§çš„ä¸€è‡´æ€§ï¼‰
        attr_consistency = (attr_norm * attr_mean).sum(dim=-1)  # [B]
        attr_consistency_loss = (1.0 - attr_consistency).mean() * 0.4
        
        # ===== æŸå¤±5ï¼šæ¢¯åº¦åˆ†ç¦»ï¼ˆé˜²æ­¢æ··å ï¼‰=====
        # IDå’ŒAttrç‰¹å¾çš„æ¢¯åº¦æ–¹å‘åº”è¯¥ä¸åŒ
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒå·®å¼‚æ¥è¿‘ä¼¼
        
        # IDç‰¹å¾ï¼šåº”è¯¥æ›´åŠ é›†ä¸­ï¼ˆèº«ä»½æ˜Žç¡®ï¼‰
        id_variance = id_norm.var(dim=1)  # [B]
        
        # Attrç‰¹å¾ï¼šå¯ä»¥æ›´åŠ åˆ†æ•£ï¼ˆå±žæ€§å¤šæ ·ï¼‰
        attr_variance = attr_norm.var(dim=1)  # [B]
        
        # IDæ–¹å·®åº”è¯¥ç›¸å¯¹è¾ƒå°ï¼ŒAttræ–¹å·®å¯ä»¥ç›¸å¯¹è¾ƒå¤§
        variance_gap_loss = F.relu(id_variance - attr_variance).mean() * 0.2
        
        # ===== æ€»æŸå¤± =====
        total_loss = (
            energy_gap_loss +           # èƒ½é‡ä¸€è‡´æ€§
            freq_energy_loss +          # é¢‘åŸŸèƒ½é‡ç›¸å…³æ€§
            freq_structure_loss +       # é¢‘åŸŸç»“æž„ä¿æŒ
            id_consistency_loss +       # IDä¸€è‡´æ€§
            attr_consistency_loss +     # Atträ¸€è‡´æ€§
            variance_gap_loss           # æ¢¯åº¦åˆ†ç¦»
        )
        
        # NaNæ£€æµ‹
        if torch.isnan(total_loss).any():
            total_loss = torch.tensor(0.0, device=device)
        
        return total_loss


class Loss(nn.Module):
    """
    === FSHDæŸå¤±å‡½æ•°æ¨¡å— (æ–¹æ¡ˆBï¼šé¢‘åŸŸå¯¹é½æŸå¤±ç‰ˆ) ===

    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ç§»é™¤CLSæŸå¤±ï¼šè§£å†³CLSæŸå¤±æ— æ³•ä¸‹é™çš„é—®é¢˜
    2. æ–°å¢žé¢‘åŸŸå¯¹é½æŸå¤±ï¼šå……åˆ†åˆ©ç”¨FSHDæž¶æž„çš„é¢‘åŸŸåˆ†è§£èƒ½åŠ›
    3. å¼ºåŒ–èº«ä»½ä¸€è‡´æ€§ï¼šæå‡TripletæŸå¤±æƒé‡
    4. ç®€åŒ–æƒé‡ç­–ç•¥ï¼šç§»é™¤åŠ¨æ€æƒé‡è°ƒæ•´ï¼Œä½¿ç”¨å›ºå®šæƒé‡

    ä¿ç•™çš„5ä¸ªæ ¸å¿ƒæŸå¤±ï¼š
    - InfoNCE (1.0): ä¸»ä»»åŠ¡
    - IdTriplet (1.0): èº«ä»½ä¸€è‡´æ€§
    - ClothSemantic (0.5): å±žæ€§å¯¹é½
    - Orthogonal (0.05): å¼±è§£è€¦çº¦æŸ
    - FrequencyAlignment (0.3): é¢‘åŸŸå¯¹é½ï¼ˆæ–°å¢žï¼‰
    """

    def __init__(self, temperature=0.1, weights=None, num_classes=None, logger=None):
        super().__init__()
        self.temperature = temperature
        self.num_classes = num_classes
        self.logger = logger

        # Label Smoothing
        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.2)

        # === æ ¸å¿ƒæŸå¤±æ¨¡å— ===
        self._orthogonal_loss_module = EnhancedOrthogonalLoss()
        self._frequency_alignment_module = FrequencyAlignmentLoss()

        # === åˆå§‹åŒ–LossLogger ===
        self.loss_logger = LossLogger(logger.debug_logger) if logger else None

        # === æ–¹æ¡ˆBæŽ¨èæƒé‡é…ç½®ï¼ˆå›ºå®šæƒé‡ï¼Œæ— åŠ¨æ€è°ƒæ•´ï¼‰===
        self.weights = weights if weights is not None else {
            # === æ ¸å¿ƒä»»åŠ¡ ===
            'info_nce': 1.0,               # ä¸»ä»»åŠ¡
            'id_triplet': 1.0,             # èº«ä»½ä¸€è‡´æ€§
            'cloth_semantic': 0.5,         # å±žæ€§å¯¹é½

            # === çº¦æŸä¸Žæ­£åˆ™ ===
            'orthogonal': 0.05,            # å¼±è§£è€¦çº¦æŸ
            'frequency_alignment': 0.3,     # é¢‘åŸŸå¯¹é½ï¼ˆæ–°å¢žï¼Œæ›¿ä»£CLSï¼‰
        }

        # ç§»é™¤åŠ¨æ€æƒé‡è°ƒæ•´ï¼ˆæ–¹æ¡ˆBä½¿ç”¨å›ºå®šæƒé‡ï¼‰
        self.current_epoch = 0
        self.enable_dynamic_weights = False

        # æ³¨å†Œdummyå‚æ•°ç”¨äºŽèŽ·å–è®¾å¤‡
        self.register_buffer('_dummy', torch.zeros(1))

        # è°ƒè¯•è®¡æ•°å™¨
        if logger:
            self.debug_logger = logger.debug_logger
            self._batch_counter = 0

    def set_semantic_guidance(self, semantic_guidance_module):
        pass

    def _get_device(self):
        return self._dummy.device

    def update_epoch(self, epoch):
        """
        æ–¹æ¡ˆBï¼šç§»é™¤åŠ¨æ€æƒé‡è°ƒæ•´ç­–ç•¥

        åŽŸå› ï¼š
        - é¢‘åŸŸå¯¹é½æŸå¤±ä¸Žæ£€ç´¢ä»»åŠ¡ä¸€è‡´ï¼Œæ— éœ€ç‰¹æ®Šè°ƒæ•´
        - å›ºå®šæƒé‡æ›´ç¨³å®šï¼Œä¾¿äºŽè°ƒè¯•å’Œå¯¹æ¯”
        - ç®€åŒ–è®­ç»ƒé€»è¾‘ï¼Œå‡å°‘è¶…å‚æœç´¢ç©ºé—´
        """
        self.current_epoch = epoch
        
        # è®°å½•å½“å‰æƒé‡ï¼ˆç”¨äºŽç›‘æŽ§ï¼‰
        if self.logger and epoch % 10 == 1:
            self.debug_logger.info(f"ðŸ“Š Fixed loss weights at epoch {epoch}:")
            for k, v in sorted(self.weights.items(), key=lambda x: -x[1]):
                if v > 0:
                    self.debug_logger.info(f"   - {k}: {v:.4f}")

    def info_nce_loss(self, image_embeds, text_embeds, fused_embeds=None):
        """InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤±"""
        if image_embeds is None or text_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        visual_embeds = fused_embeds if fused_embeds is not None else image_embeds

        bsz = visual_embeds.size(0)
        visual_embeds = F.normalize(visual_embeds, dim=-1, eps=1e-8)
        text_embeds = F.normalize(text_embeds, dim=-1, eps=1e-8)

        sim = torch.matmul(visual_embeds, text_embeds.t()) / self.temperature
        sim = torch.clamp(sim, min=-50, max=50)

        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)

        total_loss = (loss_i2t + loss_t2i) / 2

        if self.logger and self.loss_logger and self.loss_logger.should_log('info_nce'):
            self.loss_logger.log_info_nce_stats(visual_embeds, text_embeds, total_loss, self.temperature)

        return total_loss

    def cloth_semantic_loss(self, cloth_image_embeds, cloth_text_embeds):
        """æœè£…è¯­ä¹‰æŸå¤±"""
        if cloth_image_embeds is None or cloth_text_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        bsz = cloth_image_embeds.size(0)
        cloth_image_norm = F.normalize(cloth_image_embeds, dim=-1, eps=1e-8)
        cloth_text_norm = F.normalize(cloth_text_embeds, dim=-1, eps=1e-8)

        sim = torch.matmul(cloth_image_norm, cloth_text_norm.t()) / self.temperature
        sim = torch.clamp(sim, min=-50, max=50)

        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)

        total_loss = (loss_i2t + loss_t2i) / 2

        if self.logger and self.loss_logger and self.loss_logger.should_log('cloth_semantic'):
            self.loss_logger.log_cloth_semantic_stats(cloth_image_norm, cloth_text_norm, total_loss, self.temperature)

        return total_loss

    def orthogonal_loss(self, id_embeds, cloth_embeds):
        """æ­£äº¤çº¦æŸæŸå¤±"""
        if id_embeds is None or cloth_embeds is None:
            return torch.tensor(0.0, device=self._get_device())

        ortho_loss = self._orthogonal_loss_module(id_embeds, cloth_embeds, cross_batch=True)

        if self.logger and self.loss_logger and self.loss_logger.should_log('orthogonal'):
            self.loss_logger.log_orthogonality_stats(id_embeds, cloth_embeds, ortho_loss)

        return ortho_loss

    def triplet_loss(self, embeds, pids, margin=0.3):
        """ID ä¸€è‡´æ€§ Triplet Loss"""
        if embeds is None or pids is None:
            return torch.tensor(0.0, device=self._get_device())

        n = embeds.size(0)
        dist = torch.pow(embeds, 2).sum(dim=1, keepdim=True).expand(n, n)
        dist = dist + dist.t()
        dist.addmm_(embeds, embeds.t(), beta=1, alpha=-2)
        dist = dist.clamp(min=1e-12).sqrt()

        mask = pids.expand(n, n).eq(pids.expand(n, n).t())
        dist_ap, _ = torch.max(dist * mask.float(), dim=1)
        dist_an, _ = torch.min(dist * (1. - mask.float()) + 1e6 * mask.float(), dim=1)

        loss = F.relu(dist_ap - dist_an + margin).mean()

        if self.logger and self.loss_logger and self.loss_logger.should_log('id_triplet'):
            self.loss_logger.log_triplet_stats(embeds, pids, loss, margin)

        return loss

    def frequency_alignment_loss(self, id_feat, attr_feat, freq_info):
        """
        é¢‘åŸŸå¯¹é½æŸå¤±ï¼ˆæ–°å¢žï¼‰

        Args:
            id_feat: [B, dim] - IDç‰¹å¾
            attr_feat: [B, dim] - Attrç‰¹å¾
            freq_info: dict - é¢‘åŸŸä¿¡æ¯ï¼ˆå¯èƒ½ä¸ºNoneï¼‰

        Returns:
            loss: é¢‘åŸŸå¯¹é½æŸå¤±
        """
        if id_feat is None or attr_feat is None:
            return torch.tensor(0.0, device=self._get_device())
        
        # === ðŸ”¥ ä¿®å¤ï¼šfreq_infoå¯èƒ½ä¸ºNone ===
        if freq_info is None:
            # å¦‚æžœæ²¡æœ‰é¢‘åŸŸä¿¡æ¯ï¼Œè¿”å›ž0æŸå¤±æˆ–ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            if self.logger and self._batch_counter % 200 == 0:
                self.debug_logger.warning("âš ï¸  freq_info is None, frequency_alignment_loss disabled for this batch")
            return torch.tensor(0.0, device=self._get_device(), requires_grad=True)

        loss = self._frequency_alignment_module(id_feat, attr_feat, freq_info)

        if self.logger and self._batch_counter % 200 == 0:
            # è®°å½•é¢‘åŸŸå¯¹é½æŸå¤±çš„ç»Ÿè®¡ä¿¡æ¯
            if freq_info is not None:
                energy_ratio = freq_info.get('energy_ratio', torch.tensor([0.5]))
                freq_magnitude = freq_info.get('freq_magnitude', torch.tensor([0.0]))
                self.debug_logger.debug(
                    f"Frequency Alignment Loss: {loss.item():.6f}, "
                    f"energy_ratio={energy_ratio.mean().item():.4f}, "
                    f"freq_magnitude={freq_magnitude.mean().item():.4f}"
                )

        return loss

    def forward(self, image_embeds, id_text_embeds, fused_embeds, id_logits, id_embeds,
                cloth_embeds, cloth_text_embeds, cloth_image_embeds, pids,
                is_matched=None, epoch=None, gate=None,
                id_seq_features=None, cloth_seq_features=None, saliency_score=None,
                id_cls_features=None, original_feat=None, freq_info=None):
        """
        å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰æŸå¤± (æ–¹æ¡ˆBï¼šé¢‘åŸŸå¯¹é½ç‰ˆ)

        æ³¨æ„ï¼šä¿ç•™äº†id_logitså’Œid_cls_featureså‚æ•°ä»¥ä¿æŒå‘åŽå…¼å®¹ï¼Œ
        ä½†ä¸ä½¿ç”¨è¿™äº›å‚æ•°è¿›è¡ŒæŸå¤±è®¡ç®—ã€‚
        """
        losses = {}

        if epoch is not None:
            self.update_epoch(epoch)

        # 1. InfoNCE (ä¸»ä»»åŠ¡)
        losses['info_nce'] = self.info_nce_loss(image_embeds, id_text_embeds, fused_embeds)

        # 2. Cloth Semantic (å±žæ€§å¯¹é½)
        losses['cloth_semantic'] = self.cloth_semantic_loss(cloth_image_embeds, cloth_text_embeds)

        # 3. Orthogonal (å¼±çº¦æŸ)
        losses['orthogonal'] = self.orthogonal_loss(id_embeds, cloth_embeds)

        # 4. Triplet (IDä¸€è‡´æ€§)
        losses['id_triplet'] = self.triplet_loss(id_embeds, pids)

        # 5. Frequency Alignment (é¢‘åŸŸå¯¹é½ï¼Œæ–°å¢žï¼Œæ›¿ä»£CLS)
        losses['frequency_alignment'] = self.frequency_alignment_loss(
            id_embeds, cloth_embeds, freq_info
        )

        # === å…¼å®¹æ€§å ä½ç¬¦ (å·²åˆ é™¤çš„æŸå¤±è¿”å›ž0ï¼Œä½†ä¸åŠ å…¥total) ===
        losses['gate_adaptive'] = torch.tensor(0.0, device=self._get_device())
        losses['semantic_alignment'] = torch.tensor(0.0, device=self._get_device())
        losses['freq_consistency'] = torch.tensor(0.0, device=self._get_device())
        losses['freq_separation'] = torch.tensor(0.0, device=self._get_device())
        losses['anti_collapse'] = torch.tensor(0.0, device=self._get_device())
        losses['reconstruction'] = torch.tensor(0.0, device=self._get_device())
        losses['cls'] = torch.tensor(0.0, device=self._get_device())  # CLSæŸå¤±å·²åºŸå¼ƒ

        # === NaNæ£€æµ‹ä¸Žæ±‚å’Œ ===
        total_loss = torch.tensor(0.0, device=self._get_device())
        for key, value in losses.items():
            if key == 'total': continue
            if torch.isnan(value).any() or torch.isinf(value).any():
                if self.logger:
                    self.debug_logger.warning(f"âš ï¸  Loss '{key}' is NaN/Inf! Resetting to 0.")
                losses[key] = torch.tensor(0.0, device=self._get_device(), requires_grad=True)
            
            weight = self.weights.get(key, 0.0)
            if weight > 0:
                total_loss += weight * losses[key]

        losses['total'] = total_loss

        # æ—¥å¿—è®°å½•
        if self.logger and self.loss_logger:
            self._batch_counter += 1
            if self._batch_counter % 100 == 0:
                self.loss_logger.log_weighted_loss_summary(losses, self.weights)

        return losses
