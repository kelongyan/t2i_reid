import torch
import torch.nn as nn
import torch.nn.functional as F

class Loss(nn.Module):
    def __init__(self, temperature=0.1, weights=None, num_classes=None):
        super().__init__()
        self.temperature = temperature
        self.num_classes = num_classes
        
        # ä½¿ç”¨Label Smoothingé™ä½åˆ†ç±»æŸå¤±çš„åˆå§‹å€¼
        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # === P0æ–¹æ¡ˆï¼šæé«˜clsæƒé‡ï¼Œä¿®å¤æƒé‡å¤±è¡¡ ===
        # === P1æ–¹æ¡ˆï¼šåŠ¨æ€æƒé‡è°ƒæ•´çš„åˆå§‹å€¼ ===
        self.weights = weights if weights is not None else {
            'info_nce': 1.0,        # å¯¹æ¯”å­¦ä¹ æƒé‡ - ä¸»å¯¼æŸå¤±
            'cls': 0.2,             # ğŸ”¥ P0: ä»0.05æé«˜åˆ°0.2ï¼ˆ4å€ï¼‰
            'cloth_semantic': 0.15, # é€‚åº¦é™ä½ï¼Œé¿å…ç«äº‰
            'orthogonal': 0.5,      # ğŸ”¥ P1: ä»0.3æé«˜åˆ°0.5ï¼ŒåŠ å¼ºè§£è€¦
            'gate_adaptive': 0.1,   # ç®€åŒ–åé™ä½æƒé‡
        }
        
        # åŠ¨æ€æƒé‡è°ƒæ•´å‚æ•°
        self.current_epoch = 0
        self.enable_dynamic_weights = True  # æ˜¯å¦å¯ç”¨åŠ¨æ€æƒé‡
        
        # ç»´åº¦è½¬æ¢å±‚ï¼ˆç”¨äºcloth_semanticçš„å»IDæ­£åˆ™ï¼‰
        # è¿™äº›å±‚ä¼šåœ¨ç¬¬ä¸€æ¬¡forwardæ—¶åŠ¨æ€åˆå§‹åŒ–
        self.id_to_256 = None  # å°†768ç»´id_embedsæŠ•å½±åˆ°256ç»´
        self.initialized = False

    
    def _initialize_projection_layers(self, device):
        """åŠ¨æ€åˆå§‹åŒ–æŠ•å½±å±‚"""
        if not self.initialized:
            # 768ç»´ -> 256ç»´çš„æŠ•å½±å±‚
            self.id_to_256 = nn.Linear(768, 256).to(device)
            self.initialized = True
    
    def update_epoch(self, epoch):
        """
        === P1æ–¹æ¡ˆï¼šåŠ¨æ€æƒé‡è°ƒæ•´ ===
        æ ¹æ®è®­ç»ƒé˜¶æ®µè‡ªé€‚åº”è°ƒæ•´æŸå¤±æƒé‡
        """
        self.current_epoch = epoch
        
        if not self.enable_dynamic_weights:
            return
        
        # é˜¶æ®µ1 (Epoch 1-10): å¿«é€Ÿé™ä½clsï¼Œå¼ºåŒ–è§£è€¦
        if epoch <= 10:
            self.weights['cls'] = 0.25           # æ›´é«˜çš„clsæƒé‡
            self.weights['cloth_semantic'] = 0.1  # é™ä½clothæƒé‡
            self.weights['orthogonal'] = 0.6      # éå¸¸å¼ºçš„æ­£äº¤çº¦æŸ
            self.weights['gate_adaptive'] = 0.05  # é—¨æ§æ™šæœŸä»‹å…¥
            
        # é˜¶æ®µ2 (Epoch 11-30): å¹³è¡¡ä¼˜åŒ–
        elif epoch <= 30:
            self.weights['cls'] = 0.15
            self.weights['cloth_semantic'] = 0.15
            self.weights['orthogonal'] = 0.5
            self.weights['gate_adaptive'] = 0.1
            
        # é˜¶æ®µ3 (Epoch 31-50): ç²¾ç»†è°ƒä¼˜
        elif epoch <= 50:
            self.weights['cls'] = 0.1
            self.weights['cloth_semantic'] = 0.2
            self.weights['orthogonal'] = 0.4
            self.weights['gate_adaptive'] = 0.15
            
        # é˜¶æ®µ4 (Epoch 51+): æœ€ç»ˆå¾®è°ƒ
        else:
            self.weights['cls'] = 0.08
            self.weights['cloth_semantic'] = 0.25
            self.weights['orthogonal'] = 0.3
            self.weights['gate_adaptive'] = 0.15
        """
        ä¿®å¤åçš„é—¨æ§è‡ªé€‚åº”æŸå¤±
        ç›®æ ‡: æ ¹æ®ç‰¹å¾è´¨é‡åŠ¨æ€å¹³è¡¡gateå€¼ï¼Œç¡®ä¿é—¨æ§æœºåˆ¶æ­£å¸¸å·¥ä½œ
        """
        if gate is None or id_embeds is None or cloth_embeds is None:
            # è¿”å›ä¸€ä¸ªå¯å¾®åˆ†çš„é›¶å¼ é‡
            if id_embeds is not None:
                # ä½¿ç”¨id_embedsåˆ›å»ºä¸€ä¸ªå¯å¾®åˆ†çš„é›¶å€¼
                return id_embeds.sum() * 0.0
            elif cloth_embeds is not None:
                return cloth_embeds.sum() * 0.0
            else:
                # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿ
                return torch.tensor(0.0, requires_grad=True)
        
        batch_size = id_embeds.size(0)
        
        # === ä¿®å¤1: ç»Ÿä¸€gateç»´åº¦å¤„ç† ===
        if gate.dim() == 2:
            if gate.size(1) > 1:
                # gateå½¢çŠ¶ä¸º[B, dim]ï¼Œå–å¹³å‡å¾—åˆ°[B]
                gate_value = gate.mean(dim=1)
            else:
                # gateå½¢çŠ¶ä¸º[B, 1]ï¼Œsqueezeå¾—åˆ°[B]
                gate_value = gate.squeeze(1)
        elif gate.dim() == 1:
            # gateå½¢çŠ¶å·²ç»æ˜¯[B]
            gate_value = gate
        else:
            # gateæ˜¯æ ‡é‡ï¼Œæ‰©å±•ä¸º[B]
            gate_value = gate.expand(batch_size)
        
        # === ä¿®å¤2: é‡æ–°è®¾è®¡ç‰¹å¾è´¨é‡åº¦é‡ ===
        # ä½¿ç”¨batchå†…çš„æ–¹å·®ä½œä¸ºåˆ¤åˆ«æ€§åº¦é‡ï¼Œè€Œä¸æ˜¯ç‰¹å¾ç»´åº¦çš„æ–¹å·®
        if batch_size < 2:
            # æ‰¹æ¬¡å¤ªå°æ—¶ï¼Œä½¿ç”¨ç‰¹å¾çš„L2èŒƒæ•°ä½œä¸ºè´¨é‡æŒ‡æ ‡
            id_quality = id_embeds.norm(dim=1).mean()
            cloth_quality = cloth_embeds.norm(dim=1).mean()
        else:
            # è®¡ç®—batchå†…ç‰¹å¾çš„æ ‡å‡†å·®ï¼ˆæ¯ä¸ªç‰¹å¾ç»´åº¦ä¸Šæ ·æœ¬çš„æ ‡å‡†å·®ï¼‰
            # æ ‡å‡†å·®å¤§è¯´æ˜è¯¥ç»´åº¦çš„åˆ¤åˆ«æ€§å¼º
            id_quality = id_embeds.std(dim=0).mean()    # å¯¹æ‰€æœ‰ç‰¹å¾ç»´åº¦çš„æ–¹å·®å–å¹³å‡
            cloth_quality = cloth_embeds.std(dim=0).mean()
        
        # é˜²æ­¢æ•°å€¼ä¸ç¨³å®šï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„èŒƒå›´
        id_quality = torch.clamp(id_quality, min=0.01, max=10.0)
        cloth_quality = torch.clamp(cloth_quality, min=0.01, max=10.0)
        
        # === ä¿®å¤3: åŠ¨æ€ç›®æ ‡gateè®¡ç®— ===
        # gateåº”è¯¥åæ˜ idç‰¹å¾çš„é‡è¦æ€§ç›¸å¯¹äºæ€»é‡è¦æ€§çš„æ¯”ä¾‹
        total_quality = id_quality + cloth_quality + 1e-6  # åŠ epsé˜²æ­¢é™¤é›¶
        target_gate_value = id_quality / total_quality
        
        # å°†æ ‡é‡æ‰©å±•ä¸º[B]ä»¥åŒ¹é…gate_valueçš„å½¢çŠ¶
        if target_gate_value.dim() == 0:
            target_gate_value = target_gate_value.detach().expand(batch_size)
        else:
            target_gate_value = target_gate_value.detach()
        
        # === æŸå¤±ç»„æˆ ===
        # 1. MSEæŸå¤±: ä½¿gateæ¥è¿‘ç›®æ ‡å€¼
        mse_loss = F.mse_loss(gate_value, target_gate_value)
        
        # 2. ç†µæ­£åˆ™: é˜²æ­¢gateè¿‡äºæç«¯ï¼ˆè¿‡äºæ¥è¿‘0æˆ–1ï¼‰
        gate_clamp = torch.clamp(gate_value, min=1e-6, max=1-1e-6)
        entropy = -(gate_clamp * torch.log(gate_clamp) + 
                    (1 - gate_clamp) * torch.log(1 - gate_clamp))
        entropy_reg = -entropy.mean()  # è´Ÿå·è¡¨ç¤ºæœ€å¤§åŒ–ç†µï¼ˆé¼“åŠ±ä¸ç¡®å®šæ€§ï¼‰
        
        # 3. ç¨³å®šæ€§çº¦æŸ: é˜²æ­¢gateåœ¨batchå†…å˜åŒ–è¿‡å¤§
        if batch_size > 1:
            gate_var = gate_value.var()
            # æœŸæœ›æ–¹å·®å°äº0.01ï¼Œè¿‡å¤§åˆ™æƒ©ç½š
            stability_loss = torch.clamp(gate_var - 0.01, min=0.0)
        else:
            # ä½¿ç”¨ä¸€ä¸ªå¯å¾®åˆ†çš„é›¶å€¼
            stability_loss = gate_value.mean() * 0.0
        
        # æ€»æŸå¤±: åŠ æƒç»„åˆä¸‰ä¸ªéƒ¨åˆ†
        # é™ä½ç†µæ­£åˆ™æƒé‡ï¼Œé¿å…æ€»æŸå¤±å˜è´Ÿ
        total_loss = mse_loss + 0.01 * entropy_reg + 0.05 * stability_loss
        
        # æœ€ç»ˆè£å‰ªï¼Œç¡®ä¿æŸå¤±åœ¨åˆç†èŒƒå›´å†…
        # ä¸å†ä½¿ç”¨min=0.0ï¼Œå…è®¸å°çš„è´Ÿå€¼ï¼ˆç†µæ­£åˆ™å¯èƒ½ç•¥å¤§ï¼‰
        total_loss = torch.clamp(total_loss, min=-1.0, max=10.0)
        # ä½†æœ€ç»ˆè¿”å›æ—¶ç¡®ä¿éè´Ÿ
        total_loss = torch.relu(total_loss)
        
        return total_loss

    def info_nce_loss(self, image_embeds, text_embeds):
        bsz = image_embeds.size(0)
        # ç¡®ä¿ç‰¹å¾å·²å½’ä¸€åŒ–
        image_embeds = F.normalize(image_embeds, dim=-1, eps=1e-8)
        text_embeds = F.normalize(text_embeds, dim=-1, eps=1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim = torch.matmul(image_embeds, text_embeds.t()) / self.temperature
        
        # é˜²æ­¢æ•°å€¼æº¢å‡ºï¼ˆsoftmaxç¨³å®šæ€§ï¼‰
        sim = torch.clamp(sim, min=-50, max=50)
        
        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        
        # è®¡ç®—åŒå‘å¯¹æ¯”æŸå¤±
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)
        
        return (loss_i2t + loss_t2i) / 2

    def id_classification_loss(self, id_logits, pids):
        return self.ce_loss(id_logits, pids)

    def cloth_semantic_loss(self, cloth_image_embeds, cloth_text_embeds, id_embeds=None):
        """
        æ”¹è¿›çš„æœè£…è¯­ä¹‰æŸå¤±
        ç›®æ ‡: å¯¹é½æœè£…ç‰¹å¾ï¼ŒåŒæ—¶æ·»åŠ å»IDæ­£åˆ™ï¼Œé¿å…clothç‰¹å¾åŒ…å«èº«ä»½ä¿¡æ¯
        """
        if cloth_image_embeds is None or cloth_text_embeds is None:
            return torch.tensor(0.0, device=cloth_image_embeds.device if cloth_image_embeds is not None else 'cuda')
        
        bsz = cloth_image_embeds.size(0)
        
        # === æ ‡å‡†å¯¹æ¯”å­¦ä¹ æŸå¤± ===
        # ç¡®ä¿ç‰¹å¾å·²å½’ä¸€åŒ–
        cloth_image_norm = F.normalize(cloth_image_embeds, dim=-1, eps=1e-8)
        cloth_text_norm = F.normalize(cloth_text_embeds, dim=-1, eps=1e-8)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim = torch.matmul(cloth_image_norm, cloth_text_norm.t()) / self.temperature
        sim = torch.clamp(sim, min=-50, max=50)
        
        labels = torch.arange(bsz, device=sim.device, dtype=torch.long)
        
        # è®¡ç®—åŒå‘æŸå¤±
        loss_i2t = self.ce_loss(sim, labels)
        loss_t2i = self.ce_loss(sim.t(), labels)
        
        base_loss = (loss_i2t + loss_t2i) / 2
        
        # === æ”¹è¿›: æ·»åŠ å»IDæ­£åˆ™ ===
        # ç¡®ä¿clothç‰¹å¾ä¸åŒ…å«idä¿¡æ¯ï¼Œé˜²æ­¢ä¿¡æ¯æ³„æ¼
        if id_embeds is not None:
            id_norm = F.normalize(id_embeds, dim=-1, eps=1e-8)
            
            # clothç‰¹å¾ä¸åº”è¯¥ä¸idç‰¹å¾ç›¸ä¼¼
            # è®¡ç®—cloth_imageå’Œidçš„ä½™å¼¦ç›¸ä¼¼åº¦
            cloth_id_sim = (cloth_image_norm * id_norm).sum(dim=-1)
            
            # æœ€å°åŒ–ç›¸ä¼¼åº¦çš„ç»å¯¹å€¼ï¼ˆå¸Œæœ›clothå’Œidæ­£äº¤ï¼‰
            de_id_penalty = cloth_id_sim.abs().mean()
            
            # å°†å»IDæ­£åˆ™åŠ å…¥æ€»æŸå¤±ï¼Œæƒé‡ä¸º0.2
            return base_loss + 0.2 * de_id_penalty
        
        return base_loss

    def orthogonal_loss(self, id_embeds, cloth_embeds):
        """
        å¢å¼ºçš„æ­£äº¤çº¦æŸæŸå¤±
        ç›®æ ‡: å¼ºåˆ¶idå’Œclothç‰¹å¾æ­£äº¤ï¼Œå‡å°‘ä¿¡æ¯æ³„æ¼ï¼ŒåŠ å¼ºè§£è€¦æ•ˆæœ
        """
        if id_embeds is None or cloth_embeds is None:
            return torch.tensor(0.0, device=id_embeds.device if id_embeds is not None else 'cuda')
        
        batch_size = id_embeds.size(0)
        
        # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        id_norm = F.normalize(id_embeds, dim=-1, eps=1e-8)
        cloth_norm = F.normalize(cloth_embeds, dim=-1, eps=1e-8)
        
        # === æ”¹è¿›1: æ‰¹æ¬¡å†…æ­£äº¤çº¦æŸï¼ˆæ ·æœ¬å†…è§£è€¦ï¼‰===
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = (id_norm * cloth_norm).sum(dim=-1)  # [B]
        cosine_sim = torch.clamp(cosine_sim, min=-1.0, max=1.0)
        
        # ä½¿ç”¨å¹³æ–¹æŸå¤±è€Œä¸æ˜¯ç»å¯¹å€¼ï¼Œæ¢¯åº¦æ›´ç¨³å®š
        ortho_loss_batch = cosine_sim.pow(2).mean()
        
        # === æ”¹è¿›2: æ·»åŠ è·¨æ ·æœ¬æ­£äº¤çº¦æŸï¼ˆæ ·æœ¬é—´ç‹¬ç«‹æ€§ï¼‰===
        # ç¡®ä¿ä¸åŒæ ·æœ¬çš„idå’Œclothç‰¹å¾ä¹Ÿç›¸äº’ç‹¬ç«‹
        if batch_size > 1:
            # è®¡ç®—GramçŸ©é˜µ: æ¯å¯¹æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦
            id_gram = torch.matmul(id_norm, id_norm.t())      # [B, B]
            cloth_gram = torch.matmul(cloth_norm, cloth_norm.t())  # [B, B]
            
            # è®¡ç®—idå’Œclothçš„GramçŸ©é˜µçš„é€å…ƒç´ ä¹˜ç§¯
            # å¯¹è§’çº¿æ˜¯è‡ªç›¸å…³(å·²ç»åœ¨ortho_loss_batchå¤„ç†)ï¼Œéå¯¹è§’çº¿åº”è¯¥æ¥è¿‘0
            # åˆ›å»ºæ©ç ï¼Œç§»é™¤å¯¹è§’çº¿å…ƒç´ 
            mask = ~torch.eye(batch_size, dtype=torch.bool, device=id_gram.device)
            
            # è®¡ç®—éå¯¹è§’çº¿å…ƒç´ çš„äº¤å‰ç›¸å…³
            cross_correlation = (id_gram[mask] * cloth_gram[mask]).abs().mean()
            
            # === æ”¹è¿›3: æ·»åŠ è‡ªç›¸å…³æƒ©ç½š ===
            # ç¡®ä¿åŒæ¨¡æ€å†…ä¸åŒæ ·æœ¬ä¹Ÿä¿æŒå¤šæ ·æ€§
            # éå¯¹è§’çº¿å…ƒç´ ä¸åº”è¿‡å¤§ï¼ˆé¿å…ç‰¹å¾åç¼©ï¼‰
            id_self_corr = id_gram[mask].abs().mean()
            cloth_self_corr = cloth_gram[mask].abs().mean()
            
            # æœŸæœ›è‡ªç›¸å…³åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸è¦å¤ªå¤§ï¼Œå¦åˆ™ç‰¹å¾ç›¸ä¼¼ï¼›ä¸è¦å¤ªå°ï¼Œå¦åˆ™è¿‡åº¦åˆ†æ•£ï¼‰
            self_corr_penalty = torch.clamp(id_self_corr - 0.5, min=0.0) + \
                               torch.clamp(cloth_self_corr - 0.5, min=0.0)
        else:
            cross_correlation = torch.tensor(0.0, device=id_embeds.device)
            self_corr_penalty = torch.tensor(0.0, device=id_embeds.device)
        
        # æ€»æŸå¤±: åŠ æƒç»„åˆä¸‰ä¸ªéƒ¨åˆ†
        # ä¸»è¦çº¦æŸæ˜¯æ‰¹æ¬¡å†…æ­£äº¤(1.0)ï¼Œè·¨æ ·æœ¬äº¤å‰ç›¸å…³æ¬¡ä¹‹(0.1)ï¼Œè‡ªç›¸å…³æƒ©ç½šæœ€å°(0.05)
        total_loss = ortho_loss_batch + 0.1 * cross_correlation + 0.05 * self_corr_penalty
        
        return total_loss
    
    def opa_alignment_loss(self, id_seq_features, cloth_seq_features):
        """
        OPA å¯¹é½æŸå¤±ï¼ˆG-S3 ä¸“ç”¨ï¼‰
        ç¡®ä¿ OPA è¾“å‡ºçš„èº«ä»½å’Œæœè£…åºåˆ—ç‰¹å¾æ­£äº¤
        """
        if id_seq_features is None or cloth_seq_features is None:
            return torch.tensor(0.0, device=self.ce_loss.weight.device)
        
        id_norm = F.normalize(id_seq_features, dim=-1)
        cloth_norm = F.normalize(cloth_seq_features, dim=-1)
        cosine_sim = (id_norm * cloth_norm).sum(dim=-1)
        
        return cosine_sim.abs().mean()
    
    def mamba_filter_quality_loss(self, filtered_features, saliency_score):
        """
        Mamba è¿‡æ»¤è´¨é‡æŸå¤±ï¼ˆG-S3 ä¸“ç”¨ï¼‰
        ç¡®ä¿é«˜æ˜¾è‘—æ€§åŒºåŸŸçš„ç‰¹å¾è¢«æœ‰æ•ˆæŠ‘åˆ¶
        """
        if filtered_features is None or saliency_score is None:
            return torch.tensor(0.0, device=self.ce_loss.weight.device)
        
        feature_strength = filtered_features.norm(dim=-1, keepdim=True)
        suppression_loss = (feature_strength * saliency_score).mean()
        
        return suppression_loss

    def forward(self, image_embeds, id_text_embeds, fused_embeds, id_logits, id_embeds,
                cloth_embeds, cloth_text_embeds, cloth_image_embeds, pids, is_matched=None, epoch=None, gate=None,
                id_seq_features=None, cloth_seq_features=None, saliency_score=None):
        
        losses = {}
        
        # æ ¸å¿ƒæŸå¤±
        losses['info_nce'] = self.info_nce_loss(image_embeds, id_text_embeds) if image_embeds is not None and id_text_embeds is not None else torch.tensor(0.0, device=self.ce_loss.weight.device)
        losses['cls'] = self.id_classification_loss(id_logits, pids) if id_logits is not None and pids is not None else torch.tensor(0.0, device=self.ce_loss.weight.device)
        
        # æœè£…è¯­ä¹‰æŸå¤±ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ·»åŠ å»IDæ­£åˆ™ï¼‰
        # æ³¨æ„: cloth_image_embedsæ˜¯æŠ•å½±åçš„256ç»´ï¼Œéœ€è¦ä½¿ç”¨åŒæ ·æŠ•å½±åçš„image_embeds
        losses['cloth_semantic'] = self.cloth_semantic_loss(cloth_image_embeds, cloth_text_embeds, image_embeds)
        
        # æ­£äº¤çº¦æŸæŸå¤±ï¼ˆå¢å¼ºç‰ˆï¼‰
        losses['orthogonal'] = self.orthogonal_loss(id_embeds, cloth_embeds)
        
        # è‡ªé€‚åº”é—¨æ§æ­£åˆ™ï¼ˆä¿®å¤ç‰ˆï¼‰
        losses['gate_adaptive'] = self.gate_adaptive_loss(gate, id_embeds, cloth_embeds)
        
        # G-S3 ä¸“ç”¨æŸå¤±ï¼ˆå¯é€‰ï¼‰
        if 'opa_alignment' in self.weights and id_seq_features is not None and cloth_seq_features is not None:
            losses['opa_alignment'] = self.opa_alignment_loss(id_seq_features, cloth_seq_features)
        
        if 'mamba_quality' in self.weights and id_seq_features is not None and saliency_score is not None:
            losses['mamba_quality'] = self.mamba_filter_quality_loss(id_seq_features, saliency_score)
        
        # æ£€æŸ¥NaN/Infå¹¶æ›¿æ¢ä¸º0ï¼ˆé¿å…è®­ç»ƒå´©æºƒï¼‰
        for key, value in losses.items():
            if isinstance(value, torch.Tensor):
                if torch.isnan(value).any() or torch.isinf(value).any():
                    losses[key] = torch.tensor(0.0, device=value.device, requires_grad=True)
        
        # ç®€å•åŠ æƒæ±‚å’Œ
        total_loss = sum(self.weights.get(k, 0) * losses[k] for k in losses.keys() if k != 'total')
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(total_loss).any() or torch.isinf(total_loss).any():
            total_loss = torch.tensor(0.0, device=total_loss.device, requires_grad=True)
        
        losses['total'] = total_loss
        
        # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•æŸå¤±æ¢¯åº¦å’Œæ•°å€¼ç¨³å®šæ€§
        if hasattr(self, '_debug_mode') and self._debug_mode:
            self._debug_loss_info = {
                'loss_values': {k: v.item() for k, v in losses.items() if isinstance(v, torch.Tensor)},
                'loss_requires_grad': {k: v.requires_grad for k, v in losses.items() if isinstance(v, torch.Tensor)},
                'has_nan': any(torch.isnan(v).any() for v in losses.values() if isinstance(v, torch.Tensor)),
                'has_inf': any(torch.isinf(v).any() for v in losses.values() if isinstance(v, torch.Tensor))
            }
        
        return losses
