# evaluators/evaluator.py
"""
T2I-ReID Evaluator
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
"""

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from sklearn.metrics import average_precision_score


class Evaluator:
    """
    T2I-ReID è¯„ä¼°å™¨
    """
    def __init__(self, model, args=None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model: å¾…è¯„ä¼°çš„æ¨¡å‹
            args: é…ç½®å‚æ•°
        """
        self.model = model
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def evaluate(self, query_loader, gallery_loader, query_data, gallery_data, 
                 checkpoint_path=None, epoch=None):
        """
        æ‰§è¡Œè¯„ä¼° - Text-to-Image ReID
        
        ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®çš„Text-to-Image ReIDè¯„ä¼°
        - Query: ä½¿ç”¨æ–‡æœ¬ç‰¹å¾ (encode_text)
        - Gallery: ä½¿ç”¨å›¾åƒç‰¹å¾ (encode_image)
        
        Args:
            query_loader: Queryæ•°æ®åŠ è½½å™¨ï¼ˆæ–‡æœ¬ï¼‰
            gallery_loader: Galleryæ•°æ®åŠ è½½å™¨ï¼ˆå›¾åƒï¼‰
            query_data: Queryæ•°æ®é›†
            gallery_data: Galleryæ•°æ®é›†
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            epoch: å½“å‰epochï¼ˆå¯é€‰ï¼‰
        
        Returns:
            dict: åŒ…å«mAP, rank1, rank5, rank10çš„å­—å…¸
        """
        self.model.eval()
        
        with torch.no_grad():
            # ğŸ”¥ ä¿®å¤ï¼šQueryä½¿ç”¨æ–‡æœ¬ç‰¹å¾
            query_features = []
            query_pids = []
            query_camids = []
            
            for batch in tqdm(query_loader, desc="Extracting query text features"):
                images, _, captions, pids, cam_ids, _ = batch
                
                # ğŸ”¥ ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨è€Œä¸æ˜¯å›¾åƒç¼–ç å™¨
                text_embeds = self.model.encode_text(captions)
                
                query_features.append(text_embeds.cpu())
                query_pids.append(pids)
                query_camids.append(cam_ids)
            
            query_features = torch.cat(query_features, dim=0)
            query_pids = torch.cat(query_pids, dim=0).numpy()
            query_camids = torch.cat(query_camids, dim=0).numpy()
            
            # Galleryä½¿ç”¨å›¾åƒç‰¹å¾ï¼ˆè¿™éƒ¨åˆ†æ˜¯æ­£ç¡®çš„ï¼‰
            gallery_features = []
            gallery_pids = []
            gallery_camids = []
            
            for batch in tqdm(gallery_loader, desc="Extracting gallery image features"):
                images, _, captions, pids, cam_ids, _ = batch
                images = images.to(self.device)
                
                # ä½¿ç”¨å›¾åƒç¼–ç å™¨
                image_embeds = self.model.encode_image(images)
                
                gallery_features.append(image_embeds.cpu())
                gallery_pids.append(pids)
                gallery_camids.append(cam_ids)
            
            gallery_features = torch.cat(gallery_features, dim=0)
            gallery_pids = torch.cat(gallery_pids, dim=0).numpy()
            gallery_camids = torch.cat(gallery_camids, dim=0).numpy()
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæ–‡æœ¬ x å›¾åƒï¼‰
        query_features = query_features / query_features.norm(dim=1, keepdim=True)
        gallery_features = gallery_features / gallery_features.norm(dim=1, keepdim=True)
        
        similarity_matrix = torch.mm(query_features, gallery_features.t()).numpy()
        
        # è®¡ç®—æŒ‡æ ‡
        cmc, mAP = self.compute_metrics(
            similarity_matrix,
            query_pids,
            gallery_pids,
            query_camids,
            gallery_camids
        )
        
        metrics = {
            'mAP': mAP,
            'rank1': cmc[0],
            'rank5': cmc[4] if len(cmc) > 4 else cmc[-1],
            'rank10': cmc[9] if len(cmc) > 9 else cmc[-1]
        }
        
        return metrics
    
    def compute_metrics(self, similarity_matrix, query_pids, gallery_pids, 
                       query_camids, gallery_camids):
        """
        è®¡ç®—CMCå’ŒmAPæŒ‡æ ‡
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ [num_query, num_gallery]
            query_pids: Queryçš„person IDs
            gallery_pids: Galleryçš„person IDs
            query_camids: Queryçš„camera IDs
            gallery_camids: Galleryçš„camera IDs
        
        Returns:
            tuple: (cmc, mAP)
        """
        num_query = similarity_matrix.shape[0]
        
        # å­˜å‚¨æ‰€æœ‰queryçš„APå€¼
        all_AP = []
        all_cmc = []
        
        for i in range(num_query):
            # è·å–å½“å‰query
            query_pid = query_pids[i]
            query_camid = query_camids[i]
            
            # è·å–ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆé™åºæ’åˆ—çš„ç´¢å¼•ï¼‰
            scores = similarity_matrix[i]
            indices = np.argsort(-scores)
            
            # è·å–åŒ¹é…æƒ…å†µ
            matches = (gallery_pids[indices] == query_pid)
            
            # === ğŸ”¥ ä¿®å¤ï¼šä¼˜åŒ–åŒæ‘„åƒå¤´è¿‡æ»¤é€»è¾‘ ===
            # å¦‚æœæ‰€æœ‰camera_idéƒ½ç›¸åŒï¼ˆå¦‚å…¨ä¸º0ï¼‰ï¼Œåˆ™ä¸è¿›è¡Œcameraè¿‡æ»¤
            unique_cameras = np.unique(np.concatenate([query_camids, gallery_camids]))
            if len(unique_cameras) > 1:
                # å¤šä¸ªcameraï¼Œæ­£å¸¸è¿‡æ»¤åŒcameraçš„æ­£æ ·æœ¬
                same_camera = (gallery_camids[indices] == query_camid)
                valid = ~(matches & same_camera)  # ç§»é™¤åŒcameraçš„æ­£æ ·æœ¬
                matches = matches[valid]
            # å¦åˆ™ä¸è¿‡æ»¤ï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½æ˜¯åŒä¸€ä¸ªcameraï¼‰
            
            if not np.any(matches):
                continue
            
            # è®¡ç®—CMC
            cmc = matches.cumsum()
            cmc[cmc > 1] = 1
            all_cmc.append(cmc)
            
            # è®¡ç®—AP
            num_rel = matches.sum()
            tmp_cmc = matches.cumsum()
            tmp_cmc = tmp_cmc / (np.arange(len(tmp_cmc)) + 1.0)
            tmp_cmc = tmp_cmc * matches
            AP = tmp_cmc.sum() / num_rel
            all_AP.append(AP)
        
        # === ğŸ”¥ ä¿®å¤ï¼šå¤„ç†ç©ºCMCåˆ—è¡¨ ===
        if len(all_cmc) == 0 or len(all_AP) == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„query-galleryåŒ¹é…ï¼Œè¿”å›0æŒ‡æ ‡
            print(f"âš ï¸  Warning: No valid query-gallery matches found!")
            print(f"   Query samples: {num_query}")
            print(f"   Valid matches: 0")
            # è¿”å›å…¨0çš„CMCå’ŒmAP
            return np.zeros(100), 0.0
        
        # å¹³å‡CMC
        max_len = max([len(cmc) for cmc in all_cmc])
        for i in range(len(all_cmc)):
            if len(all_cmc[i]) < max_len:
                # å¡«å……æœ€åä¸€ä¸ªå€¼
                all_cmc[i] = np.concatenate([
                    all_cmc[i],
                    np.ones(max_len - len(all_cmc[i])) * all_cmc[i][-1]
                ])
        
        all_cmc = np.array(all_cmc).astype(float)
        all_cmc = all_cmc.sum(axis=0) / len(all_cmc)
        
        # è®¡ç®—mAP
        mAP = np.mean(all_AP)
        
        return all_cmc, mAP
