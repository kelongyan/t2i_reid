#!/usr/bin/env python3
"""
Mamba SSM æ¨¡å‹æµ‹è¯•è„šæœ¬
åŠŸèƒ½:éªŒè¯Mambaæ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½,åŒ…æ‹¬å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
"""

import torch
import torch.nn as nn
from typing import Optional
import time

try:
    from mamba_ssm import Mamba
except ImportError:
    print("é”™è¯¯ï¼šæœªå®‰è£… mamba-ssm åº“")
    print("è¯·è¿è¡Œï¼špip install mamba-ssm causal-conv1d")
    exit(1)


def setup_device() -> torch.device:
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ“ ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        device = torch.device("cpu")
        print("âš  ä½¿ç”¨CPUè®¾å¤‡ (å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿ)")
    return device


def create_mamba_model(d_model: int = 128, d_state: int = 16, 
                      d_conv: int = 4, expand: int = 2, 
                      device: Optional[torch.device] = None) -> nn.Module:
    """åˆ›å»ºMambaæ¨¡å‹"""
    if device is None:
        device = setup_device()
    
    try:
        model = Mamba(
            d_model=d_model,
            d_state=d_state,
            d_conv=d_conv,
            expand=expand,
        ).to(device)
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ“ Mambaæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
        print(f"  ç‰¹å¾ç»´åº¦: {d_model}, çŠ¶æ€ç»´åº¦: {d_state}")
        
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        raise


def test_forward_pass(model: nn.Module, batch_size: int = 4, 
                     seq_length: int = 64, d_model: int = 128,
                     device: torch.device = None) -> torch.Tensor:
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print(f"\n--- å‰å‘ä¼ æ’­æµ‹è¯• ---")
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    input_tensor = torch.randn(batch_size, seq_length, d_model, device=device)
    print(f"è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
    
    # æ‰§è¡Œå‰å‘ä¼ æ’­å¹¶è®¡æ—¶
    model.eval()
    start_time = time.time()
    
    with torch.no_grad():
        output = model(input_tensor)
    
    forward_time = time.time() - start_time
    
    # éªŒè¯è¾“å‡º
    expected_shape = (batch_size, seq_length, d_model)
    if output.shape == expected_shape:
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
    else:
        print(f"âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {output.shape}")
    
    print(f"âœ“ å‰å‘ä¼ æ’­è€—æ—¶: {forward_time*1000:.2f}ms")
    print(f"  è¾“å‡ºæ ·æœ¬ (å‰5ä¸ªå€¼): {output[0, 0, :5].cpu().numpy()}")
    print(f"  è¾“å‡ºç»Ÿè®¡ - å‡å€¼: {output.mean():.4f}, æ ‡å‡†å·®: {output.std():.4f}")
    
    return output


def test_backward_pass(model: nn.Module, batch_size: int = 4, 
                      seq_length: int = 64, d_model: int = 128,
                      device: torch.device = None) -> None:
    """æµ‹è¯•åå‘ä¼ æ’­"""
    print(f"\n--- åå‘ä¼ æ’­æµ‹è¯• ---")
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„è¾“å…¥
    input_tensor = torch.randn(batch_size, seq_length, d_model, 
                              device=device, requires_grad=True)
    
    model.train()
    start_time = time.time()
    
    # å‰å‘ä¼ æ’­
    output = model(input_tensor)
    
    # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
    loss = output.mean()  # ä½¿ç”¨å‡å€¼è€Œä¸æ˜¯æ±‚å’Œï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
    loss.backward()
    
    backward_time = time.time() - start_time
    
    # æ£€æŸ¥æ¢¯åº¦
    if input_tensor.grad is not None:
        grad_norm = input_tensor.grad.norm().item()
        print(f"âœ“ åå‘ä¼ æ’­æˆåŠŸ")
        print(f"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        print(f"  æŸå¤±å€¼: {loss.item():.6f}")
    else:
        print("âŒ æ¢¯åº¦è®¡ç®—å¤±è´¥")
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ¢¯åº¦
    param_grads = [p.grad.norm().item() for p in model.parameters() 
                   if p.grad is not None]
    if param_grads:
        print(f"  æ¨¡å‹å‚æ•°æ¢¯åº¦èŒƒæ•°: æœ€å¤§={max(param_grads):.6f}, æœ€å°={min(param_grads):.6f}")
    
    print(f"âœ“ åå‘ä¼ æ’­è€—æ—¶: {backward_time*1000:.2f}ms")


def benchmark_model(model: nn.Module, device: torch.device, 
                   batch_size: int = 4, seq_length: int = 64, 
                   d_model: int = 128, num_runs: int = 10) -> None:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print(f"\n--- æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_runs}æ¬¡è¿è¡Œ) ---")
    
    model.eval()
    input_tensor = torch.randn(batch_size, seq_length, d_model, device=device)
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(3):
            _ = model(input_tensor)
    
    # å®é™…æµ‹è¯•
    torch.cuda.synchronize() if device.type == 'cuda' else None
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(num_runs):
            _ = model(input_tensor)
    
    torch.cuda.synchronize() if device.type == 'cuda' else None
    total_time = time.time() - start_time
    
    avg_time = total_time / num_runs
    throughput = batch_size / avg_time
    
    print(f"âœ“ å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}ms")
    print(f"âœ“ ååé‡: {throughput:.1f} samples/sec")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Mamba SSM æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    config = {
        'd_model': 128,
        'd_state': 16,
        'd_conv': 4,
        'expand': 2,
        'batch_size': 4,
        'seq_length': 64
    }
    
    try:
        # 1. è®¾ç½®è®¾å¤‡
        device = setup_device()
        
        # 2. åˆ›å»ºæ¨¡å‹
        model = create_mamba_model(
            d_model=config['d_model'],
            d_state=config['d_state'],
            d_conv=config['d_conv'],
            expand=config['expand'],
            device=device
        )
        
        # 3. æµ‹è¯•å‰å‘ä¼ æ’­
        _ = test_forward_pass(
            model, config['batch_size'], 
            config['seq_length'], config['d_model'], device
        )
        
        # 4. æµ‹è¯•åå‘ä¼ æ’­
        test_backward_pass(
            model, config['batch_size'], 
            config['seq_length'], config['d_model'], device
        )
        
        # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_model(model, device, 
                       config['batch_size'], config['seq_length'], 
                       config['d_model'])
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()