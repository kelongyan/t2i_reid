import argparse
import ast
import gc
import logging
import random
import sys
from pathlib import Path
import torch
from torch.backends import cudnn
from torch.cuda.amp import GradScaler

ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from utils.serialization import save_checkpoint
from models.model import Model
from datasets.data_builder import DataBuilder
from trainers.trainer import Trainer
from utils.lr_scheduler import WarmupMultiStepLR
from utils.monitor import get_monitor_for_dataset

def configuration():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Train T2I-ReID model")
    parser.add_argument('--root', type=str, default=str(ROOT_DIR / 'datasets'),
                       help='Root directory of the dataset')
    parser.add_argument('--dataset-configs', nargs='+', type=str, help='List of dataset configurations in JSON format')
    parser.add_argument('--loss-weights', type=str, help='Loss weights in JSON format')
    parser.add_argument('-b', '--batch-size', type=int, default=128, help='Batch size for training')
    parser.add_argument('-j', '--workers', type=int, default=4, help='Number of data loading workers')
    parser.add_argument('--height', type=int, default=224, help='Image height')
    parser.add_argument('--width', type=int, default=224, help='Image width')
    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=0.001, help='Weight decay')
    parser.add_argument('--warmup-step', type=int, default=500, help='Warmup steps')
    parser.add_argument('--milestones', nargs='+', type=int, default=[40, 60], help='Milestones for LR scheduler')
    parser.add_argument('--epochs', type=int, default=80, help='Number of training epochs')
    parser.add_argument('--seed', type=int, default=0, help='Random seed')
    parser.add_argument('--print-freq', type=int, default=50, help='Print frequency')
    parser.add_argument('--fp16', action='store_true', help='Use mixed precision training')
    parser.add_argument('--bert-base-path', type=str, default=str(ROOT_DIR / 'pretrained' / 'bert-base-uncased'),
                       help='Path to BERT model')
    parser.add_argument('--vit-pretrained', type=str, default=str(ROOT_DIR / 'pretrained' / 'vit-base-patch16-224'),
                       help='Path to ViT model')
    parser.add_argument('--vision-backbone', type=str, default='vim', choices=['vit', 'vim'],
                       help='Vision backbone type: vit or vim')
    parser.add_argument('--vim-pretrained', type=str, default=str(ROOT_DIR / 'pretrained' / 'Vision Mamba' / 'vim_s_midclstok.pth'),
                       help='Path to Vision Mamba model')
    parser.add_argument('--logs-dir', type=str, default=str(ROOT_DIR / 'log'), help='Directory for logs')
    parser.add_argument('--num-classes', type=int, default=8000, help='Number of identity classes')

    # Fusion module parameters
    parser.add_argument('--fusion-type', type=str, default='enhanced_mamba', help='Type of fusion module')
    parser.add_argument('--fusion-dim', type=int, default=256, help='Fusion module dimension')
    parser.add_argument('--fusion-d-state', type=int, default=16, help='Fusion module d_state')
    parser.add_argument('--fusion-d-conv', type=int, default=4, help='Fusion module d_conv')
    parser.add_argument('--fusion-num-layers', type=int, default=2, help='Fusion module number of layers')
    parser.add_argument('--fusion-output-dim', type=int, default=256, help='Fusion module output dimension')
    parser.add_argument('--fusion-dropout', type=float, default=0.1, help='Fusion module dropout')

    # Disentangle module parameters
    parser.add_argument('--id-projection-dim', type=int, default=768, help='ID projection dimension')
    parser.add_argument('--cloth-projection-dim', type=int, default=768, help='Cloth projection dimension')
    
    # G-S3 module parameters
    parser.add_argument('--disentangle-type', type=str, default='gs3', 
                       choices=['gs3', 'simple'],
                       help='Type of disentangle module: gs3 (G-S3 Module) or simple (DisentangleModule)')
    parser.add_argument('--gs3-num-heads', type=int, default=8, 
                       help='Number of attention heads in G-S3 OPA')
    parser.add_argument('--gs3-d-state', type=int, default=16, 
                       help='State dimension for G-S3 Mamba filter')
    parser.add_argument('--gs3-d-conv', type=int, default=4, 
                       help='Convolution kernel size for G-S3 Mamba filter')
    parser.add_argument('--gs3-dropout', type=float, default=0.1, 
                       help='Dropout rate for G-S3 module')

    # Loss weightsï¼ˆé‡æ–°è°ƒæ•´æƒé‡ä»¥å¹³è¡¡å„æŸå¤±é¡¹ï¼Œé¿å…ç«äº‰ï¼‰
    parser.add_argument('--loss-info-nce', type=float, default=1.0, help='InfoNCE loss weight')
    parser.add_argument('--loss-cls', type=float, default=0.05, help='Classification loss weight')
    parser.add_argument('--loss-cloth-semantic', type=float, default=0.2, help='Cloth semantic loss weight (ä»0.5é™åˆ°0.2)')
    parser.add_argument('--loss-orthogonal', type=float, default=0.3, help='Orthogonal loss weight (ä»0.1æé«˜åˆ°0.3)')
    parser.add_argument('--loss-gate-adaptive', type=float, default=0.15, help='Gate adaptive loss weight (ä»0.05æé«˜åˆ°0.15)')

    # Optimizer and scheduler
    parser.add_argument('--optimizer', type=str, default='Adam', help='Optimizer type')
    parser.add_argument('--scheduler', type=str, default='cosine', help='Scheduler type')

    # [ä¿®æ”¹ç‚¹ 1] æ·»åŠ  finetune-from å‚æ•°
    parser.add_argument('--finetune-from', type=str, help='Path to checkpoint to finetune from')

    args = parser.parse_args()

    # åˆå§‹åŒ– disentangle å­—å…¸
    args.disentangle = {}

    # å¤„ç†æŸå¤±æƒé‡
    if args.loss_weights:
        args.disentangle['loss_weights'] = ast.literal_eval(args.loss_weights)
    else:
        # è®¾ç½®é»˜è®¤æŸå¤±æƒé‡
        args.disentangle['loss_weights'] = {
            'info_nce': args.loss_info_nce,
            'cls': args.loss_cls,
            'cloth_semantic': args.loss_cloth_semantic,
            'orthogonal': args.loss_orthogonal,
            'gate_adaptive': args.loss_gate_adaptive
        }

    # å¤„ç†æ•°æ®é›†é…ç½®
    if args.dataset_configs:
        dataset_configs = []
        for cfg in args.dataset_configs:
            parsed = ast.literal_eval(cfg)
            dataset_configs.extend(parsed if isinstance(parsed, list) else [parsed])
        args.dataset_configs = dataset_configs
    else:
        args.dataset_configs = [
            {
                'name': 'CUHK-PEDES',
                'root': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES'),
                'json_file': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES' / 'annotations' / 'caption_all.json'),
                'cloth_json': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES' / 'annotations' / 'caption_cloth.json'),
                'id_json': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES' / 'annotations' / 'caption_id.json')
            },
            {
                'name': 'ICFG-PEDES',
                'root': str(ROOT_DIR / 'datasets' / 'ICFG-PEDES'),
                'json_file': str(ROOT_DIR / 'datasets' / 'ICFG-PEDES' / 'annotations' / 'ICFG-PEDES.json'),
                'cloth_json': str(ROOT_DIR / 'datasets' / 'ICFG-PEDES' / 'annotations' / 'caption_cloth.json'),
                'id_json': str(ROOT_DIR / 'datasets' / 'ICFG-PEDES' / 'annotations' / 'caption_id.json')
            },
            {
                'name': 'RSTPReid',
                'root': str(ROOT_DIR / 'datasets' / 'RSTPReid'),
                'json_file': str(ROOT_DIR / 'datasets' / 'RSTPReid' / 'annotations' / 'data_captions.json'),
                'cloth_json': str(ROOT_DIR / 'datasets' / 'RSTPReid' / 'annotations' / 'caption_cloth.json'),
                'id_json': str(ROOT_DIR / 'datasets' / 'RSTPReid' / 'annotations' / 'caption_id.json')
            }
        ]

    # ç¡®ä¿è·¯å¾„ä½¿ç”¨ Path å¯¹è±¡
    args.bert_base_path = str(Path(args.bert_base_path))
    args.vit_pretrained = str(Path(args.vit_pretrained))
    args.logs_dir = str(Path(args.logs_dir))
    args.root = str(Path(args.root))

    # éªŒè¯è·¯å¾„æœ‰æ•ˆæ€§
    if not Path(args.bert_base_path).exists():
        raise FileNotFoundError(f"BERT base path not found at: {args.bert_base_path}")
    if not Path(args.vit_pretrained).exists():
        raise FileNotFoundError(f"ViT base path not found at: {args.vit_pretrained}")

    args.img_size = (args.height, args.width)
    args.task_name = 't2i'
    return args, {}

class Runner:
    def __init__(self, args, config):
        # åˆå§‹åŒ–è¿è¡Œå™¨ï¼Œè®¾ç½®å‚æ•°å’Œè®¾å¤‡
        self.args = args
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = torch.amp.GradScaler('cuda', enabled=args.fp16) if self.device.type == 'cuda' else None
        if args.fp16 and self.device.type != 'cuda':
            logging.warning("FP16 is enabled but no CUDA device is available. Disabling mixed precision.")

        # ä¿å­˜åŸå§‹çš„æ—¥å¿—ç›®å½•ï¼Œä»¥é˜²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«ä¿®æ”¹
        self.args.original_logs_dir = args.logs_dir

        # åˆå§‹åŒ–ç›‘æ§å™¨
        # ä»æ•°æ®é›†é…ç½®ä¸­è·å–æ•°æ®é›†åç§°
        if hasattr(args, 'dataset_configs') and args.dataset_configs:
            dataset_name = args.dataset_configs[0]['name'] if args.dataset_configs else 'unknown'
        else:
            dataset_name = 'unknown'
        # ä½¿ç”¨åŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ log ç›®å½•ï¼Œè€Œä¸æ˜¯ä¾èµ–args.logs_dir
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = Path(__file__).parent  # scripts/
        project_root = script_dir.parent   # é¡¹ç›®æ ¹ç›®å½•
        log_base_dir = str(project_root / 'log')

        # ä¼ é€’åŸå§‹çš„dataset_nameç»™get_monitor_for_datasetï¼Œå®ƒä¼šå†…éƒ¨è§„èŒƒåŒ–
        self.monitor = get_monitor_for_dataset(dataset_name, log_base_dir)

    def verify_freeze_status(self, model):
        """
        éªŒè¯å†»ç»“çŠ¶æ€ï¼Œç¡®ä¿freezeå‡½æ•°ç”Ÿæ•ˆ
        
        Returns:
            dict: åŒ…å«å„æ¨¡å—çš„å†»ç»“ç»Ÿè®¡ä¿¡æ¯
        """
        vit_frozen = sum(p.numel() for n, p in model.named_parameters() 
                        if 'visual_encoder' in n and not p.requires_grad)
        vit_total = sum(p.numel() for n, p in model.named_parameters() 
                       if 'visual_encoder' in n)
        vit_trainable = vit_total - vit_frozen
        
        bert_frozen = sum(p.numel() for n, p in model.named_parameters() 
                         if 'text_encoder' in n and not p.requires_grad)
        bert_total = sum(p.numel() for n, p in model.named_parameters() 
                        if 'text_encoder' in n)
        bert_trainable = bert_total - bert_frozen
        
        task_trainable = sum(p.numel() for n, p in model.named_parameters() 
                            if 'visual_encoder' not in n and 'text_encoder' not in n and p.requires_grad)
        
        total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        
        stats = {
            'vit_frozen': vit_frozen,
            'vit_trainable': vit_trainable,
            'vit_total': vit_total,
            'bert_frozen': bert_frozen,
            'bert_trainable': bert_trainable,
            'bert_total': bert_total,
            'task_trainable': task_trainable,
            'total_trainable': total_trainable,
            'total_params': total_params
        }
        
        logging.info("=" * 70)
        logging.info("ğŸ“Š Freeze Status Verification")
        logging.info("=" * 70)
        logging.info(f"ViT:  {vit_trainable:,}/{vit_total:,} trainable "
                    f"({100*vit_trainable/vit_total:.1f}%), "
                    f"{vit_frozen:,} frozen ({100*vit_frozen/vit_total:.1f}%)")
        logging.info(f"BERT: {bert_trainable:,}/{bert_total:,} trainable "
                    f"({100*bert_trainable/bert_total:.1f}%), "
                    f"{bert_frozen:,} frozen ({100*bert_frozen/bert_total:.1f}%)")
        logging.info(f"Task: {task_trainable:,} trainable")
        logging.info(f"Total: {total_trainable:,}/{total_params:,} trainable "
                    f"({100*total_trainable/total_params:.1f}%)")
        logging.info("=" * 70)
        
        return stats
    
    def freeze_bert_layers(self, model, unfreeze_from_layer=None):
        """
        å†»ç»“/è§£å†»BERTçš„æŒ‡å®šå±‚ï¼ˆåŸºäºBERT-Base 12å±‚ç»“æ„ï¼‰
        
        Args:
            model: å®Œæ•´çš„T2I-ReIDæ¨¡å‹
            unfreeze_from_layer: 
                - None: å†»ç»“æ‰€æœ‰BERTå±‚
                - 8: è§£å†»layer 8-11ï¼ˆå4å±‚ï¼‰
                - 4: è§£å†»layer 4-11ï¼ˆå8å±‚ï¼‰
                - 0: è§£å†»æ‰€æœ‰12å±‚
        """
        # æ­¥éª¤1: å…ˆå†»ç»“æ‰€æœ‰BERTå‚æ•°
        for name, param in model.named_parameters():
            if 'text_encoder' in name:
                param.requires_grad = False
        
        # æ­¥éª¤2: å¦‚æœæŒ‡å®šäº†è§£å†»å±‚ï¼Œåˆ™è§£å†»å¯¹åº”çš„å±‚
        if unfreeze_from_layer is not None:
            unfrozen_count = 0
            for name, param in model.named_parameters():
                if 'text_encoder' in name:
                    # å½“å®Œå…¨è§£å†»æ—¶(unfreeze_from_layer=0)ï¼Œè§£å†»embeddings
                    if unfreeze_from_layer == 0 and 'embeddings' in name:
                        param.requires_grad = True
                        unfrozen_count += 1
                    
                    # è§£å†»æŒ‡å®šå±‚åŠå…¶ä¹‹åçš„æ‰€æœ‰å±‚
                    # BERTå‘½å: encoder.layer.X (X=0-11)
                    if 'encoder.layer.' in name:
                        try:
                            # æå–å±‚å·
                            parts = name.split('encoder.layer.')[1].split('.')
                            layer_num = int(parts[0])
                            
                            # éªŒè¯å±‚å·èŒƒå›´ (0-11)
                            if 0 <= layer_num <= 11 and layer_num >= unfreeze_from_layer:
                                param.requires_grad = True
                                unfrozen_count += 1
                        except (IndexError, ValueError) as e:
                            logging.warning(f"Could not parse BERT layer number from: {name}, error: {e}")
                            continue
                    
                    # è§£å†»poolerï¼ˆå¦‚æœå®Œå…¨è§£å†»ï¼‰
                    if unfreeze_from_layer == 0 and 'pooler' in name:
                        param.requires_grad = True
                        unfrozen_count += 1
            
            logging.info(f"BERT: Unfrozen {unfrozen_count} parameter groups from layer {unfreeze_from_layer}")
        else:
            logging.info(f"BERT: All layers frozen")
        
        # æ­¥éª¤3: ç»Ÿè®¡å¹¶è®°å½•å¯è®­ç»ƒå‚æ•°
        bert_trainable = sum(p.numel() for n, p in model.named_parameters() 
                            if p.requires_grad and 'text_encoder' in n)
        bert_total = sum(p.numel() for n, p in model.named_parameters() if 'text_encoder' in n)
        
        logging.info(f"BERT: {bert_trainable:,}/{bert_total:,} trainable ({100*bert_trainable/bert_total:.1f}%)")
    
    def freeze_vit_layers(self, model, unfreeze_from_layer=None):
        """
        å†»ç»“/è§£å†»ViTæˆ–Vimçš„æŒ‡å®šå±‚
        
        ViT-Base: 12å±‚ (encoder.layer.X)
        Vim-S: 24å±‚ (layers.X)
        
        Args:
            unfreeze_from_layer:
                å¯¹äºViT (12å±‚):
                - None: å…¨å†»ç»“
                - 8: è§£å†» 8-11 (å4å±‚)
                - 4: è§£å†» 4-11 (å8å±‚)
                - 0: å…¨è§£å†»
                
                å¯¹äºVim (24å±‚) - è‡ªåŠ¨æ˜ å°„:
                - None: å…¨å†»ç»“
                - 8 -> æ˜ å°„ä¸º 16 (è§£å†» 16-23, å8å±‚) -> ç­‰ç­‰ï¼Œä¿æŒæ¯”ä¾‹
                  ä¸ºäº†ç®€åŒ–é€»è¾‘ï¼Œæˆ‘ä»¬æŒ‰ç…§"åNå±‚"çš„æ¦‚å¿µ:
                  Stage 1 (ViTå4å±‚) -> Vimå4å±‚ (20-23)
                  Stage 2 (ViTå8å±‚) -> Vimå8å±‚ (16-23)
                  Stage 3 (ViTå8å±‚) -> Vimå12å±‚ (12-23) ? 
                  Let's align by ratio or fixed blocks.
                  
                  å®šä¹‰æ˜ å°„:
                  unfreeze_from_layer=8 (ViT 8/12, last 4) -> Vim 20/24 (last 4)
                  unfreeze_from_layer=4 (ViT 4/12, last 8) -> Vim 16/24 (last 8)
                  unfreeze_from_layer=0 (ViT 0/12, all)    -> Vim 0/24 (all)
        """
        is_vim = getattr(model, 'vision_backbone_type', 'vit') == 'vim'
        total_layers = 24 if is_vim else 12
        
        # ç¡®å®šè§£å†»çš„èµ·å§‹å±‚ç´¢å¼•
        target_start_layer = None
        if unfreeze_from_layer is not None:
            if unfreeze_from_layer == 0:
                target_start_layer = 0
            elif unfreeze_from_layer == 8: # ViTå4å±‚
                target_start_layer = total_layers - 4 # Vim: 20, ViT: 8
            elif unfreeze_from_layer == 4: # ViTå8å±‚
                target_start_layer = total_layers - 8 # Vim: 16, ViT: 4
            else:
                # é»˜è®¤æ˜ å°„
                target_start_layer = unfreeze_from_layer if not is_vim else unfreeze_from_layer * 2

        # æ­¥éª¤1: å…ˆå†»ç»“æ‰€æœ‰è§†è§‰ç¼–ç å™¨å‚æ•°
        for name, param in model.named_parameters():
            if 'visual_encoder' in name:
                param.requires_grad = False
                # æŠ•å½±å±‚å§‹ç»ˆè®­ç»ƒ
                if 'visual_proj' in name:
                    param.requires_grad = True
        
        # æ­¥éª¤2: è§£å†»æŒ‡å®šå±‚
        if target_start_layer is not None:
            unfrozen_count = 0
            for name, param in model.named_parameters():
                if 'visual_encoder' in name:
                    # è§£å†»embeddings (å½“å®Œå…¨è§£å†»æ—¶)
                    if target_start_layer == 0 and ('embeddings' in name or 'patch_embed' in name or 'cls_token' in name or 'pos_embed' in name):
                        param.requires_grad = True
                        unfrozen_count += 1
                        continue

                    # è¯†åˆ«å±‚å·
                    layer_num = -1
                    if is_vim:
                        # Vimå‘½å: visual_encoder.layers.X
                        if 'layers.' in name:
                            try:
                                parts = name.split('layers.')[1].split('.')
                                layer_num = int(parts[0])
                            except (IndexError, ValueError):
                                pass
                    else:
                        # ViTå‘½å: visual_encoder.encoder.layer.X
                        if 'encoder.layer.' in name:
                            try:
                                parts = name.split('encoder.layer.')[1].split('.')
                                layer_num = int(parts[0])
                            except (IndexError, ValueError):
                                pass
                    
                    # åˆ¤æ–­æ˜¯å¦è§£å†»
                    if layer_num != -1 and layer_num >= target_start_layer:
                        param.requires_grad = True
                        unfrozen_count += 1
                    
                    # è§£å†»æœ€åçš„Normå±‚ (å¦‚æœå®Œå…¨è§£å†»æˆ–éƒ¨åˆ†è§£å†»)
                    # Vim: norm_f, ViT: layernorm/pooler
                    if target_start_layer == 0 or target_start_layer < total_layers:
                        if is_vim and 'norm_f' in name:
                            param.requires_grad = True
                        elif not is_vim and ('layernorm' in name or 'pooler' in name):
                            param.requires_grad = True

            logging.info(f"{'Vim' if is_vim else 'ViT'}: Unfrozen params from layer {target_start_layer}/{total_layers}")
        else:
            logging.info(f"{'Vim' if is_vim else 'ViT'}: All layers frozen")
        
        # æ­¥éª¤3: ç»Ÿè®¡
        vit_trainable = sum(p.numel() for n, p in model.named_parameters() 
                           if p.requires_grad and 'visual_encoder' in n)
        vit_total = sum(p.numel() for n, p in model.named_parameters() if 'visual_encoder' in n)
        
        logging.info(f"Visual Encoder: {vit_trainable:,}/{vit_total:,} trainable ({100*vit_trainable/vit_total:.1f}%)")
        
        # æ€»ä½“ç»Ÿè®¡
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        logging.info(f"Overall: {trainable:,}/{total:,} trainable ({100*trainable/total:.1f}%)")
    
    def get_param_groups_with_diff_lr(self, model, base_lr, stage):
        """
        ä¸ºä¸åŒçš„æ¨¡å—è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ï¼ˆæ–¹æ¡ˆBï¼šæ¸è¿›è§£å†»ï¼‰
        æ”¯æŒ ViT (12å±‚) å’Œ Vim (24å±‚)
        
        Vim åˆ†ç»„æ˜ å°„:
        - Low: layers 0-7
        - Mid: layers 8-15
        - High: layers 16-23
        """
        is_vim = getattr(model, 'vision_backbone_type', 'vit') == 'vim'
        
        # åˆå§‹åŒ–å‚æ•°ç»„
        bert_embed_params = []
        bert_low_params = []     # BERT layer 0-3
        bert_mid_params = []     # BERT layer 4-7
        bert_high_params = []    # BERT layer 8-11
        bert_other_params = []
        
        vit_embed_params = []
        vit_low_params = []      # ViT 0-3 / Vim 0-7
        vit_mid_params = []      # ViT 4-7 / Vim 8-15
        vit_high_params = []     # ViT 8-11 / Vim 16-23
        vit_other_params = []
        
        task_params = []         # G-S3, Fusion, Classifier, Projectionç­‰
        
        for name, param in model.named_parameters():
            # ã€å…³é”®ä¿®å¤ã€‘è·³è¿‡å†»ç»“çš„å‚æ•°
            if not param.requires_grad:
                continue
            
            # å¤„ç†BERTå‚æ•°
            if 'text_encoder' in name:
                if 'embeddings' in name:
                    bert_embed_params.append(param)
                elif 'encoder.layer.' in name:
                    try:
                        parts = name.split('encoder.layer.')[1].split('.')
                        layer_num = int(parts[0])
                        
                        if 0 <= layer_num <= 3:
                            bert_low_params.append(param)
                        elif 4 <= layer_num <= 7:
                            bert_mid_params.append(param)
                        elif 8 <= layer_num <= 11:
                            bert_high_params.append(param)
                    except (IndexError, ValueError):
                        task_params.append(param)
                elif 'pooler' in name:
                    bert_other_params.append(param)
                else:
                    task_params.append(param)
            
            # å¤„ç†è§†è§‰ç¼–ç å™¨å‚æ•° (ViT or Vim)
            elif 'visual_encoder' in name:
                # Embeddings
                if 'embeddings' in name or 'patch_embed' in name or 'cls_token' in name or 'pos_embed' in name:
                    vit_embed_params.append(param)
                    continue
                    
                # Layers
                layer_num = -1
                if is_vim and 'layers.' in name: # Vim: layers.X
                    try:
                        parts = name.split('layers.')[1].split('.')
                        layer_num = int(parts[0])
                    except (IndexError, ValueError):
                        pass
                elif not is_vim and 'encoder.layer.' in name: # ViT: encoder.layer.X
                    try:
                        parts = name.split('encoder.layer.')[1].split('.')
                        layer_num = int(parts[0])
                    except (IndexError, ValueError):
                        pass
                
                if layer_num != -1:
                    if is_vim:
                        # Vim 24 layers grouping
                        if 0 <= layer_num <= 7:
                            vit_low_params.append(param)
                        elif 8 <= layer_num <= 15:
                            vit_mid_params.append(param)
                        elif 16 <= layer_num <= 23:
                            vit_high_params.append(param)
                        else:
                            vit_other_params.append(param)
                    else:
                        # ViT 12 layers grouping
                        if 0 <= layer_num <= 3:
                            vit_low_params.append(param)
                        elif 4 <= layer_num <= 7:
                            vit_mid_params.append(param)
                        elif 8 <= layer_num <= 11:
                            vit_high_params.append(param)
                        else:
                            vit_other_params.append(param)
                # Other parts (Norms etc.)
                elif 'layernorm' in name or 'pooler' in name or 'norm_f' in name:
                    vit_other_params.append(param)
                else:
                    # æ— æ³•åˆ†ç±»çš„è§†è§‰å‚æ•°æ”¾å…¥ task_params æˆ–å…¶ä»–
                    task_params.append(param)
            
            # å…¶ä»–å‚æ•°ï¼ˆä»»åŠ¡ç‰¹å®šæ¨¡å— + æŠ•å½±å±‚ï¼‰
            else:
                task_params.append(param)
        
        # æ ¹æ®è®­ç»ƒé˜¶æ®µè®¾ç½®å­¦ä¹ ç‡
        if stage == 1:  # Stage 1 (Epoch 1-10): ViTå4å±‚ + ä»»åŠ¡æ¨¡å—
            param_groups = [
                {'params': vit_high_params, 'lr': base_lr * 0.3, 'weight_decay': 0.0001, 'name': 'vit_high'},
                {'params': task_params, 'lr': base_lr * 1.0, 'weight_decay': 0.0001, 'name': 'task_modules'}
            ]
            logging.info(f"Stage 1 LR: vit_high={base_lr*0.3:.2e}, task={base_lr*1.0:.2e}")
            
        elif stage == 2:  # Stage 2 (Epoch 11-30): BERTå’ŒViTå4å±‚
            backbone_high_params = bert_high_params + vit_high_params
            param_groups = [
                {'params': backbone_high_params, 'lr': base_lr * 0.5, 'weight_decay': 0.0001, 'name': 'backbone_high'},
                {'params': task_params, 'lr': base_lr * 1.0, 'weight_decay': 0.0001, 'name': 'task_modules'}
            ]
            logging.info(f"Stage 2 LR: backbone_high={base_lr*0.5:.2e}, task={base_lr*1.0:.2e}")
            
        elif stage == 3:  # Stage 3 (Epoch 31-60): BERTå’ŒViTå8å±‚
            backbone_mid_high_params = bert_mid_params + bert_high_params + vit_mid_params + vit_high_params
            param_groups = [
                {'params': backbone_mid_high_params, 'lr': base_lr * 0.6, 'name': 'backbone_mid_high'},
                {'params': task_params, 'lr': base_lr * 1.0, 'name': 'task_modules'}
            ]
            logging.info(f"Stage 3 LR: backbone_mid_high={base_lr*0.6:.2e}, task={base_lr*1.0:.2e}")
            
        elif stage == 4:  # Stage 4 (Epoch 61-80): å…¨éƒ¨è§£å†»ï¼Œåˆ†å±‚å­¦ä¹ ç‡
            all_embed_params = bert_embed_params + bert_other_params + vit_embed_params + vit_other_params
            all_low_params = bert_low_params + vit_low_params
            all_mid_params = bert_mid_params + vit_mid_params
            all_high_params = bert_high_params + vit_high_params
            
            param_groups = [
                {'params': all_embed_params, 'lr': base_lr * 0.05, 'name': 'backbone_embed'},
                {'params': all_low_params, 'lr': base_lr * 0.2, 'name': 'backbone_low'},
                {'params': all_mid_params, 'lr': base_lr * 0.4, 'name': 'backbone_mid'},
                {'params': all_high_params, 'lr': base_lr * 0.6, 'name': 'backbone_high'},
                {'params': task_params, 'lr': base_lr * 0.8, 'name': 'task_modules'}
            ]
            logging.info(f"Stage 4 LR: embed={base_lr*0.05:.2e}, low={base_lr*0.2:.2e}, "
                        f"mid={base_lr*0.4:.2e}, high={base_lr*0.6:.2e}, task={base_lr*0.8:.2e}")
            
        else:
            raise ValueError(f"Invalid stage: {stage}. Must be 1-4.")
        
        # ç»Ÿè®¡æ¯ç»„å‚æ•°æ•°é‡
        for group in param_groups:
            num_params = sum(p.numel() for p in group['params'])
            logging.info(f"  {group['name']}: {num_params:,} parameters")
        
        return param_groups

    def build_optimizer(self, model, stage=1):
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼Œæ ¹æ®è®­ç»ƒé˜¶æ®µä½¿ç”¨ä¸åŒçš„å‚æ•°ç»„
        param_groups = self.get_param_groups_with_diff_lr(model, self.args.lr, stage)
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä¼˜åŒ–å™¨
        optimizer_type = self.args.optimizer.lower()
        if optimizer_type == 'adamw':
            return torch.optim.AdamW(param_groups, eps=1e-8, betas=(0.9, 0.999))
        elif optimizer_type == 'adam':
            return torch.optim.Adam(param_groups, eps=1e-8, betas=(0.9, 0.999))
        else:
            logging.warning(f"Unknown optimizer {self.args.optimizer}, defaulting to AdamW")
            return torch.optim.AdamW(param_groups, eps=1e-8, betas=(0.9, 0.999))

    def build_scheduler(self, optimizer):
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.args.scheduler == 'cosine':
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=self.args.epochs, eta_min=1e-6
            )
        return WarmupMultiStepLR(
            optimizer, self.args.milestones, gamma=0.1,
            warmup_factor=0.1, warmup_iters=self.args.warmup_step
        )

    def load_param(self, model, trained_path):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°
        param_dict = torch.load(trained_path, map_location=self.device, weights_only=False)
        param_dict = param_dict.get('state_dict', param_dict.get('model', param_dict))
        model_dict = model.state_dict()
        for i in param_dict:
            # è¿™é‡Œçš„å½¢çŠ¶æ£€æŸ¥éå¸¸å…³é”®ï¼šé¡ºåºè®­ç»ƒä¸åŒæ•°æ®é›†æ—¶ï¼ŒIDåˆ†ç±»å™¨(id_classifier)çš„ç»´åº¦ä¸åŒ
            # å½¢çŠ¶æ£€æŸ¥å¯ä»¥ç¡®ä¿ä¸åŠ è½½å½¢çŠ¶ä¸åŒ¹é…çš„åˆ†ç±»å¤´ï¼Œä»è€Œè®©æ–°é˜¶æ®µä»éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å™¨å¼€å§‹
            if i in model_dict and model_dict[i].shape == param_dict[i].shape:
                model_dict[i] = param_dict[i]
        model.load_state_dict(model_dict, strict=False)
        logging.info(f"Loaded pretrained weights from {trained_path}")

    def run(self):
        # æ‰§è¡Œè®­ç»ƒå’Œè¯„ä¼°æµç¨‹
        args = self.args
        config = self.config  # ç°åœ¨configä¸ºç©ºå­—å…¸ï¼Œä½†æˆ‘ä»¬ç›´æ¥ä½¿ç”¨argsä¸­çš„å‚æ•°
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

        # åˆ›å»ºä¸¤ä¸ªloggerï¼šä¸€ä¸ªç”¨äºè¯¦ç»†æ—¥å¿—ï¼Œä¸€ä¸ªç”¨äºé‡è¦ä¿¡æ¯æ˜¾ç¤º
        detailed_logger = logging.getLogger('detailed')
        detailed_logger.setLevel(logging.DEBUG)

        # æ¸…é™¤å·²æœ‰å¤„ç†å™¨
        for handler in detailed_logger.handlers[:]:
            detailed_logger.removeHandler(handler)

        # ä¸ºdetailed_loggeræ·»åŠ å¤„ç†å™¨ï¼Œå†™å…¥æ•°æ®é›†ç‰¹å®šçš„æ—¥å¿—æ–‡ä»¶
        # è·å–æ•°æ®é›†åç§°ä»¥ç¡®å®šæ—¥å¿—æ–‡ä»¶ä½ç½®
        if hasattr(args, 'dataset_configs') and args.dataset_configs:
            dataset_full_name = args.dataset_configs[0]['name'].lower()
            if 'cuhk' in dataset_full_name:
                dataset_dir_name = 'cuhk'
            elif 'rstp' in dataset_full_name:
                dataset_dir_name = 'rstp'
            elif 'icfg' in dataset_full_name:
                dataset_dir_name = 'icfg'
            else:
                dataset_dir_name = dataset_full_name
        else:
            dataset_dir_name = 'unknown'

        # ç¡®ä¿ä½¿ç”¨ log ç›®å½•è€Œä¸æ˜¯ logs ç›®å½•
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = Path(__file__).parent  # scripts/
        project_root = script_dir.parent   # é¡¹ç›®æ ¹ç›®å½•
        log_base_dir = project_root / 'log'
        dataset_log_dir = log_base_dir / dataset_dir_name
        dataset_log_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ•°æ®é›†ç‰¹å®šçš„ä¸»è¦æ—¥å¿—æ–‡ä»¶
        main_log_file = dataset_log_dir / 'log.txt'

        # ä¸ºdetailed_loggeræ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(main_log_file, mode='a', encoding='utf-8')
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        detailed_logger.addHandler(file_handler)

        # æ§åˆ¶å°logger - åªæ˜¾ç¤ºé‡è¦ä¿¡æ¯
        console_logger = logging.getLogger('console')
        console_logger.setLevel(logging.INFO)

        # æ¸…é™¤å·²æœ‰å¤„ç†å™¨
        for handler in console_logger.handlers[:]:
            console_logger.removeHandler(handler)

        # æ§åˆ¶å°å¤„ç†å™¨ - åªæ˜¾ç¤ºé‡è¦ä¿¡æ¯
        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = logging.Formatter('%(message)s')  # ç®€åŒ–æ ¼å¼ï¼Œåªæ˜¾ç¤ºæ¶ˆæ¯
        console_handler.setFormatter(console_formatter)
        console_logger.addHandler(console_handler)

        # è®¾ç½®åŸºç¡€æ—¥å¿—é…ç½®ï¼Œå°†è°ƒè¯•ä¿¡æ¯å†™å…¥æ•°æ®é›†ç‰¹å®šçš„æ—¥å¿—æ–‡ä»¶
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = Path(__file__).parent  # scripts/
        project_root = script_dir.parent   # é¡¹ç›®æ ¹ç›®å½•
        log_base_dir = project_root / 'log'
        dataset_log_dir = log_base_dir / dataset_dir_name
        dataset_log_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ•°æ®é›†ç‰¹å®šçš„è°ƒè¯•æ—¥å¿—æ–‡ä»¶
        debug_log_file = dataset_log_dir / 'debug.txt'

        # è®¾ç½®åŸºç¡€æ—¥å¿—é…ç½®ï¼Œå°†è°ƒè¯•ä¿¡æ¯å†™å…¥æ•°æ®é›†ç‰¹å®šçš„è°ƒè¯•æ—¥å¿—æ–‡ä»¶
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(debug_log_file, mode='a', encoding='utf-8'),  # å†™å…¥è°ƒè¯•æ—¥å¿—æ–‡ä»¶
                logging.StreamHandler(sys.stdout)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¯ä»¥ç§»é™¤æ­¤é¡¹ä»¥ä»…å†™å…¥æ–‡ä»¶ï¼‰
            ]
        )

        # ä½†ä¸ºäº†æ»¡è¶³è¦æ±‚ï¼Œåªå°†è°ƒè¯•ä¿¡æ¯å†™å…¥æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°
        # é‡æ–°é…ç½®ï¼Œåªå†™å…¥æ–‡ä»¶
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)

        file_handler = logging.FileHandler(debug_log_file, mode='a', encoding='utf-8')
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        logging.root.addHandler(file_handler)
        logging.root.setLevel(logging.DEBUG)

        # æ„å»ºæ•°æ®é›†
        console_logger.info("Building dataset...")
        data_builder = DataBuilder(args, is_distributed=False)
        args.num_classes = data_builder.get_num_classes()
        detailed_logger.info(f"Set num_classes = {args.num_classes}")

        console_logger.info("Loading training data...")
        train_loader, _ = data_builder.build_data(is_train=True)
        console_logger.info("Loading query and gallery data...")
        query_loader, gallery_loader = data_builder.build_data(is_train=False)
        console_logger.info(f"Train data size: {len(train_loader.dataset.data)}")
        console_logger.info(f"Query data size: {len(query_loader.dataset.data)}")
        console_logger.info(f"Gallery data size: {len(gallery_loader.dataset.data)}")
        detailed_logger.info(f"Train data size: {len(train_loader.dataset.data)}")
        detailed_logger.info(f"Query data size: {len(query_loader.dataset.data)}")


        # æ„å»ºæ¨¡å‹é…ç½®å­—å…¸
        model_config = {
            'bert_base_path': args.bert_base_path,
            'vit_pretrained': args.vit_pretrained,
            'vision_backbone': args.vision_backbone,
            'vim_pretrained': args.vim_pretrained,
            'num_classes': args.num_classes,
            'disentangle_type': args.disentangle_type,
            'gs3': {
                'num_heads': args.gs3_num_heads,
                'd_state': args.gs3_d_state,
                'd_conv': args.gs3_d_conv,
                'dropout': args.gs3_dropout
            },
            'fusion': {
                'type': args.fusion_type,
                'dim': args.fusion_dim,
                'd_state': args.fusion_d_state,
                'd_conv': args.fusion_d_conv,
                'num_layers': args.fusion_num_layers,
                'output_dim': args.fusion_output_dim,
                'dropout': args.fusion_dropout
            }
        }

        # åˆå§‹åŒ–æ¨¡å‹
        console_logger.info("Initializing model...")
        model = Model(net_config=model_config).to(self.device)
        detailed_logger.info(f"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters")

        # è®°å½•è®­ç»ƒå¼€å§‹ä¿¡æ¯
        self.monitor.log_training_start(model, args)

        # [ä¿®æ”¹ç‚¹ 2] å¦‚æœæŒ‡å®šäº† finetune-fromï¼Œåˆ™åœ¨æ„å»ºä¼˜åŒ–å™¨ä¹‹å‰åŠ è½½æƒé‡
        if args.finetune_from:
            detailed_logger.info(f"Finetuning: Loading checkpoint from {args.finetune_from}")
            console_logger.info(f"Loading checkpoint from {args.finetune_from}")
            self.load_param(model, args.finetune_from)

        # ã€æ–¹æ¡ˆBï¼šæ¸è¿›è§£å†»ç­–ç•¥ - Stage 1ã€‘Epoch 1-10: è§£å†»ViTå4å±‚ (å…³é”®ä¿®å¤!)
        console_logger.info("=" * 70)
        console_logger.info("ğŸ”“ Progressive Unfreezing Strategy - Solution B")
        console_logger.info("=" * 70)
        console_logger.info("Stage 1 (Epoch 1-10): Unfreeze ViT last 4 layers (layer 8-11)")
        console_logger.info("                      Keep ViT first 8 layers frozen (layer 0-7)")
        console_logger.info("                      Keep all BERT layers frozen")
        console_logger.info("")
        console_logger.info("ğŸ¯ Key Fix: Let CLS loss backpropagate through ViT!")
        console_logger.info("   - id_embeds will update via ViT gradients")
        console_logger.info("   - Classification head can learn properly")
        console_logger.info("=" * 70)
        
        # å†»ç»“ç­–ç•¥ï¼šåªè§£å†»ViTå4å±‚ï¼Œå…¶ä½™å…¨éƒ¨å†»ç»“
        self.freeze_bert_layers(model, unfreeze_from_layer=None)  # å…¨éƒ¨å†»ç»“
        self.freeze_vit_layers(model, unfreeze_from_layer=8)      # è§£å†»layer 8-11

        # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        console_logger.info("Building optimizer and scheduler...")
        optimizer = self.build_optimizer(model, stage=1)
        lr_scheduler = self.build_scheduler(optimizer)
        
        # ã€å…³é”®éªŒè¯ã€‘ç¡®ä¿freezeç”Ÿæ•ˆ
        console_logger.info("\nğŸ” Verifying freeze status after optimizer build...")
        self.verify_freeze_status(model)

        # è®­ç»ƒæ¨¡å‹
        console_logger.info("Starting training...")
        trainer = Trainer(model, args, self.monitor, runner=self)  # ä¼ é€’runnerå¼•ç”¨ä»¥ä¾¿è°ƒç”¨freezeæ–¹æ³•
        trainer.train(
            train_loader, optimizer, lr_scheduler, query_loader, gallery_loader, checkpoint_dir=args.logs_dir
        )

        # è¯„ä¼°æ¨¡å‹ - ç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¸å†ä¿å­˜å’ŒåŠ è½½æœ€ç»ˆæ£€æŸ¥ç‚¹
        console_logger.info("Evaluating model...")
        from evaluators.evaluator import Evaluator
        # ç›´æ¥ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæ— éœ€ä¿å­˜å’Œé‡æ–°åŠ è½½
        evaluator = Evaluator(model, args=args)
        metrics = evaluator.evaluate(
            query_loader, gallery_loader, query_loader.dataset.data,
            gallery_loader.dataset.data, checkpoint_path=None
        )
        console_logger.info(f"Evaluation Results: {metrics}")
        detailed_logger.info(f"Evaluation Results: {metrics}")
        # è®°å½•è®­ç»ƒç»“æŸä¿¡æ¯
        self.monitor.log_training_end(metrics)

if __name__ == '__main__':
    args, config = configuration()
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    cudnn.deterministic = True
    cudnn.benchmark = False
    runner = Runner(args, config)
    runner.run()