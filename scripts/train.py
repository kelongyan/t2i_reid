# scripts/train.py
import argparse
import ast
import gc
import logging
import random
import sys
from pathlib import Path
import torch
from torch.backends import cudnn
from torch.cuda.amp import GradScaler

# è®¾ç½®æ ¹ç›®å½•å¹¶æ·»åŠ åˆ°è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥å†…éƒ¨æ¨¡å—
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from utils.serialization import save_checkpoint
from models.model import Model
from datasets.data_builder import DataBuilder
from trainers.trainer import Trainer
from utils.lr_scheduler import WarmupMultiStepLR
from utils.monitor import get_monitor_for_dataset

def configuration():
    # é…ç½®å‘½ä»¤è¡Œå‚æ•°ï¼Œå®šä¹‰è®­ç»ƒæ‰€éœ€çš„è¶…å‚æ•°ã€è·¯å¾„å’Œæ¨¡å‹ç»„ä»¶é€‰é¡¹
    parser = argparse.ArgumentParser(description="Train T2I-ReID model (CLIP Upgrade)")
    parser.add_argument('--root', type=str, default=str(ROOT_DIR / 'datasets'),
                       help='Root directory of the dataset')
    parser.add_argument('--dataset-configs', nargs='+', type=str, help='List of dataset configurations in JSON format')
    parser.add_argument('--loss-weights', type=str, help='Loss weights in JSON format')
    parser.add_argument('-b', '--batch-size', type=int, default=128, help='Batch size for training')
    parser.add_argument('-j', '--workers', type=int, default=4, help='Number of data loading workers')
    parser.add_argument('--height', type=int, default=224, help='Image height')
    parser.add_argument('--width', type=int, default=224, help='Image width')
    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=0.001, help='Weight decay')
    parser.add_argument('--warmup-step', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--milestones', nargs='+', type=int, default=[40, 60], help='Milestones for LR scheduler')
    parser.add_argument('--epochs', type=int, default=80, help='Number of training epochs')
    parser.add_argument('--seed', type=int, default=0, help='Random seed')
    parser.add_argument('--print-freq', type=int, default=50, help='Print frequency')
    parser.add_argument('--fp16', action='store_true', help='Use mixed precision training')
    
    # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„é…ç½®
    parser.add_argument('--clip-pretrained', type=str, 
                       default=str(ROOT_DIR / 'pretrained' / 'clip-vit-base-patch16'),
                       help='Path to CLIP text encoder model')
    parser.add_argument('--vit-pretrained', type=str, default=str(ROOT_DIR / 'pretrained' / 'vit-base-patch16-224'),
                       help='Path to ViT model')
    parser.add_argument('--vision-backbone', type=str, default='vim', choices=['vit', 'vim'],
                       help='Vision backbone type: vit or vim')
    parser.add_argument('--vim-pretrained', type=str, default=str(ROOT_DIR / 'pretrained' / 'Vision Mamba' / 'vim_s_midclstok.pth'),
                       help='Path to Vision Mamba model')
    parser.add_argument('--logs-dir', type=str, default=str(ROOT_DIR / 'log'), help='Directory for logs')
    parser.add_argument('--num-classes', type=int, default=8000, help='Number of identity classes')

    # èåˆæ¨¡å—é…ç½®å‚æ•°
    parser.add_argument('--fusion-type', type=str, default='enhanced_mamba', help='Type of fusion module')
    parser.add_argument('--fusion-dim', type=int, default=256, help='Fusion module dimension')
    parser.add_argument('--fusion-d-state', type=int, default=16, help='Fusion module d_state')
    parser.add_argument('--fusion-d-conv', type=int, default=4, help='Fusion module d_conv')
    parser.add_argument('--fusion-num-layers', type=int, default=2, help='Fusion module number of layers')
    parser.add_argument('--fusion-output-dim', type=int, default=256, help='Fusion module output dimension')
    parser.add_argument('--fusion-dropout', type=float, default=0.1, help='Fusion module dropout')

    # è§£è€¦æ¨¡å—ï¼ˆAH-Net/G-S3ï¼‰ç›¸å…³å‚æ•°
    parser.add_argument('--id-projection-dim', type=int, default=768, help='ID projection dimension')
    parser.add_argument('--cloth-projection-dim', type=int, default=768, help='Cloth projection dimension')
    parser.add_argument('--gs3-num-heads', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--gs3-d-state', type=int, default=16, help='State dimension for G-S3')
    parser.add_argument('--gs3-d-conv', type=int, default=4, help='Conv kernel size for G-S3')
    parser.add_argument('--gs3-dropout', type=float, default=0.1, help='Dropout rate for G-S3')
    parser.add_argument('--gs3-img-size', nargs=2, type=int, default=[14, 14], help='Image patch grid size')

    # å„é¡¹æŸå¤±å‡½æ•°æƒé‡åˆå§‹å€¼
    parser.add_argument('--loss-info-nce', type=float, default=1.0, help='InfoNCE loss weight')
    parser.add_argument('--loss-id-triplet', type=float, default=2.0, help='ID Triplet loss weight')
    parser.add_argument('--loss-cloth-semantic', type=float, default=0.1, help='Cloth semantic loss weight')
    parser.add_argument('--loss-spatial-orthogonal', type=float, default=0.0, help='Spatial Orthogonal loss weight')
    parser.add_argument('--loss-semantic-alignment', type=float, default=0.0, help='Semantic Alignment loss weight')
    parser.add_argument('--loss-ortho-reg', type=float, default=0.0, help='Query Orthogonality weight')

    # å¯¹æŠ—æ€§è§£è€¦æŸå¤±æƒé‡
    parser.add_argument('--loss-adversarial-attr', type=float, default=0.0, help='Adversarial Attribute weight')
    parser.add_argument('--loss-adversarial-domain', type=float, default=0.0, help='Adversarial Domain weight')
    parser.add_argument('--loss-discriminator-attr', type=float, default=0.0, help='Discriminator Attribute weight')
    parser.add_argument('--loss-discriminator-domain', type=float, default=0.0, help='Discriminator Domain weight')

    # å¯è§†åŒ–ç›¸å…³é…ç½®
    parser.add_argument('--visualization-enabled', action='store_true', help='Enable visualization')
    parser.add_argument('--visualization-save-dir', type=str, default='visualizations', help='Dir to save visualizations')
    parser.add_argument('--visualization-frequency', type=int, default=5, help='Frequency to save visualizations')
    parser.add_argument('--visualization-batch-interval', type=int, default=200, help='Batch interval for visualizations')

    # ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨é…ç½®
    parser.add_argument('--optimizer', type=str, default='Adam', help='Optimizer type')
    parser.add_argument('--scheduler', type=str, default='cosine', help='Scheduler type')
    parser.add_argument('--finetune-from', type=str, help='Checkpoint path to finetune from')

    args = parser.parse_args()

    # èšåˆæŸå¤±æƒé‡åˆ° disentangle å­—å…¸
    args.disentangle = {}
    if args.loss_weights:
        args.disentangle['loss_weights'] = ast.literal_eval(args.loss_weights)
    else:
        args.disentangle['loss_weights'] = {
            'info_nce': args.loss_info_nce,
            'cloth_semantic': args.loss_cloth_semantic,
            'id_triplet': args.loss_id_triplet,
            'spatial_orthogonal': args.loss_spatial_orthogonal,
            'semantic_alignment': args.loss_semantic_alignment,
            'ortho_reg': args.loss_ortho_reg,
            'adversarial_attr': args.loss_adversarial_attr,
            'adversarial_domain': args.loss_adversarial_domain,
            'discriminator_attr': args.loss_discriminator_attr,
            'discriminator_domain': args.loss_discriminator_domain
        }
    
    # èšåˆå¯è§†åŒ–é…ç½®
    args.visualization = {
        'enabled': args.visualization_enabled,
        'save_dir': args.visualization_save_dir,
        'frequency': args.visualization_frequency,
        'batch_interval': args.visualization_batch_interval
    }

    # å¤„ç†å¤šæ•°æ®é›†é…ç½®
    if args.dataset_configs:
        dataset_configs = []
        for cfg in args.dataset_configs:
            parsed = ast.literal_eval(cfg)
            dataset_configs.extend(parsed if isinstance(parsed, list) else [parsed])
        args.dataset_configs = dataset_configs
    else:
        args.dataset_configs = [{
            'name': 'CUHK-PEDES',
            'root': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES'),
            'json_file': str(ROOT_DIR / 'datasets' / 'CUHK-PEDES' / 'annotations' / 'caption_all.json')
        }]

    # è§„èŒƒåŒ–æ‰€æœ‰è·¯å¾„
    args.clip_pretrained = str(Path(args.clip_pretrained))
    args.vit_pretrained = str(Path(args.vit_pretrained))
    args.logs_dir = str(Path(args.logs_dir))
    args.root = str(Path(args.root))

    if not Path(args.vit_pretrained).exists():
        raise FileNotFoundError(f"ViT base path not found at: {args.vit_pretrained}")

    args.img_size = (args.height, args.width)
    args.task_name = 't2i'
    return args, {}


class Runner:
    # è®­ç»ƒè¿è¡Œå™¨ï¼šç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸã€å­¦ä¹ ç‡è°ƒåº¦ã€å‚æ•°å†»ç»“ä»¥åŠæ—¥å¿—ç›‘æ§
    def __init__(self, args, config):
        self.args = args
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = torch.amp.GradScaler('cuda', enabled=args.fp16) if self.device.type == 'cuda' else None
        self.args.original_logs_dir = args.logs_dir

        # æ•°æ®é›†åç§°æ˜ å°„ï¼Œç”¨äºè·å–å¯¹åº”çš„ç›‘æ§å™¨
        if hasattr(args, 'dataset_configs') and args.dataset_configs:
            dataset_full_name = args.dataset_configs[0]['name'].lower()
            if 'cuhk' in dataset_full_name: dataset_name = 'cuhk_pedes'
            elif 'rstp' in dataset_full_name: dataset_name = 'rstp'
            elif 'icfg' in dataset_full_name: dataset_name = 'icfg'
            else: dataset_name = dataset_full_name
        else:
            dataset_name = 'unknown'
        
        project_root = Path(__file__).parent.parent
        log_base_dir = str(project_root / 'log')
        self.monitor = get_monitor_for_dataset(dataset_name, log_base_dir)

    def verify_freeze_status(self, model):
        # éªŒè¯å¹¶æ‰“å°æ¨¡å‹å„éƒ¨åˆ†çš„å†»ç»“/å¯è®­ç»ƒå‚æ•°çŠ¶æ€
        vit_frozen = sum(p.numel() for n, p in model.named_parameters() if 'visual_encoder' in n and not p.requires_grad)
        vit_total = sum(p.numel() for n, p in model.named_parameters() if 'visual_encoder' in n)
        vit_trainable = vit_total - vit_frozen
        
        text_frozen = sum(p.numel() for n, p in model.named_parameters() if 'text_encoder' in n and not p.requires_grad)
        text_total = sum(p.numel() for n, p in model.named_parameters() if 'text_encoder' in n)
        text_trainable = text_total - text_frozen
        
        adapter_trainable = sum(p.numel() for n, p in model.named_parameters() if 'text_proj' in n and p.requires_grad)
        adapter_total = sum(p.numel() for n, p in model.named_parameters() if 'text_proj' in n)
        
        task_trainable = sum(p.numel() for n, p in model.named_parameters() 
                            if 'visual_encoder' not in n and 'text_encoder' not in n and 'text_proj' not in n and p.requires_grad)
        
        total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        
        logging.info("=" * 70)
        logging.info("ğŸ“Š æ¨¡å‹å†»ç»“çŠ¶æ€éªŒè¯")
        logging.info("=" * 70)
        logging.info(f"è§†è§‰éª¨å¹²: {vit_trainable:,}/{vit_total:,} å¯è®­ç»ƒ ({100*vit_trainable/(vit_total+1):.1f}%)")
        logging.info(f"æ–‡æœ¬éª¨å¹² (CLIP): {text_trainable:,}/{text_total:,} å¯è®­ç»ƒ ({100*text_trainable/(text_total+1):.1f}%)")
        logging.info(f"æ–‡æœ¬é€‚é…å™¨: {adapter_trainable:,}/{adapter_total:,} å¯è®­ç»ƒ")
        logging.info(f"ä»»åŠ¡ç‰¹å®šæ¨¡å—: {task_trainable:,} å¯è®­ç»ƒ")
        logging.info(f"æ€»è®¡å¯è®­ç»ƒ: {total_trainable:,}/{total_params:,}")
        logging.info("=" * 70)
        return {}
    
    def freeze_text_layers(self, model, unfreeze_from_layer=None):
        # å†»ç»“ CLIP æ–‡æœ¬ç¼–ç å™¨ã€‚unfreeze_from_layer å®šä¹‰ä»å“ªä¸€å±‚å¼€å§‹è§£å†»ã€‚
        for name, param in model.named_parameters():
            if 'text_encoder' in name:
                param.requires_grad = False
            if 'text_proj' in name: # é€‚é…å™¨å±‚å§‹ç»ˆå¯è®­ç»ƒ
                param.requires_grad = True
                
        if unfreeze_from_layer is not None:
            unfrozen_count = 0
            for name, param in model.named_parameters():
                if 'text_encoder' in name:
                    if 'layers.' in name:
                        try:
                            layer_num = int(name.split('layers.')[1].split('.')[0])
                            if layer_num >= unfreeze_from_layer:
                                param.requires_grad = True
                                unfrozen_count += 1
                        except: pass
                    if 'final_layer_norm' in name:
                        param.requires_grad = True
                        unfrozen_count += 1
                    if unfreeze_from_layer == 0 and ('embeddings' in name):
                        param.requires_grad = True
                        unfrozen_count += 1
            logging.info(f"CLIP: å·²è§£å†»ä»ç¬¬ {unfreeze_from_layer} å±‚èµ·çš„å‚æ•° (å…± {unfrozen_count} ç»„)")
        else:
            logging.info(f"CLIP: æ‰€æœ‰å±‚å·²å†»ç»“ (é€‚é…å™¨ä¿æŒå¯è®­ç»ƒ)")

    def freeze_vit_layers(self, model, unfreeze_from_layer=None):
        # å†»ç»“è§†è§‰ç¼–ç å™¨ï¼ˆViT/Vimï¼‰ã€‚å¤„ç†é€»è¾‘ä¸æ–‡æœ¬åˆ†æ”¯ç±»ä¼¼ã€‚
        is_vim = getattr(model, 'vision_backbone_type', 'vit') == 'vim'
        total_layers = 24 if is_vim else 12
        
        target_start_layer = None
        if unfreeze_from_layer is not None:
            if unfreeze_from_layer == 0: target_start_layer = 0
            elif unfreeze_from_layer == 8: target_start_layer = total_layers - 4 
            elif unfreeze_from_layer == 4: target_start_layer = total_layers - 8
            else: target_start_layer = unfreeze_from_layer if not is_vim else unfreeze_from_layer * 2

        for name, param in model.named_parameters():
            if 'visual_encoder' in name:
                param.requires_grad = False
                if 'visual_proj' in name: param.requires_grad = True
        
        if target_start_layer is not None:
            for name, param in model.named_parameters():
                if 'visual_encoder' in name:
                    if target_start_layer == 0 and any(k in name for k in ['embeddings', 'patch_embed', 'cls_token', 'pos_embed']):
                        param.requires_grad = True
                        continue
                    layer_num = -1
                    if is_vim and 'layers.' in name:
                        try: layer_num = int(name.split('layers.')[1].split('.')[0])
                        except: pass
                    elif not is_vim and 'encoder.layer.' in name:
                        try: layer_num = int(name.split('encoder.layer.')[1].split('.')[0])
                        except: pass
                    if layer_num != -1 and layer_num >= target_start_layer:
                        param.requires_grad = True
                    if target_start_layer == 0 or target_start_layer < total_layers:
                        if is_vim and 'norm_f' in name: param.requires_grad = True
                        elif not is_vim and ('layernorm' in name or 'pooler' in name): param.requires_grad = True
            logging.info(f"{'Vim' if is_vim else 'ViT'}: å·²è§£å†»ä»ç¬¬ {target_start_layer}/{total_layers} å±‚èµ·çš„å‚æ•°")
        else:
            logging.info(f"{'Vim' if is_vim else 'ViT'}: æ‰€æœ‰å±‚å·²å†»ç»“")

    def get_param_groups_with_diff_lr(self, model, base_lr, stage):
        # å®ç°åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥ï¼šæ•æ„Ÿçš„éª¨å¹²ç½‘ç»œä½¿ç”¨æä½å­¦ä¹ ç‡ï¼Œä»»åŠ¡ç›¸å…³æ¨¡å—ä½¿ç”¨å…¨é€Ÿå­¦ä¹ ç‡
        is_vim = getattr(model, 'vision_backbone_type', 'vit') == 'vim'
        clip_params, text_adapter_params, vit_low_params, vit_mid_params, vit_high_params, vit_embed_params, task_params = [], [], [], [], [], [], []
        
        for name, param in model.named_parameters():
            if not param.requires_grad: continue
            if 'text_encoder' in name: clip_params.append(param)
            elif 'text_proj' in name: text_adapter_params.append(param)
            elif 'visual_encoder' in name:
                if any(k in name for k in ['embeddings', 'patch_embed', 'cls_token', 'pos_embed']): vit_embed_params.append(param)
                else:
                    layer_num = -1
                    if is_vim and 'layers.' in name:
                        try: layer_num = int(name.split('layers.')[1].split('.')[0])
                        except: pass
                    elif not is_vim and 'encoder.layer.' in name:
                        try: layer_num = int(name.split('encoder.layer.')[1].split('.')[0])
                        except: pass
                    if layer_num != -1:
                        if is_vim:
                            if layer_num < 8: vit_low_params.append(param)
                            elif layer_num < 16: vit_mid_params.append(param)
                            else: vit_high_params.append(param)
                        else:
                            if layer_num < 4: vit_low_params.append(param)
                            elif layer_num < 8: vit_mid_params.append(param)
                            else: vit_high_params.append(param)
                    elif any(k in name for k in ['layernorm', 'pooler', 'norm_f']): vit_high_params.append(param)
                    else: task_params.append(param)
            else: task_params.append(param)
        
        clip_lr_ratio = 0.05
        groups = [
            {'params': task_params, 'lr': base_lr, 'name': 'task_modules'},
            {'params': text_adapter_params, 'lr': base_lr, 'name': 'text_adapter'}
        ]
        if vit_embed_params: groups.append({'params': vit_embed_params, 'lr': base_lr * 0.01, 'name': 'vit_embed'})
        if vit_low_params: groups.append({'params': vit_low_params, 'lr': base_lr * 0.05, 'name': 'vit_low'})
        if vit_mid_params: groups.append({'params': vit_mid_params, 'lr': base_lr * 0.1, 'name': 'vit_mid'})
        if vit_high_params: groups.append({'params': vit_high_params, 'lr': base_lr * 0.2, 'name': 'vit_high'})
        if clip_params: groups.append({'params': clip_params, 'lr': base_lr * clip_lr_ratio, 'name': 'clip_encoder'})
        return groups

    def build_optimizer(self, model, stage=1):
        # æ ¹æ®é…ç½®æ„å»ºä¼˜åŒ–å™¨ï¼Œæ”¯æŒ AdamW å’Œ Adam
        param_groups = self.get_param_groups_with_diff_lr(model, self.args.lr, stage)
        optimizer_type = self.args.optimizer.lower()
        if optimizer_type == 'adamw':
            return torch.optim.AdamW(param_groups, eps=1e-8, betas=(0.9, 0.999), weight_decay=self.args.weight_decay)
        elif optimizer_type == 'adam':
            return torch.optim.Adam(param_groups, eps=1e-8, betas=(0.9, 0.999))
        return torch.optim.AdamW(param_groups, eps=1e-8, betas=(0.9, 0.999))

    def build_scheduler(self, optimizer):
        # æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ”¯æŒä½™å¼¦é€€ç«å’Œå¸¦é¢„çƒ­çš„å¤šæ­¥ä¸‹é™
        if self.args.scheduler == 'cosine':
            return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.args.epochs, eta_min=1e-6)
        return WarmupMultiStepLR(optimizer, self.args.milestones, gamma=0.1, warmup_factor=0.1, warmup_iters=self.args.warmup_step)

    def load_param(self, model, trained_path):
        # åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Œå¹¶è¿‡æ»¤ç»´åº¦ä¸åŒ¹é…çš„å±‚
        param_dict = torch.load(trained_path, map_location=self.device, weights_only=False)
        param_dict = param_dict.get('state_dict', param_dict.get('model', param_dict))
        model_dict = model.state_dict()
        for i in param_dict:
            if i in model_dict and model_dict[i].shape == param_dict[i].shape:
                model_dict[i] = param_dict[i]
        model.load_state_dict(model_dict, strict=False)
        logging.info(f"å·²ä» {trained_path} åŠ è½½é¢„è®­ç»ƒæƒé‡")

    def run(self):
        # è®­ç»ƒä¸»å¾ªç¯å…¥å£ï¼šåˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿã€æ„å»ºæ•°æ®æµã€å®ä¾‹åŒ–æ¨¡å‹ã€åº”ç”¨å†»ç»“ç­–ç•¥å¹¶å¯åŠ¨è®­ç»ƒ
        args = self.args
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

        for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)
        dataset_log_dir = self.monitor.dataset_log_dir

        # 1. è¯¦ç»†æ—¥å¿—ï¼ˆæ–‡ä»¶ï¼‰
        detailed_logger = logging.getLogger('detailed')
        detailed_logger.setLevel(logging.DEBUG)
        detailed_logger.propagate = False
        file_handler = logging.FileHandler(dataset_log_dir / 'log.txt', mode='a', encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        detailed_logger.addHandler(file_handler)

        # 2. è°ƒè¯•æ—¥å¿—ï¼ˆæ–‡ä»¶ï¼‰
        root_file_handler = logging.FileHandler(dataset_log_dir / 'debug.txt', mode='a', encoding='utf-8')
        root_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logging.root.addHandler(root_file_handler)
        logging.root.setLevel(logging.DEBUG)

        # 3. æ§åˆ¶å°æ—¥å¿—ï¼ˆç»ˆç«¯ï¼‰
        console_logger = logging.getLogger('console')
        console_logger.setLevel(logging.INFO)
        console_logger.propagate = False
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(logging.Formatter('%(message)s'))
        console_logger.addHandler(console_handler)

        console_logger.info("æ­£åœ¨æ„å»ºæ•°æ®é›†...")
        data_builder = DataBuilder(args, is_distributed=False)
        args.num_classes = data_builder.get_num_classes()
        
        console_logger.info("æ­£åœ¨åŠ è½½è®­ç»ƒä¸æµ‹è¯•æ•°æ®...")
        train_loader, _ = data_builder.build_data(is_train=True)
        query_loader, gallery_loader = data_builder.build_data(is_train=False)

        # åˆå§‹åŒ–æ¨¡å‹æ¶æ„ï¼ˆCLIP + Vimï¼‰
        model_config = {
            'clip_pretrained': args.clip_pretrained,
            'vit_pretrained': args.vit_pretrained,
            'vision_backbone': args.vision_backbone,
            'vim_pretrained': args.vim_pretrained,
            'img_size': (args.height, args.width),
            'num_classes': args.num_classes,
            'gs3': {
                'num_heads': args.gs3_num_heads,
                'd_state': args.gs3_d_state,
                'd_conv': args.gs3_d_conv,
                'dropout': args.gs3_dropout,
                'img_size': tuple(args.gs3_img_size)
            },
            'fusion': {
                'type': args.fusion_type,
                'dim': args.fusion_dim,
                'd_state': args.fusion_d_state,
                'd_conv': args.fusion_d_conv,
                'num_layers': args.fusion_num_layers,
                'output_dim': args.fusion_output_dim,
                'dropout': args.fusion_dropout
            }
        }

        console_logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ç»“æ„...")
        model = Model(net_config=model_config).to(self.device)
        if args.finetune_from: self.load_param(model, args.finetune_from)

        # åº”ç”¨é¢„çƒ­å†»ç»“ç­–ç•¥ï¼Œé˜²æ­¢è®­ç»ƒåˆæœŸæ¢¯åº¦ä¸ç¨³å®š
        console_logger.info("=" * 60)
        console_logger.info("ğŸš€ è®­ç»ƒå¯åŠ¨: CLIP + Vim æ··åˆæ¶æ„")
        console_logger.info("   â„ï¸  ç­–ç•¥: åˆå§‹é˜¶æ®µå†»ç»“éª¨å¹²ç½‘ç»œ (Epoch 0-5)")
        console_logger.info("=" * 60)
        
        self.freeze_text_layers(model, unfreeze_from_layer=None)
        self.freeze_vit_layers(model, unfreeze_from_layer=None)

        optimizer = self.build_optimizer(model, stage=1)
        lr_scheduler = self.build_scheduler(optimizer)
        self.verify_freeze_status(model)

        # å¯åŠ¨ Trainer è¿›è¡Œæ­£å¼è®­ç»ƒ
        trainer = Trainer(model, args, self.monitor, runner=self)
        trainer.train(train_loader, optimizer, lr_scheduler, query_loader, gallery_loader, checkpoint_dir=str(dataset_log_dir))


if __name__ == '__main__':
    args, config = configuration()
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(args.seed)
    cudnn.deterministic = True
    cudnn.benchmark = False
    runner = Runner(args, config)
    runner.run()
