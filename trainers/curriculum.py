# trainers/curriculum.py
"""
Curriculum Learning Scheduler
è¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨ï¼šä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥
"""

import torch
import torch.nn as nn


class CurriculumScheduler:
    """
    ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨
    
    Phase 1 (Epoch 1-15): ID-First Training
        - ç›®æ ‡ï¼šå»ºç«‹åŸºç¡€IDåˆ¤åˆ«èƒ½åŠ›
        - ç­–ç•¥ï¼šç¦ç”¨è§£è€¦æŸå¤±ï¼Œå…¨åŠ›ä¼˜åŒ–IDåˆ†ç±»
        - é¢„æœŸï¼šRank-1 > 30%
    
    Phase 2 (Epoch 16-40): Adversarial Decoupling
        - ç›®æ ‡ï¼šå¼•å…¥å¯¹æŠ—å¼è§£è€¦
        - ç­–ç•¥ï¼šé€æ­¥å¢å¤§å¯¹æŠ—æŸå¤±æƒé‡ï¼Œä½¿ç”¨GRL
        - é¢„æœŸï¼šRank-1 > 50%, è§£è€¦è´¨é‡æå‡
    
    Phase 3 (Epoch 41+): Fine-tuning
        - ç›®æ ‡ï¼šç²¾ç»†åŒ–è°ƒæ•´æ‰€æœ‰æ¨¡å—
        - ç­–ç•¥ï¼šå¹³è¡¡æ‰€æœ‰æŸå¤±ï¼Œä¼˜åŒ–èåˆæ¨¡å—
        - é¢„æœŸï¼šRank-1 > 60%
    """
    
    def __init__(self, total_epochs=80, logger=None):
        self.total_epochs = total_epochs
        self.logger = logger
        
        # é˜¶æ®µè¾¹ç•Œ
        self.phase1_end = 15
        self.phase2_end = 40
        
        # åŸºç¡€æŸå¤±æƒé‡ï¼ˆPhase 1ï¼‰
        # ğŸ”¥ ä¿®å¤ï¼šé‡æ–°å¹³è¡¡æŸå¤±æƒé‡
        self.base_weights = {
            'info_nce': 1.0,
            'id_triplet': 2.0,  # ğŸ”¥ ä»10.0é™åˆ°2.0ï¼Œé¿å…ä¸»å¯¼è®­ç»ƒ
            'cloth_semantic': 0.1,  # ğŸ”¥ ä»0.01å‡åˆ°0.1ï¼Œå¢å¼ºæœè£…è¯­ä¹‰å­¦ä¹ 
            'spatial_orthogonal': 0.0,  # Phase 1ç¦ç”¨
            'semantic_alignment': 0.0,  # Phase 1ç¦ç”¨
            'ortho_reg': 0.0,
            'adversarial_attr': 0.0,  # Phase 1å®Œå…¨ç¦ç”¨
            'adversarial_domain': 0.0,
            'discriminator_attr': 0.0,
            'discriminator_domain': 0.0
        }
        
        if logger:
            logger.debug_logger.info("=" * 70)
            logger.debug_logger.info("ğŸ“š Curriculum Learning Scheduler Initialized")
            logger.debug_logger.info("=" * 70)
            logger.debug_logger.info(f"Phase 1 (Epoch 1-{self.phase1_end}): ID-First Training")
            logger.debug_logger.info(f"Phase 2 (Epoch {self.phase1_end+1}-{self.phase2_end}): Adversarial Decoupling")
            logger.debug_logger.info(f"Phase 3 (Epoch {self.phase2_end+1}+): Fine-tuning")
            logger.debug_logger.info("=" * 70)
    
    def get_current_phase(self, epoch):
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µ"""
        if epoch <= self.phase1_end:
            return 1
        elif epoch <= self.phase2_end:
            return 2
        else:
            return 3
    
    def get_loss_weights(self, epoch, performance_history=None):
        """
        åŠ¨æ€è·å–æŸå¤±æƒé‡
        
        Args:
            epoch: å½“å‰epoch
            performance_history: dict, å†å²æ€§èƒ½æŒ‡æ ‡ {'epoch': X, 'mAP': Y, 'rank1': Z}
        
        Returns:
            weights: dict, æŸå¤±æƒé‡
        """
        phase = self.get_current_phase(epoch)
        
        if phase == 1:
            # Phase 1: ID-First Training
            # ğŸ”¥ ä¿®å¤ï¼šé‡æ–°å¹³è¡¡æŸå¤±æƒé‡
            weights = {
                'info_nce': 1.0,
                'id_triplet': 2.0,  # ğŸ”¥ ä»10.0é™åˆ°2.0
                'cloth_semantic': 0.1,  # ğŸ”¥ ä»0.01å‡åˆ°0.1
                'spatial_orthogonal': 0.0,
                'semantic_alignment': 0.0,
                'ortho_reg': 0.0,
                'adversarial_attr': 0.0,  # Phase 1å®Œå…¨ç¦ç”¨å¯¹æŠ—
                'adversarial_domain': 0.0,
                'discriminator_attr': 0.0,
                'discriminator_domain': 0.0
            }

            # åŠ¨æ€è°ƒæ•´ï¼šå¦‚æœRank-1å·²ç»è¶…è¿‡30%ï¼Œæå‰è¿›å…¥Phase 2
            if performance_history and len(performance_history) > 0:
                latest_rank1 = performance_history[-1].get('rank1', 0.0)
                if latest_rank1 > 0.30 and epoch >= 10:
                    if self.logger:
                        self.logger.logger.info(f"ğŸ¯ Early Phase Transition: Rank-1={latest_rank1:.1%} > 30%, advancing to Phase 2")
                    return self.get_loss_weights(self.phase1_end + 1, performance_history)

        elif phase == 2:
            # Phase 2: Adversarial Decoupling
            # çº¿æ€§å¢åŠ å¯¹æŠ—æŸå¤±æƒé‡
            progress = (epoch - self.phase1_end) / (self.phase2_end - self.phase1_end)

            # ğŸ”¥ ä¿®å¤ï¼šå¹³æ»‘è¿‡æ¸¡ï¼Œé™ä½å¯¹æŠ—æŸå¤±æƒé‡
            weights = {
                'info_nce': 1.0,
                'id_triplet': 2.0 - progress * 0.5,  # ä»2.0é™åˆ°1.5
                'cloth_semantic': 0.1 + progress * 0.2,  # ä»0.1å‡åˆ°0.3
                'spatial_orthogonal': progress * 0.3,  # ä»0å‡åˆ°0.3
                'semantic_alignment': progress * 0.05,  # ä»0å‡åˆ°0.05
                'ortho_reg': progress * 0.2,  # ä»0å‡åˆ°0.2
                'adversarial_attr': progress * 0.3,  # ğŸ”¥ ä»0å‡åˆ°0.3ï¼ˆä»1.0å¤§å¹…é™ä½ï¼‰
                'adversarial_domain': progress * 0.1,  # ä»0å‡åˆ°0.1
                'discriminator_attr': 0.5,  # ğŸ”¥ é™ä½åˆ¤åˆ«å™¨æƒé‡
                'discriminator_domain': 0.2  # ğŸ”¥ é™ä½åˆ¤åˆ«å™¨æƒé‡
            }

            # æ£€æŸ¥æ€§èƒ½åœæ»
            if performance_history and len(performance_history) >= 5:
                recent_maps = [h.get('mAP', 0.0) for h in performance_history[-5:]]
                if max(recent_maps) - min(recent_maps) < 0.01:  # mAPå˜åŒ–<1%
                    if self.logger:
                        self.logger.logger.warning(f"âš ï¸  Performance plateau detected in Phase 2, adjusting weights")
                    # å¢å¼ºIDå­¦ä¹ 
                    weights['id_triplet'] *= 1.2
                    weights['adversarial_attr'] *= 0.5

        else:
            # Phase 3: Fine-tuning
            # ğŸ”¥ ä¿®å¤ï¼šå¹³è¡¡æ‰€æœ‰æŸå¤±ï¼Œé™ä½å¯¹æŠ—æŸå¤±æƒé‡
            weights = {
                'info_nce': 1.0,
                'id_triplet': 1.5,  # ä¿æŒé€‚ä¸­
                'cloth_semantic': 0.3,  # æå‡æœè£…è¯­ä¹‰
                'spatial_orthogonal': 0.3,  # é™ä½
                'semantic_alignment': 0.05,  # é™ä½
                'ortho_reg': 0.2,
                'adversarial_attr': 0.2,  # ğŸ”¥ å¤§å¹…é™ä½
                'adversarial_domain': 0.05,  # é™ä½
                'discriminator_attr': 0.3,  # é™ä½
                'discriminator_domain': 0.1  # é™ä½
            }
        
        return weights
    
    def get_learning_rate_multiplier(self, epoch):
        """
        è·å–å­¦ä¹ ç‡å€æ•°
        
        Phase 1: 1.0x (å¿«é€Ÿå­¦ä¹ ID)
        Phase 2: 0.5x (ç¨³å®šè§£è€¦)
        Phase 3: 0.3x (ç²¾ç»†è°ƒæ•´)
        """
        phase = self.get_current_phase(epoch)
        
        if phase == 1:
            return 1.0
        elif phase == 2:
            return 0.5
        else:
            return 0.3
    
    def should_train_discriminator(self, epoch, batch_idx, total_batches):
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è®­ç»ƒåˆ¤åˆ«å™¨
        
        ç­–ç•¥ï¼š
        - Phase 1: ä¸è®­ç»ƒ
        - Phase 2/3: æ¯2ä¸ªbatchè®­ç»ƒ1æ¬¡åˆ¤åˆ«å™¨
        """
        phase = self.get_current_phase(epoch)
        
        if phase == 1:
            return False
        
        # æ¯2ä¸ªbatchè®­ç»ƒ1æ¬¡åˆ¤åˆ«å™¨
        return batch_idx % 2 == 0
    
    def get_freeze_config(self, epoch):
        """
        è·å–å†»ç»“é…ç½®
        
        Phase 1: å†»ç»“CLIPå6å±‚ï¼ˆä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼‰
        Phase 2: è§£å†»æ‰€æœ‰CLIPå±‚
        Phase 3: ä¿æŒè§£å†»
        """
        phase = self.get_current_phase(epoch)
        
        if phase == 1:
            return {
                'clip_unfreeze_from_layer': 6,  # åªè§£å†»å‰6å±‚
                'vim_unfreeze_from_layer': 0,  # Vimå…¨éƒ¨è§£å†»
                'freeze_bn': True  # å†»ç»“BatchNorm
            }
        else:
            return {
                'clip_unfreeze_from_layer': 0,  # å…¨éƒ¨è§£å†»
                'vim_unfreeze_from_layer': 0,
                'freeze_bn': False
            }
    
    def print_phase_summary(self, epoch):
        """æ‰“å°å½“å‰é˜¶æ®µæ‘˜è¦"""
        phase = self.get_current_phase(epoch)
        weights = self.get_loss_weights(epoch)
        lr_mult = self.get_learning_rate_multiplier(epoch)
        
        if self.logger:
            self.logger.logger.info("=" * 70)
            self.logger.logger.info(f"ğŸ“š Curriculum Learning - Phase {phase} (Epoch {epoch})")
            self.logger.logger.info("=" * 70)
            
            if phase == 1:
                self.logger.logger.info("ğŸ¯ Goal: Establish basic ID discrimination (Rank-1 > 30%)")
                self.logger.logger.info("ğŸ”§ Strategy: Disable decoupling, focus on Triplet Loss")
            elif phase == 2:
                self.logger.logger.info("ğŸ¯ Goal: Adversarial decoupling (Rank-1 > 50%)")
                self.logger.logger.info("ğŸ”§ Strategy: Gradually increase adversarial loss with GRL")
            else:
                self.logger.logger.info("ğŸ¯ Goal: Fine-tuning all modules (Rank-1 > 60%)")
                self.logger.logger.info("ğŸ”§ Strategy: Balance all losses, optimize fusion")
            
            self.logger.logger.info(f"ğŸ“Š LR Multiplier: {lr_mult:.2f}x")
            self.logger.logger.info("ğŸ“ˆ Active Loss Weights:")
            for key, val in weights.items():
                if val > 0:
                    self.logger.logger.info(f"  - {key}: {val:.4f}")
            self.logger.logger.info("=" * 70)
    
    def should_transition_phase(self, epoch, performance_history):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æå‰è¿‡æ¸¡åˆ°ä¸‹ä¸€é˜¶æ®µ
        
        æ¡ä»¶ï¼š
        - Phase 1 -> Phase 2: Rank-1 > 30% ä¸” epoch >= 10
        - Phase 2 -> Phase 3: Rank-1 > 50% ä¸” epoch >= 30
        """
        if not performance_history or len(performance_history) == 0:
            return False
        
        phase = self.get_current_phase(epoch)
        latest_rank1 = performance_history[-1].get('rank1', 0.0)
        
        if phase == 1 and latest_rank1 > 0.30 and epoch >= 10:
            return True
        elif phase == 2 and latest_rank1 > 0.50 and epoch >= 30:
            return True
        
        return False
