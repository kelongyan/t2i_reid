# src/trainer/trainer.py
import logging
import torch
from pathlib import Path
from tqdm import tqdm
from losses.loss import Loss
from evaluators.evaluator import Evaluator
from utils.serialization import save_checkpoint
from utils.meters import AverageMeter

class Trainer:
    def __init__(self, model, args, monitor=None, runner=None):
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾ç½®æ¨¡å‹ã€å‚æ•°å’Œè®¾å¤‡
        self.model = model
        self.args = args
        self.monitor = monitor  # æ·»åŠ ç›‘æ§å™¨
        self.runner = runner  # æ·»åŠ runnerå¼•ç”¨ä»¥ä¾¿è°ƒç”¨freezeæ–¹æ³•
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # å®šä¹‰é»˜è®¤æŸå¤±æƒé‡ï¼ˆä¸loss.pyä¿æŒä¸€è‡´ï¼‰
        default_loss_weights = {
            'info_nce': 1.0, 'cls': 0.05, 'cloth_semantic': 0.5, 
            'orthogonal': 0.1, 'gate_adaptive': 0.05
        }
        # ä»é…ç½®æ–‡ä»¶è·å–æŸå¤±æƒé‡ï¼Œåˆå¹¶é»˜è®¤å€¼
        loss_weights = getattr(args, 'disentangle', {}).get('loss_weights', default_loss_weights)
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é”®éƒ½å­˜åœ¨
        for key, value in default_loss_weights.items():
            if key not in loss_weights:
                loss_weights[key] = value
        self.combined_loss = Loss(temperature=0.1, weights=loss_weights).to(self.device)
        self.scaler = torch.amp.GradScaler('cuda', enabled=args.fp16) if self.device.type == 'cuda' else None
        if args.fp16 and self.device.type != 'cuda':
            logging.warning("FP16 is enabled but no CUDA device is available. Disabling mixed precision.")

    def run(self, inputs, epoch, batch_idx, total_batches):
        # æ‰§è¡Œå•æ¬¡è®­ç»ƒæ­¥éª¤ï¼Œè®¡ç®—æ‰€æœ‰æŸå¤±
        image, cloth_captions, id_captions, pid, cam_id, is_matched = inputs
        image = image.to(self.device)
        pid = pid.to(self.device)
        cam_id = cam_id.to(self.device) if cam_id is not None else None
        is_matched = is_matched.to(self.device)

        # éªŒè¯è¾“å…¥æ ¼å¼
        if batch_idx == 0:
            if not isinstance(cloth_captions, (list, tuple)) or not all(isinstance(c, str) for c in cloth_captions):
                raise ValueError("cloth_captions must be a list of strings")
            if not isinstance(id_captions, (list, tuple)) or not all(isinstance(c, str) for c in id_captions):
                raise ValueError("id_captions must be a list of strings")

        with torch.amp.autocast('cuda', enabled=self.args.fp16):
            # è®­ç»ƒæ—¶ä¸éœ€è¦æ³¨æ„åŠ›å›¾ï¼Œreturn_attention=Falseï¼ˆé»˜è®¤å€¼ï¼‰
            outputs = self.model(image=image, cloth_instruction=cloth_captions, id_instruction=id_captions)

            # è®­ç»ƒæ—¶æ¨¡å‹è¿”å› 11 ä¸ªè¾“å‡ºï¼ˆå¢åŠ äº†id_cls_featuresï¼‰
            if len(outputs) != 11:
                raise ValueError(f"Expected 11 model outputs during training, got {len(outputs)}")

            image_feats, id_text_feats, fused_feats, id_logits, id_embeds, \
            cloth_embeds, cloth_text_embeds, cloth_image_embeds, gate, gate_weights, \
            id_cls_features = outputs

            loss_dict = self.combined_loss(
                image_embeds=image_feats, id_text_embeds=id_text_feats, fused_embeds=fused_feats,
                id_logits=id_logits, id_embeds=id_embeds, cloth_embeds=cloth_embeds,
                cloth_text_embeds=cloth_text_embeds, cloth_image_embeds=cloth_image_embeds,
                pids=pid, is_matched=is_matched, epoch=epoch, gate=gate,
                id_cls_features=id_cls_features  # æ–°å¢ï¼šä¼ å…¥åˆ†ç±»åˆ†æ”¯ç‰¹å¾
            )

        # è®°å½•æ¨¡å‹å†…éƒ¨çŠ¶æ€ä¿¡æ¯
        if self.monitor and batch_idx % 200 == 0:  # æ¯200ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            self.monitor.log_feature_statistics(image_feats, "image_features")
            self.monitor.log_feature_statistics(id_text_feats, "id_text_features")
            self.monitor.log_feature_statistics(fused_feats, "fused_features")
            self.monitor.log_feature_statistics(id_embeds, "identity_embeds")
            self.monitor.log_feature_statistics(cloth_embeds, "clothing_embeds")
            self.monitor.log_feature_statistics(cloth_text_embeds, "cloth_text_features")
            self.monitor.log_feature_statistics(cloth_image_embeds, "cloth_image_features")

            if gate is not None:
                self.monitor.log_gate_weights(gate, "disentangle_gate")
            if gate_weights is not None:
                self.monitor.log_gate_weights(gate_weights, "fusion_gate")

            self.monitor.log_loss_components(loss_dict)

            # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
            self.monitor.log_memory_usage()

        return loss_dict

    def compute_similarity(self, train_loader):
        # è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸ä¼¼åº¦
        self.model.eval()
        with torch.no_grad():
            for image, cloth_captions, id_captions, pid, cam_id, is_matched in train_loader:
                image = image.to(self.device)
                outputs = self.model(image=image, cloth_instruction=cloth_captions, id_instruction=id_captions)
                image_feats, id_text_feats, _, _, _, _, _, _, _, gate_weights = outputs
                sim = torch.matmul(image_feats, id_text_feats.t())
                pos_sim = sim.diag().mean().item()
                neg_sim = sim[~torch.eye(sim.shape[0], dtype=bool, device=self.device)].mean().item()
                scale = self.model.scale
                return pos_sim, neg_sim, None, scale
        self.model.train()
        return None, None, None, None

    def _format_loss_display(self, loss_meters):
        # æ ¼å¼åŒ–æŸå¤±æ˜¾ç¤ºï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—å¹¶éšè—ç‰¹å®šé¡¹
        display_order = ['info_nce', 'cls', 'cloth_semantic', 'orthogonal', 'gate_adaptive', 'total']
        hidden_losses = set()  # æ‰€æœ‰æŸå¤±éƒ½æ˜¾ç¤º

        avg_losses = []
        for key in display_order:
            if key in loss_meters and loss_meters[key].count > 0:
                avg_losses.append(f"{key}={loss_meters[key].avg:.4f}")

        return avg_losses

    def train(self, train_loader, optimizer, lr_scheduler, query_loader=None, gallery_loader=None, checkpoint_dir=None):
        # è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…å«æŸå¤±è®¡ç®—ã€ä¼˜åŒ–å’Œæ£€æŸ¥ç‚¹ä¿å­˜
        self.model.train()
        best_mAP = 0.0
        best_checkpoint_path = None
        total_batches = len(train_loader)
        loss_meters = {k: AverageMeter() for k in self.combined_loss.weights.keys() | {'total'}}

        for epoch in range(1, self.args.epochs + 1):
            # ã€æ–¹æ¡ˆBï¼šæ¸è¿›è§£å†»ç­–ç•¥ã€‘åœ¨ç‰¹å®šepochæ£€æŸ¥å¹¶è°ƒæ•´å†»ç»“çŠ¶æ€å’Œä¼˜åŒ–å™¨
            stage_changed = False
            if self.runner:
                if epoch == 11:  # Stage 2: è§£å†»ViTå4å±‚ (å…³é”®ä¿®å¤ï¼)
                    print("\n" + "="*70)
                    print("ğŸ”“ Progressive Unfreezing: Stage 2")
                    print("="*70)
                    print("Epoch 11-30: Unfreezing ViT last 4 layers (layer 8-11)")
                    print("             + BERT last 4 layers (layer 8-11)")
                    print("Goal: Let classification head see learnable ViT features")
                    print("="*70 + "\n")
                    self.runner.freeze_bert_layers(self.model, unfreeze_from_layer=8)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=8)
                    # é‡æ–°æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                    optimizer = self.runner.build_optimizer(self.model, stage=2)
                    lr_scheduler = self.runner.build_scheduler(optimizer)
                    stage_changed = True
                elif epoch == 31:  # Stage 3: è§£å†»ViTå8å±‚
                    print("\n" + "="*70)
                    print("ğŸ”“ Progressive Unfreezing: Stage 3")
                    print("="*70)
                    print("Epoch 31-60: Unfreezing ViT last 8 layers (layer 4-11)")
                    print("             + BERT last 8 layers (layer 4-11)")
                    print("Goal: Deep feature adaptation for ReID task")
                    print("="*70 + "\n")
                    self.runner.freeze_bert_layers(self.model, unfreeze_from_layer=4)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=4)
                    optimizer = self.runner.build_optimizer(self.model, stage=3)
                    lr_scheduler = self.runner.build_scheduler(optimizer)
                    stage_changed = True
                elif epoch == 61:  # Stage 4: å…¨éƒ¨è§£å†»
                    print("\n" + "="*70)
                    print("ğŸ”“ Progressive Unfreezing: Stage 4")
                    print("="*70)
                    print("Epoch 61-80: Unfreezing all BERT and ViT layers")
                    print("Goal: End-to-end fine-tuning")
                    print("="*70 + "\n")
                    self.runner.freeze_bert_layers(self.model, unfreeze_from_layer=0)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=0)
                    optimizer = self.runner.build_optimizer(self.model, stage=4)
                    lr_scheduler = self.runner.build_scheduler(optimizer)
                    stage_changed = True
                    
            if stage_changed and self.monitor:
                self.monitor.logger.info(f"Stage changed at epoch {epoch}")
            
            # æ‰“å°ä¸Šä¸€ä¸ª epoch çš„å¹³å‡æŸå¤±
            if epoch > 1:
                avg_losses = self._format_loss_display(loss_meters)
                if avg_losses:
                    # å°†å¹³å‡æŸå¤±è®°å½•åˆ°æ—¥å¿—ï¼Œè€Œä¸æ˜¯æ‰“å°åˆ°ç»ˆç«¯
                    if self.monitor:
                        self.monitor.logger.info(f"[Epoch {epoch-1} Avg Loss:] : {', '.join(avg_losses)}")
                    else:
                        print(f"[Avg Loss:] : {', '.join(avg_losses)}")

            # é‡ç½®æŸå¤±è®°å½•å™¨
            for meter in loss_meters.values():
                meter.reset()

            progress_bar = tqdm(
                train_loader, desc=f"[Epoch {epoch}/{self.args.epochs}] Training",
                dynamic_ncols=True, leave=True, total=total_batches
            )

            for i, inputs in enumerate(progress_bar):
                optimizer.zero_grad()
                loss_dict = self.run(inputs, epoch, i, total_batches)
                loss = loss_dict['total']

                if self.scaler:
                    self.scaler.scale(loss).backward()
                    
                    # æ¢¯åº¦è£å‰ªï¼šæ”¾å®½é™åˆ¶ï¼Œå…è®¸åˆ†ç±»æŸå¤±æ›´å¿«ä¸‹é™
                    self.scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=2.0)

                    # è®°å½•æ¢¯åº¦æµåŠ¨ï¼ˆæ¯1000ä¸ªbatchè®°å½•ä¸€æ¬¡ï¼‰
                    if self.monitor and i % 1000 == 0:
                        self.monitor.log_gradient_flow(self.model)
                    # è®°å½•åŸºç¡€æ¢¯åº¦ä¿¡æ¯ï¼ˆæ¯100ä¸ªbatchï¼‰
                    elif self.monitor and i % 100 == 0:
                        self.monitor.log_gradients(self.model, f"epoch_{epoch}_batch_{i}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ¢¯åº¦
                    has_grads = any(p.grad is not None for group in optimizer.param_groups for p in group['params'])
                    if has_grads:
                        self.scaler.step(optimizer)
                        self.scaler.update()
                    else:
                        logging.warning(f"âš ï¸  Skipping step at epoch {epoch} batch {i}: No gradients found (likely due to NaN loss).")
                else:
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ªï¼šæ”¾å®½é™åˆ¶
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=2.0)

                    # è®°å½•æ¢¯åº¦æµåŠ¨ï¼ˆæ¯1000ä¸ªbatchè®°å½•ä¸€æ¬¡ï¼‰
                    if self.monitor and i % 1000 == 0:
                        self.monitor.log_gradient_flow(self.model)
                    # è®°å½•åŸºç¡€æ¢¯åº¦ä¿¡æ¯ï¼ˆæ¯100ä¸ªbatchï¼‰
                    elif self.monitor and i % 100 == 0:
                        self.monitor.log_gradients(self.model, f"epoch_{epoch}_batch_{i}")

                    optimizer.step()

                # æ›´æ–°æŸå¤±è®°å½•
                for key, val in loss_dict.items():
                    if key in loss_meters:
                        loss_meters[key].update(val.item() if isinstance(val, torch.Tensor) else val)
                
                # è®°å½•è¯¦ç»†æŸå¤±åˆ†è§£ï¼ˆæ¯100ä¸ªbatchï¼‰
                if self.monitor and i % 100 == 0:
                    self.monitor.log_loss_breakdown(loss_dict, epoch, i)

                # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
                if self.monitor and i % 50 == 0:  # æ¯50ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                    current_lr = optimizer.param_groups[0]['lr']
                    self.monitor.log_batch_info(epoch, i, total_batches,
                                              {k: v.avg for k, v in loss_meters.items()},
                                              current_lr)

            progress_bar.close()
            
            # åªåœ¨stageæœªæ”¹å˜æ—¶è°ƒç”¨lr_scheduler.step()
            if not stage_changed:
                lr_scheduler.step()

            # è®°å½•epochä¿¡æ¯
            if self.monitor:
                epoch_metrics = {k: v.avg for k, v in loss_meters.items()}
                self.monitor.log_epoch_info(epoch, self.args.epochs, epoch_metrics)

            # æ¯ä¸ªepochç»“æŸåè¿›è¡Œè¯„ä¼°
            if query_loader and gallery_loader:
                # è¯„ä¼°æ¨¡å‹
                evaluator = Evaluator(self.model, args=self.args)
                metrics = evaluator.evaluate(
                    query_loader, gallery_loader, query_loader.dataset.data,
                    gallery_loader.dataset.data, checkpoint_path=None, epoch=epoch
                )

                current_mAP = metrics['mAP']

                # è®°å½•è¯„ä¼°ç»“æœåˆ°æ—¥å¿—
                if self.monitor:
                    self.monitor.logger.info(f"Epoch {epoch} - Evaluation Results: {metrics}")
                
                # æŒ‰ç…§ç”¨æˆ·è¦æ±‚ï¼Œåœ¨ç»ˆç«¯ç«–åˆ—æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                print(f"\n[Epoch {epoch} Evaluation Results]")
                print(f"  mAP:    {metrics['mAP']:.4f}")
                print(f"  Rank-1: {metrics['rank1']:.4f}")
                print(f"  Rank-5: {metrics['rank5']:.4f}")
                print(f"  Rank-10: {metrics['rank10']:.4f}\n")

                # ä¿å­˜æœ€ä¼˜æ£€æŸ¥ç‚¹
                if current_mAP > best_mAP:
                    best_mAP = current_mAP

                    # ç”Ÿæˆæœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„
                    if checkpoint_dir:
                        # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šæ¨¡å‹æ–‡ä»¶å
                        dataset_short_name = self._get_dataset_name()

                        # è·å–æ•°æ®é›†çš„æ ‡å‡†åç§°ç”¨äºç›®å½•ç»“æ„
                        if hasattr(self.args, 'dataset_configs') and self.args.dataset_configs:
                            dataset_full_name = self.args.dataset_configs[0]['name'].lower()
                            if 'cuhk' in dataset_full_name:
                                dataset_dir_name = 'cuhk_pedes'
                            elif 'rstp' in dataset_full_name:
                                dataset_dir_name = 'rstp'
                            elif 'icfg' in dataset_full_name:
                                dataset_dir_name = 'icfg'
                            else:
                                dataset_dir_name = dataset_full_name
                        else:
                            dataset_dir_name = dataset_short_name

                        # æ„å»ºæ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„ä¸º PROJECT_ROOT/log/DATASET_DIR_NAME/model/best_DATASET_SHORTNAME.pth
                        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆé€šè¿‡å½“å‰è„šæœ¬è·¯å¾„å‘ä¸Šä¸¤çº§ï¼‰
                        script_dir = Path(__file__).parent  # trainers/
                        project_root = script_dir.parent   # é¡¹ç›®æ ¹ç›®å½•

                        # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šæ­£ç¡®çš„ç›®å½•åï¼Œä¸ä¾èµ–äºä¼ å…¥çš„checkpoint_dir
                        if hasattr(self.args, 'dataset_configs') and self.args.dataset_configs:
                            dataset_full_name = self.args.dataset_configs[0]['name'].lower()
                            if 'cuhk' in dataset_full_name:
                                dataset_dir_name_correct = 'cuhk'
                            elif 'rstp' in dataset_full_name:
                                dataset_dir_name_correct = 'rstp'
                            elif 'icfg' in dataset_full_name:
                                dataset_dir_name_correct = 'icfg'
                            else:
                                dataset_dir_name_correct = dataset_short_name
                        else:
                            dataset_dir_name_correct = dataset_short_name

                        log_base_path = project_root / 'log'

                        model_dir = log_base_path / dataset_dir_name_correct / 'model'
                        model_dir.mkdir(parents=True, exist_ok=True)

                        new_best_checkpoint_path = str(model_dir / f"best_{dataset_short_name}.pth")

                        # åˆ é™¤æ—§çš„æœ€ä½³æ£€æŸ¥ç‚¹
                        if best_checkpoint_path and Path(best_checkpoint_path).exists():
                            try:
                                Path(best_checkpoint_path).unlink()
                                if self.monitor:
                                    self.monitor.logger.info(f"Removed old best checkpoint: {best_checkpoint_path}")
                            except OSError:
                                if self.monitor:
                                    self.monitor.logger.warning(f"Could not remove old best checkpoint: {best_checkpoint_path}")

                        # ä¿å­˜æ–°çš„æœ€ä½³æ£€æŸ¥ç‚¹
                        save_checkpoint({
                            'model': self.model.state_dict(),
                            'optimizer': optimizer.state_dict(),
                            'lr_scheduler': lr_scheduler.state_dict(),
                            'epoch': epoch,
                            'mAP': current_mAP
                        }, fpath=new_best_checkpoint_path)

                        best_checkpoint_path = new_best_checkpoint_path

                        if self.monitor:
                            self.monitor.logger.info(f"New best checkpoint saved: {best_checkpoint_path}, mAP: {best_mAP:.4f}")
                    else:
                        if self.monitor:
                            self.monitor.logger.warning("checkpoint_dir not provided, cannot save best checkpoint")

        # è®°å½•æœ€ç»ˆç»“æœ
        if self.monitor:
            self.monitor.logger.info(f"Training completed. Best mAP: {best_mAP:.4f}")

        # æ‰“å°æœ€ç»ˆå¹³å‡æŸå¤±
        avg_losses = self._format_loss_display(loss_meters)
        if avg_losses:
            # å°†æœ€ç»ˆå¹³å‡æŸå¤±è®°å½•åˆ°æ—¥å¿—ï¼Œè€Œä¸æ˜¯æ‰“å°åˆ°ç»ˆç«¯
            if self.monitor:
                self.monitor.logger.info(f"[Final Avg Loss:] : {', '.join(avg_losses)}")
            else:
                print(f"[Avg Loss:] : {', '.join(avg_losses)}")

        if best_checkpoint_path:
            logging.info(f"Final best checkpoint: {best_checkpoint_path}, mAP: {best_mAP:.4f}")

    def _get_dataset_name(self):
        """è·å–æ•°æ®é›†åç§°ç”¨äºæ¨¡å‹æ–‡ä»¶å‘½å"""
        if hasattr(self.args, 'dataset_configs') and self.args.dataset_configs:
            dataset_name = self.args.dataset_configs[0]['name'].lower()
            if 'cuhk' in dataset_name:
                return 'cuhk'
            elif 'rstp' in dataset_name:
                return 'rstp'
            elif 'icfg' in dataset_name:
                return 'icfg'
            else:
                return dataset_name
        else:
            return 'unknown'