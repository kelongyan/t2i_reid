# src/trainer/trainer.py
import torch
import torch.nn as nn
from pathlib import Path
from tqdm import tqdm
from losses.loss import Loss
from evaluators.evaluator import Evaluator
from utils.serialization import save_checkpoint
from utils.meters import AverageMeter
from utils.visualization import FSHDVisualizer
from trainers.curriculum import CurriculumScheduler  # ğŸ”¥ æ–°å¢

class EarlyStopping:
    """æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆä¿®æ”¹ä¸º20ä¸ªepochï¼‰"""
    def __init__(self, patience=20, min_delta=0.001, logger=None):
        self.patience = patience
        self.min_delta = min_delta
        self.logger = logger
        self.best_score = None
        self.counter = 0
        self.early_stop = False
    
    def __call__(self, mAP):
        if self.best_score is None:
            self.best_score = mAP
        elif mAP < self.best_score - self.min_delta:
            self.counter += 1
            if self.logger:
                self.logger.debug_logger.info(
                    f"EarlyStopping: {self.counter}/{self.patience} "
                    f"(best={self.best_score:.4f}, current={mAP:.4f})"
                )
            if self.counter >= self.patience:
                self.early_stop = True
                if self.logger:
                    self.logger.logger.warning("Early stopping triggered!")
        else:
            self.best_score = mAP
            self.counter = 0

class Trainer:
    def __init__(self, model, args, monitor=None, runner=None):
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾ç½®æ¨¡å‹ã€å‚æ•°å’Œè®¾å¤‡
        self.model = model
        self.args = args
        self.monitor = monitor
        self.runner = runner
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ğŸ”¥ æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨
        self.curriculum = CurriculumScheduler(
            total_epochs=args.epochs,
            logger=monitor
        )

        # æ€§èƒ½å†å²è®°å½•ï¼ˆç”¨äºè¯¾ç¨‹å­¦ä¹ ï¼‰
        self.performance_history = []

        # ğŸ”¥ ä¿®å¤ï¼šåˆå§‹åŒ–Lossæ¨¡å—ï¼ˆæ”¯æŒå¯¹æŠ—è®­ç»ƒï¼Œä½¿ç”¨æ­£ç¡®çš„æ¸©åº¦å‚æ•°ï¼‰
        self.loss = Loss(
            temperature=0.07,  # æ ‡å‡†çš„InfoNCEæ¸©åº¦å‚æ•°
            weights=self.curriculum.base_weights,  # ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„åˆå§‹æƒé‡
            logger=monitor,
            semantic_guidance=model.semantic_guidance,
            adversarial_decoupler=model.adversarial_decoupler  # ğŸ”¥ æ–°å¢
        ).to(self.device)
        
        # === åˆå§‹åŒ–å¯è§†åŒ–å™¨ ===
        visualize_config = getattr(args, 'visualization', {})
        if visualize_config.get('enabled', True):
            vis_save_dir = visualize_config.get('save_dir', 'visualizations')
            self.visualizer = FSHDVisualizer(save_dir=vis_save_dir, logger=monitor)
            self.visualize_freq = visualize_config.get('frequency', 5)
            self.visualize_batch_interval = visualize_config.get('batch_interval', 200)
            if self.monitor:
                self.monitor.debug_logger.info(f"âœ… Visualizer enabled (freq={self.visualize_freq}, batch_interval={self.visualize_batch_interval})")
        else:
            self.visualizer = None
        
        self.scaler = torch.amp.GradScaler('cuda', enabled=args.fp16) if self.device.type == 'cuda' else None
        if args.fp16 and self.device.type != 'cuda':
            if self.monitor: 
                self.monitor.logger.warning("FP16 is enabled but no CUDA device is available. Disabling mixed precision.")

    def reinit_clip_bias_layers(self, model, logger=None):
        """é‡æ–°åˆå§‹åŒ–CLIPæ–‡æœ¬ç¼–ç å™¨çš„biasï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±"""
        reinitialized_count = 0
        for name, param in model.named_parameters():
            if 'text_encoder' in name and 'bias' in name and param.requires_grad:
                # ä½¿ç”¨è¾ƒå°çš„stdåˆå§‹åŒ–
                nn.init.normal_(param, std=0.02)
                reinitialized_count += 1
                if logger and reinitialized_count <= 5:  # åªæ‰“å°å‰5ä¸ª
                    logger.debug_logger.info(f"Reinitialized CLIP bias: {name}")
        if logger:
            logger.debug_logger.info(f"Total CLIP bias params reinitialized: {reinitialized_count}")
    
    def build_optimizer_with_lr_groups(self, model, stage):
        """ä¸ºæ–°è§£å†»å±‚è®¾ç½®ç‹¬ç«‹å­¦ä¹ ç‡"""
        if stage >= 2:
            # CLIPæ–‡æœ¬ç¼–ç å™¨åå‡ å±‚ä½¿ç”¨0.5å€å­¦ä¹ ç‡
            clip_params = []
            other_params = []
            
            for name, param in model.named_parameters():
                if param.requires_grad:
                    if 'text_encoder.text_model.encoder' in name:
                        try:
                            layer_num = int(name.split('.')[4])  # text_model.encoder.layers.11
                            if layer_num >= 11:
                                clip_params.append(param)
                                continue
                        except (IndexError, ValueError):
                            pass
                    other_params.append(param)
            
            if clip_params:
                param_groups = [
                    {'params': clip_params, 'lr': self.args.lr * 0.5, 'name': 'clip_text', 'weight_decay': self.args.weight_decay},
                    {'params': other_params, 'lr': self.args.lr, 'name': 'others', 'weight_decay': self.args.weight_decay}
                ]
                if self.monitor:
                    self.monitor.logger.info(f"Built optimizer with {len(clip_params)} CLIP params (0.5x lr) and {len(other_params)} other params")
                return torch.optim.AdamW(param_groups)
        return self._build_default_optimizer(model)
    
    def _build_default_optimizer(self, model):
        """é»˜è®¤ä¼˜åŒ–å™¨æ„å»ºæ–¹æ³•"""
        return torch.optim.AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=self.args.lr,
            weight_decay=self.args.weight_decay
        )
    
    def _get_warmup_lr(self, base_lr, current_step, warmup_steps):
        """å­¦ä¹ ç‡é¢„çƒ­"""
        if current_step < warmup_steps:
            return base_lr * (current_step / warmup_steps)
        return base_lr
    
    def build_scheduler_with_cosine_warmup(self, optimizer, num_training_steps, num_warmup_steps):
        """ä½™å¼¦é€€ç«+é¢„çƒ­å­¦ä¹ ç‡"""
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_training_steps - num_warmup_steps
        )
    
    def clip_grad_norm_by_layer(self, model, max_norm=1.0):
        """ğŸ”¥ æ”¹è¿›çš„åˆ†å±‚æ¢¯åº¦è£å‰ªï¼Œç‰¹åˆ«é’ˆå¯¹Mambaæ¨¡å—"""
        for name, param in model.named_parameters():
            if param.grad is not None:
                # ğŸ”¥ Vim Mambaæ¨¡å—ï¼šæœ€ä¸¥æ ¼çš„è£å‰ªï¼ˆé˜²æ­¢NaNï¼‰
                if 'visual_encoder' in name and 'mixer' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.3)
                # Vimå…¶ä»–å±‚ï¼šä¸­ç­‰è£å‰ª
                elif 'visual_encoder' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.5)
                # CLIPæ–‡æœ¬ç¼–ç å™¨ï¼šä¸¥æ ¼è£å‰ª
                elif 'text_encoder' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.5)
                # è§£è€¦æ¨¡å—ï¼šå®½æ¾è£å‰ª
                elif 'disentangle' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.7)
                # Fusionæ¨¡å—ï¼šå®½æ¾è£å‰ª
                elif 'fusion' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.8)
                # å…¶ä»–å±‚ï¼šæ ‡å‡†è£å‰ª
                else:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm)
    
    def enable_batch_norm_warmup(self, model, momentum=0.01):
        """ä¸ºæ–°è§£å†»çš„å±‚å¯ç”¨BatchNormé¢„çƒ­"""
        for module in model.modules():
            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):
                module.momentum = momentum  # é™ä½momentumï¼ŒåŠ å¿«ç»Ÿè®¡é‡æ›´æ–°
                module.track_running_stats = True
        if self.monitor:
            self.monitor.logger.info(f"BatchNorm warmup enabled with momentum={momentum}")

    def run(self, inputs, epoch, batch_idx, total_batches, training_phase='feature'):
        # æ‰§è¡Œå•æ¬¡è®­ç»ƒæ­¥éª¤ï¼Œè®¡ç®—æ‰€æœ‰æŸå¤±ï¼ˆæ”¯æŒå¯¹æŠ—è®­ç»ƒï¼‰
        image, cloth_captions, id_captions, pid, cam_id, is_matched = inputs
        image = image.to(self.device)
        pid = pid.to(self.device)
        cam_id = cam_id.to(self.device) if cam_id is not None else None
        is_matched = is_matched.to(self.device)

        # éªŒè¯è¾“å…¥æ ¼å¼
        if batch_idx == 0:
            if not isinstance(cloth_captions, (list, tuple)) or not all(isinstance(c, str) for c in cloth_captions):
                raise ValueError("cloth_captions must be a list of strings")
            if not isinstance(id_captions, (list, tuple)) or not all(isinstance(c, str) for c in id_captions):
                raise ValueError("id_captions must be a list of strings")

        with torch.amp.autocast('cuda', enabled=self.args.fp16):
            # è®­ç»ƒæ—¶å¯ä»¥é€‰æ‹©æ€§è¿”å›æ³¨æ„åŠ›å›¾
            outputs = self.model(image=image, cloth_instruction=cloth_captions, 
                               id_instruction=id_captions)

            # æ¨¡å‹è¿”å›11ä¸ªè¾“å‡º
            if len(outputs) != 11:
                raise ValueError(f"Expected 11 model outputs during training, got {len(outputs)}")

            image_embeds, id_text_embeds, fused_embeds, id_embeds, \
            cloth_embeds, cloth_text_embeds, cloth_image_embeds, aux_info, gate_weights, \
            id_cls_features, original_feat = outputs
            
        # ğŸ”¥ è®¡ç®—æŸå¤±ï¼ˆæ”¯æŒè®­ç»ƒé˜¶æ®µåŒºåˆ†ï¼‰
        loss_dict = self.loss(
            image_embeds=image_embeds,
            id_text_embeds=id_text_embeds,
            fused_embeds=fused_embeds,
            id_logits=None,
            id_embeds=id_embeds,
            cloth_embeds=cloth_embeds,
            cloth_text_embeds=cloth_text_embeds,
            cloth_image_embeds=cloth_image_embeds,
            pids=pid,
            epoch=epoch,
            aux_info=aux_info,
            training_phase=training_phase  # ğŸ”¥ æ–°å¢ï¼šåŒºåˆ†ç‰¹å¾æå–å™¨/åˆ¤åˆ«å™¨è®­ç»ƒ
        )

        # å¯è§†åŒ–
        if self.visualizer and epoch % self.visualize_freq == 0 and batch_idx % self.visualize_batch_interval == 0:
            if hasattr(self.model.disentangle, 'forward'):
                if len(outputs) > 8:
                    aux_info = outputs[8]
                    if aux_info is not None and isinstance(aux_info, dict):
                        self.visualizer.plot_attention_maps(aux_info, epoch, batch_idx, images=image)
        
        # è®°å½•ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        if self.monitor and batch_idx % 200 == 0:
            self.monitor.log_feature_statistics(image_embeds, "image_features")
            self.monitor.log_feature_statistics(id_text_embeds, "id_text_features")
            self.monitor.log_feature_statistics(fused_embeds, "fused_features")
            self.monitor.log_feature_statistics(id_embeds, "identity_embeds")
            self.monitor.log_feature_statistics(cloth_embeds, "clothing_embeds")
            self.monitor.log_feature_statistics(cloth_text_embeds, "cloth_text_embeds")
            self.monitor.log_feature_statistics(cloth_image_embeds, "cloth_image_embeds")
            
            # Conflict Scoreæ—¥å¿—
            if aux_info is not None and isinstance(aux_info, dict):
                conflict_score = aux_info.get('conflict_score')
                if conflict_score is not None and self.monitor:
                    if batch_idx % 200 == 0:
                        self.monitor.log_conflict_score(conflict_score, step_name=f"_E{epoch}_B{batch_idx}")

        # aux_infoç»Ÿè®¡
        if aux_info is not None and isinstance(aux_info, dict):
            conflict_score = aux_info.get('conflict_score')
            ortho_reg = aux_info.get('ortho_reg')
            
            # ğŸ”¥ ä¿®å¤: å°†Tensorè½¬æ¢ä¸ºæ ‡é‡å€¼ç”¨äºæ—¥å¿—è¾“å‡º
            conflict_val = conflict_score.mean().item() if conflict_score is not None else 0.0
            ortho_val = ortho_reg.item() if ortho_reg is not None else 0.0
            
            self.monitor.debug_logger.debug(
                f"Aux info: Conflict[{conflict_val:.4f}], "
                f"Ortho Reg[{ortho_val:.4f}]"
            )

        if gate_weights is not None:
            self.monitor.log_gate_weights(gate_weights, "fusion_gate")

        self.monitor.log_loss_components(loss_dict)

        # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        self.monitor.log_memory_usage()

        return loss_dict
    

    def _format_loss_display(self, loss_meters):
        # æ ¼å¼åŒ–æŸå¤±æ˜¾ç¤ºï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—å¹¶éšè—ç‰¹å®šé¡¹
        # [Modify] é€‚é… AH-Net + æ–¹æ¡ˆä¹¦ Phase 3
        display_order = ['info_nce', 'reconstruction', 'cloth_semantic', 'id_triplet',
                        'spatial_orthogonal', 'semantic_alignment', 'total']

        avg_losses = []
        for key in display_order:
            if key in loss_meters and loss_meters[key].count > 0:
                avg_losses.append(f"{key}={loss_meters[key].avg:.4f}")

        return avg_losses

    def train(self, train_loader, optimizer, lr_scheduler, query_loader=None, gallery_loader=None, checkpoint_dir=None):
        """è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ï¼‰"""
        from trainers.train_loop import train_with_curriculum
        
        # è°ƒç”¨æ–°çš„è®­ç»ƒå¾ªç¯
        best_mAP, best_checkpoint_path = train_with_curriculum(
            trainer=self,
            train_loader=train_loader,
            query_loader=query_loader,
            gallery_loader=gallery_loader,
            checkpoint_dir=checkpoint_dir
        )
        
        return best_mAP, best_checkpoint_path

    def _get_dataset_name(self):
        """è·å–æ•°æ®é›†åç§°ç”¨äºæ¨¡å‹æ–‡ä»¶å‘½å"""
        if hasattr(self.args, 'dataset_configs') and self.args.dataset_configs:
            dataset_name = self.args.dataset_configs[0]['name'].lower()
            if 'cuhk' in dataset_name:
                return 'cuhk'
            elif 'rstp' in dataset_name:
                return 'rstp'
            elif 'icfg' in dataset_name:
                return 'icfg'
            else:
                return dataset_name
        else:
            return 'unknown'