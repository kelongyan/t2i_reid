# src/trainer/trainer.py
import torch
import torch.nn as nn
from pathlib import Path
from tqdm import tqdm
from losses.loss import Loss
from evaluators.evaluator import Evaluator
from utils.serialization import save_checkpoint
from utils.meters import AverageMeter
from utils.visualization import FSHDVisualizer  # æ–°å¢ï¼šå¯è§†åŒ–å·¥å…·

class EarlyStopping:
    """æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
    def __init__(self, patience=10, min_delta=0.001, logger=None):
        self.patience = patience
        self.min_delta = min_delta
        self.logger = logger
        self.best_score = None
        self.counter = 0
        self.early_stop = False
    
    def __call__(self, mAP):
        if self.best_score is None:
            self.best_score = mAP
        elif mAP < self.best_score - self.min_delta:
            self.counter += 1
            if self.logger:
                self.logger.debug_logger.info(
                    f"EarlyStopping: {self.counter}/{self.patience} "
                    f"(best={self.best_score:.4f}, current={mAP:.4f})"
                )
            if self.counter >= self.patience:
                self.early_stop = True
                if self.logger:
                    self.logger.logger.warning("Early stopping triggered!")
        else:
            self.best_score = mAP
            self.counter = 0

class Trainer:
    def __init__(self, model, args, monitor=None, runner=None):
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾ç½®æ¨¡å‹ã€å‚æ•°å’Œè®¾å¤‡
        self.model = model
        self.args = args
        self.monitor = monitor  # æ·»åŠ ç›‘æ§å™¨
        self.runner = runner  # æ·»åŠ runnerå¼•ç”¨ä»¥ä¾¿è°ƒç”¨freezeæ–¹æ³•
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # === FSHDæƒé‡é…ç½®ï¼ˆæ¿€è¿›ç‰ˆï¼‰===
        default_loss_weights = {
            'info_nce': 1.0, 
            'cls': 0.05,
            'cloth_semantic': 1.0, 
            'orthogonal': 0.1,            # å¤§å¹…é™ä½
            'gate_adaptive': 0.02,
            'reconstruction': 0.5,
            'semantic_alignment': 0.1,     # å¤§å¹…é™ä½
            'freq_consistency': 0.5,      # ã€æ–°å¢ã€‘é¢‘åŸŸä¸€è‡´æ€§
            'freq_separation': 0.2,       # ã€æ–°å¢ã€‘é¢‘åŸŸåˆ†ç¦»
        }
        
        # ä»é…ç½®æ–‡ä»¶è·å–æŸå¤±æƒé‡ï¼Œåˆå¹¶é»˜è®¤å€¼
        loss_weights = getattr(args, 'disentangle', {}).get('loss_weights', default_loss_weights)
        for key, value in default_loss_weights.items():
            if key not in loss_weights:
                loss_weights[key] = value
        
        # åˆå§‹åŒ–Lossæ¨¡å—
        self.combined_loss = Loss(temperature=0.1, weights=loss_weights, logger=monitor).to(self.device)
        
        # === è®¾ç½®è¯­ä¹‰å¼•å¯¼æ¨¡å—åˆ°Lossï¼ˆå…³é”®ï¼ï¼‰===
        if hasattr(model, 'semantic_guidance'):
            self.combined_loss.set_semantic_guidance(model.semantic_guidance)
            if self.monitor:
                self.monitor.debug_logger.info("âœ… Semantic guidance module connected to Loss system")
        
        # === æ–°å¢ï¼šåˆå§‹åŒ–å¯è§†åŒ–å™¨ ===
        visualize_config = getattr(args, 'visualization', {})
        if visualize_config.get('enabled', True):
            vis_save_dir = visualize_config.get('save_dir', 'visualizations')
            self.visualizer = FSHDVisualizer(save_dir=vis_save_dir, logger=monitor)
            self.visualize_freq = visualize_config.get('frequency', 5)  # æ¯Nä¸ªepochå¯è§†åŒ–ä¸€æ¬¡
            self.visualize_batch_interval = visualize_config.get('batch_interval', 200)
            if self.monitor:
                self.monitor.debug_logger.info(f"âœ… Visualizer enabled (freq={self.visualize_freq}, batch_interval={self.visualize_batch_interval})")
        else:
            self.visualizer = None
        
        self.scaler = torch.amp.GradScaler('cuda', enabled=args.fp16) if self.device.type == 'cuda' else None
        if args.fp16 and self.device.type != 'cuda':
            if self.monitor: 
                self.monitor.logger.warning("FP16 is enabled but no CUDA device is available. Disabling mixed precision.")

    def reinit_clip_bias_layers(self, model, logger=None):
        """é‡æ–°åˆå§‹åŒ–CLIPæ–‡æœ¬ç¼–ç å™¨çš„biasï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±"""
        reinitialized_count = 0
        for name, param in model.named_parameters():
            if 'text_encoder' in name and 'bias' in name and param.requires_grad:
                # ä½¿ç”¨è¾ƒå°çš„stdåˆå§‹åŒ–
                nn.init.normal_(param, std=0.02)
                reinitialized_count += 1
                if logger and reinitialized_count <= 5:  # åªæ‰“å°å‰5ä¸ª
                    logger.debug_logger.info(f"Reinitialized CLIP bias: {name}")
        if logger:
            logger.debug_logger.info(f"Total CLIP bias params reinitialized: {reinitialized_count}")
    
    def build_optimizer_with_lr_groups(self, model, stage):
        """ä¸ºæ–°è§£å†»å±‚è®¾ç½®ç‹¬ç«‹å­¦ä¹ ç‡"""
        if stage >= 2:
            # CLIPæ–‡æœ¬ç¼–ç å™¨åå‡ å±‚ä½¿ç”¨0.5å€å­¦ä¹ ç‡
            clip_params = []
            other_params = []
            
            for name, param in model.named_parameters():
                if param.requires_grad:
                    if 'text_encoder.text_model.encoder' in name:
                        try:
                            layer_num = int(name.split('.')[4])  # text_model.encoder.layers.11
                            if layer_num >= 11:
                                clip_params.append(param)
                                continue
                        except (IndexError, ValueError):
                            pass
                    other_params.append(param)
            
            if clip_params:
                param_groups = [
                    {'params': clip_params, 'lr': self.args.lr * 0.5, 'name': 'clip_text', 'weight_decay': self.args.weight_decay},
                    {'params': other_params, 'lr': self.args.lr, 'name': 'others', 'weight_decay': self.args.weight_decay}
                ]
                if self.monitor:
                    self.monitor.logger.info(f"Built optimizer with {len(clip_params)} CLIP params (0.5x lr) and {len(other_params)} other params")
                return torch.optim.AdamW(param_groups)
        return self._build_default_optimizer(model)
    
    def _build_default_optimizer(self, model):
        """é»˜è®¤ä¼˜åŒ–å™¨æ„å»ºæ–¹æ³•"""
        return torch.optim.AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=self.args.lr,
            weight_decay=self.args.weight_decay
        )
    
    def _get_warmup_lr(self, base_lr, current_step, warmup_steps):
        """å­¦ä¹ ç‡é¢„çƒ­"""
        if current_step < warmup_steps:
            return base_lr * (current_step / warmup_steps)
        return base_lr
    
    def build_scheduler_with_cosine_warmup(self, optimizer, num_training_steps, num_warmup_steps):
        """ä½™å¼¦é€€ç«+é¢„çƒ­å­¦ä¹ ç‡"""
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_training_steps - num_warmup_steps
        )
    
    def clip_grad_norm_by_layer(self, model, max_norm=1.0):
        """å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„æ¢¯åº¦è£å‰ªé˜ˆå€¼"""
        for name, param in model.named_parameters():
            if param.grad is not None:
                # CLIPæ–‡æœ¬ç¼–ç å™¨ï¼šæ›´ä¸¥æ ¼çš„è£å‰ª
                if 'text_encoder' in name:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 0.5)
                # æ–°è§£å†»çš„å±‚ï¼šè¾ƒå®½æ¾çš„è£å‰ª
                elif 'layers' in name:
                    try:
                        layer_num = int([s for s in name.split('.') if s.isdigit()][0])
                        if layer_num >= 11:
                            torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm * 2.0)
                        else:
                            torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm)
                    except (IndexError, ValueError):
                        torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm)
                else:
                    torch.nn.utils.clip_grad_norm_([param], max_norm=max_norm)
    
    def enable_batch_norm_warmup(self, model, momentum=0.01):
        """ä¸ºæ–°è§£å†»çš„å±‚å¯ç”¨BatchNormé¢„çƒ­"""
        for module in model.modules():
            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):
                module.momentum = momentum  # é™ä½momentumï¼ŒåŠ å¿«ç»Ÿè®¡é‡æ›´æ–°
                module.track_running_stats = True
        if self.monitor:
            self.monitor.logger.info(f"BatchNorm warmup enabled with momentum={momentum}")

    def run(self, inputs, epoch, batch_idx, total_batches):
        # æ‰§è¡Œå•æ¬¡è®­ç»ƒæ­¥éª¤ï¼Œè®¡ç®—æ‰€æœ‰æŸå¤±ï¼ˆFSHDç‰ˆæœ¬ï¼‰
        image, cloth_captions, id_captions, pid, cam_id, is_matched = inputs
        image = image.to(self.device)
        pid = pid.to(self.device)
        cam_id = cam_id.to(self.device) if cam_id is not None else None
        is_matched = is_matched.to(self.device)

        # éªŒè¯è¾“å…¥æ ¼å¼
        if batch_idx == 0:
            if not isinstance(cloth_captions, (list, tuple)) or not all(isinstance(c, str) for c in cloth_captions):
                raise ValueError("cloth_captions must be a list of strings")
            if not isinstance(id_captions, (list, tuple)) or not all(isinstance(c, str) for c in id_captions):
                raise ValueError("id_captions must be a list of strings")

        with torch.amp.autocast('cuda', enabled=self.args.fp16):
            # === FSHDæ¨¡å—æ”¯æŒè¿”å›é¢‘åŸŸä¿¡æ¯ ===
            # å¦‚æœä½¿ç”¨FSHDæ¨¡å—ï¼Œéœ€è¦è·å–é¢‘åŸŸä¿¡æ¯
            return_freq_info = (self.visualizer is not None and 
                               batch_idx % self.visualize_batch_interval == 0)
            
            # è®­ç»ƒæ—¶å¯ä»¥é€‰æ‹©æ€§è¿”å›æ³¨æ„åŠ›å›¾å’Œé¢‘åŸŸä¿¡æ¯
            outputs = self.model(image=image, cloth_instruction=cloth_captions, 
                               id_instruction=id_captions)

            # === FSHDæ¨¡å—è¿”å›12ä¸ªè¾“å‡ºï¼ˆä¿æŒå…¼å®¹ï¼‰===
            if len(outputs) != 12:
                raise ValueError(f"Expected 12 model outputs during training, got {len(outputs)}")

            image_feats, id_text_feats, fused_feats, id_logits, id_embeds, \
            cloth_embeds, cloth_text_embeds, cloth_image_embeds, gate_stats, gate_weights, \
            id_cls_features, original_feat = outputs
            
            # === è·å–é¢‘åŸŸä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨FSHDæ¨¡å—ï¼‰===
            freq_info = None
            if hasattr(self.model, 'disentangle') and hasattr(self.model.disentangle, 'forward'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯FSHDæ¨¡å—ï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰freq_splitterå±æ€§ï¼‰
                if hasattr(self.model.disentangle, 'freq_splitter') and return_freq_info:
                    # é‡æ–°è°ƒç”¨disentangleè·å–é¢‘åŸŸä¿¡æ¯ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼Œä¸å‚ä¸æ¢¯åº¦ï¼‰
                    with torch.no_grad():
                        # ä»æ¨¡å‹ä¸­æå–image_embeds_raw
                        if self.model.vision_backbone_type == 'vim':
                            image_embeds_raw = self.model.visual_encoder(image)
                        else:
                            image_outputs = self.model.visual_encoder(image)
                            image_embeds_raw = image_outputs.last_hidden_state
                        image_embeds_raw = self.model.visual_proj(image_embeds_raw)
                        
                        # è°ƒç”¨disentangleè·å–freq_info
                        _, _, _, _, freq_info = self.model.disentangle(
                            image_embeds_raw, return_freq_info=True
                        )
            
            # === æŸå¤±è®¡ç®—ï¼ˆæ–°å¢freq_infoå‚æ•°ï¼‰===
            loss_dict = self.combined_loss(
                image_embeds=image_feats, id_text_embeds=id_text_feats, fused_embeds=fused_feats,
                id_logits=id_logits, id_embeds=id_embeds, cloth_embeds=cloth_embeds,
                cloth_text_embeds=cloth_text_embeds, cloth_image_embeds=cloth_image_embeds,
                pids=pid, is_matched=is_matched, epoch=epoch, gate=gate_stats,
                id_cls_features=id_cls_features, original_feat=original_feat,
                freq_info=freq_info  # ã€æ–°å¢ã€‘ä¼ é€’é¢‘åŸŸä¿¡æ¯
            )

        # === å¯è§†åŒ–å›è°ƒ ===
        if self.visualizer is not None and batch_idx % self.visualize_batch_interval == 0:
            # é¢‘åŸŸæ©ç å¯è§†åŒ–
            if freq_info is not None:
                self.visualizer.plot_frequency_masks(freq_info, epoch, batch_idx)
                
                # é¢‘åŸŸèƒ½é‡è°±
                if 'freq_magnitude' in freq_info:
                    self.visualizer.plot_frequency_energy_spectrum(freq_info, epoch, batch_idx)
            
            # é—¨æ§ç»Ÿè®¡
            if gate_stats is not None and isinstance(gate_stats, dict):
                # ä»gate_statsä¸­æå–å®é™…çš„gate tensorï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                # æ³¨æ„ï¼šå½“å‰gate_statsåªåŒ…å«ç»Ÿè®¡å€¼ï¼Œå¦‚æœéœ€è¦å¯è§†åŒ–éœ€è¦ä¿®æ”¹æ¨¡å‹è¿”å›gate tensor
                pass
        
        # è®°å½•æ¨¡å‹å†…éƒ¨çŠ¶æ€ä¿¡æ¯
        if self.monitor and batch_idx % 200 == 0:  # æ¯200ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            self.monitor.log_feature_statistics(image_feats, "image_features")
            self.monitor.log_feature_statistics(id_text_feats, "id_text_features")
            self.monitor.log_feature_statistics(fused_feats, "fused_features")
            self.monitor.log_feature_statistics(id_embeds, "identity_embeds")
            self.monitor.log_feature_statistics(cloth_embeds, "clothing_embeds")
            self.monitor.log_feature_statistics(cloth_text_embeds, "cloth_text_embeds")
            self.monitor.log_feature_statistics(cloth_image_embeds, "cloth_image_embeds")

            # gate_statsæ˜¯dictï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
            if gate_stats is not None and isinstance(gate_stats, dict):
                self.monitor.debug_logger.debug(
                    f"Gate stats: ID[{gate_stats.get('gate_id_mean', 0):.4f}], "
                    f"Attr[{gate_stats.get('gate_attr_mean', 0):.4f}], "
                    f"Diversity[{gate_stats.get('diversity', 0):.4f}]"
                )
                
                # ã€æ–°å¢ã€‘é¢‘åŸŸä¿¡æ¯è®°å½•
                if 'freq_type' in gate_stats:
                    self.monitor.debug_logger.debug(
                        f"Frequency: type={gate_stats.get('freq_type')}, "
                        f"energy={gate_stats.get('low_freq_energy', 0):.4f}"
                    )
            
            if gate_weights is not None:
                self.monitor.log_gate_weights(gate_weights, "fusion_gate")

            self.monitor.log_loss_components(loss_dict)

            # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
            self.monitor.log_memory_usage()

        return loss_dict

    def compute_similarity(self, train_loader):
        # è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸ä¼¼åº¦
        self.model.eval()
        with torch.no_grad():
            for image, cloth_captions, id_captions, pid, cam_id, is_matched in train_loader:
                image = image.to(self.device)
                outputs = self.model(image=image, cloth_instruction=cloth_captions, id_instruction=id_captions)
                # å¯¹ç§°è§£è€¦ï¼š12ä¸ªè¾“å‡º
                image_feats, id_text_feats, _, _, _, _, _, _, gate_weights, _, _, _ = outputs
                sim = torch.matmul(image_feats, id_text_feats.t())
                pos_sim = sim.diag().mean().item()
                neg_sim = sim[~torch.eye(sim.shape[0], dtype=bool, device=self.device)].mean().item()
                scale = self.model.scale
                return pos_sim, neg_sim, None, scale
        self.model.train()
        return None, None, None, None

    def _format_loss_display(self, loss_meters):
        # æ ¼å¼åŒ–æŸå¤±æ˜¾ç¤ºï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—å¹¶éšè—ç‰¹å®šé¡¹
        display_order = ['info_nce', 'cls', 'cloth_semantic', 'id_triplet', 'anti_collapse', 'gate_adaptive', 'reconstruction', 'total']
        hidden_losses = set()  # æ‰€æœ‰æŸå¤±éƒ½æ˜¾ç¤º

        avg_losses = []
        for key in display_order:
            if key in loss_meters and loss_meters[key].count > 0:
                avg_losses.append(f"{key}={loss_meters[key].avg:.4f}")

        return avg_losses

    def train(self, train_loader, optimizer, lr_scheduler, query_loader=None, gallery_loader=None, checkpoint_dir=None):
        # è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…å«æŸå¤±è®¡ç®—ã€ä¼˜åŒ–å’Œæ£€æŸ¥ç‚¹ä¿å­˜
        self.model.train()
        best_mAP = 0.0
        best_checkpoint_path = None
        total_batches = len(train_loader)
        loss_meters = {k: AverageMeter() for k in self.combined_loss.weights.keys() | {'total'}}
        
        # ã€æ–°å¢ã€‘æ—©åœæœºåˆ¶
        early_stopping = EarlyStopping(patience=10, min_delta=0.001, logger=self.monitor)
        
        # ã€æ–°å¢ã€‘å­¦ä¹ ç‡é¢„çƒ­å’Œå…¨å±€æ­¥æ•°
        warmup_steps = 1000
        global_step = 0

        for epoch in range(1, self.args.epochs + 1):
            # ã€æ–¹æ¡ˆBï¼šæ¸è¿›è§£å†»ç­–ç•¥ã€‘åœ¨ç‰¹å®šepochæ£€æŸ¥å¹¶è°ƒæ•´å†»ç»“çŠ¶æ€å’Œä¼˜åŒ–å™¨
            stage_changed = False
            if self.runner:
                if epoch == 11:  # Stage 2: Vimå8å±‚ + CLIPå1å±‚
                    print("\n" + "="*70)
                    if self.monitor: self.monitor.logger.info("ğŸ”“ Progressive Unfreezing: Stage 2")
                    if self.monitor: self.monitor.logger.info("=" * 70)
                    if self.monitor: self.monitor.logger.info("Epoch 11-30: Unfreezing Vim last 8 layers (layer 16-23)")
                    if self.monitor: self.monitor.logger.info("             + CLIP last 1 layer (layer 11)")
                    if self.monitor: self.monitor.logger.info("Goal: Initial adaptation of CLIP semantic space")
                    print("="*70 + "\n")
                    self.runner.freeze_text_layers(self.model, unfreeze_from_layer=11)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=4)
                    
                    # ã€æ–°å¢ã€‘é‡æ–°åˆå§‹åŒ–CLIP biasé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
                    self.reinit_clip_bias_layers(self.model, self.monitor)
                    
                    # ã€æ–°å¢ã€‘ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨
                    optimizer = self.build_optimizer_with_lr_groups(self.model, stage=2)
                    lr_scheduler = self.build_scheduler_with_cosine_warmup(
                        optimizer, 
                        num_training_steps=(self.args.epochs - 10) * total_batches,
                        num_warmup_steps=warmup_steps
                    )
                    
                    # ã€æ–°å¢ã€‘å¯ç”¨BatchNormé¢„çƒ­
                    self.enable_batch_norm_warmup(self.model, momentum=0.01)
                    
                    stage_changed = True
                    global_step = 0  # é‡ç½®å…¨å±€æ­¥æ•°
                elif epoch == 31:  # Stage 3: Vimå12å±‚ + CLIPå6å±‚
                    print("\n" + "="*70)
                    if self.monitor: self.monitor.logger.info("ğŸ”“ Progressive Unfreezing: Stage 3")
                    if self.monitor: self.monitor.logger.info("=" * 70)
                    if self.monitor: self.monitor.logger.info("Epoch 31-60: Unfreezing Vim last 12 layers")
                    if self.monitor: self.monitor.logger.info("             + CLIP last 6 layers (layer 6-11)")
                    if self.monitor: self.monitor.logger.info("Goal: Deep interaction tuning")
                    print("="*70 + "\n")
                    self.runner.freeze_text_layers(self.model, unfreeze_from_layer=6)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=6)
                    
                    # ã€æ–°å¢ã€‘ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨
                    optimizer = self.build_optimizer_with_lr_groups(self.model, stage=3)
                    lr_scheduler = self.build_scheduler_with_cosine_warmup(
                        optimizer,
                        num_training_steps=(self.args.epochs - 30) * total_batches,
                        num_warmup_steps=warmup_steps
                    )
                    
                    # ã€æ–°å¢ã€‘å¯ç”¨BatchNormé¢„çƒ­
                    self.enable_batch_norm_warmup(self.model, momentum=0.01)
                    
                    stage_changed = True
                    global_step = 0  # é‡ç½®å…¨å±€æ­¥æ•°
                elif epoch == 61:  # Stage 4: å…¨éƒ¨è§£å†»
                    print("\n" + "="*70)
                    if self.monitor: self.monitor.logger.info("ğŸ”“ Progressive Unfreezing: Stage 4")
                    if self.monitor: self.monitor.logger.info("=" * 70)
                    if self.monitor: self.monitor.logger.info("Epoch 61-80: Unfreezing all CLIP and Vim layers")
                    if self.monitor: self.monitor.logger.info("Goal: End-to-end fine-tuning")
                    print("="*70 + "\n")
                    self.runner.freeze_text_layers(self.model, unfreeze_from_layer=0)
                    self.runner.freeze_vit_layers(self.model, unfreeze_from_layer=0)
                    
                    # ã€æ–°å¢ã€‘ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨ï¼ˆæ‰€æœ‰å±‚ç›¸åŒå­¦ä¹ ç‡ï¼‰
                    optimizer = self._build_default_optimizer(self.model)
                    lr_scheduler = self.build_scheduler_with_cosine_warmup(
                        optimizer,
                        num_training_steps=(self.args.epochs - 60) * total_batches,
                        num_warmup_steps=warmup_steps
                    )
                    
                    # ã€æ–°å¢ã€‘å¯ç”¨BatchNormé¢„çƒ­
                    self.enable_batch_norm_warmup(self.model, momentum=0.01)
                    
                    stage_changed = True
                    global_step = 0  # é‡ç½®å…¨å±€æ­¥æ•°
            
            if stage_changed and self.monitor:
                self.monitor.logger.info(f"Stage changed at epoch {epoch}")
                if self.monitor:
                    self.monitor.logger.info(f"Learning rate warmup enabled for {warmup_steps} steps")
            
            # æ˜¾ç¤ºä¸Šä¸€ä¸ªepochçš„å¹³å‡æŸå¤±ï¼ˆä»…è®°å½•åˆ°æ—¥å¿—ï¼Œä¸åœ¨ç»ˆç«¯æ˜¾ç¤ºä»¥é¿å…é‡å¤ï¼‰
            if epoch > 1:
                avg_losses = self._format_loss_display(loss_meters)
                if avg_losses:
                    avg_loss_str = ', '.join(avg_losses)
                    # ä»…è®°å½•åˆ°æ—¥å¿—ï¼Œè¯„ä¼°é˜¶æ®µä¼šå•ç‹¬æ‰“å°æŸå¤±
                    if self.monitor:
                        self.monitor.logger.info(f"[Epoch {epoch-1} Avg Loss]: {avg_loss_str}")

            # é‡ç½®æŸå¤±è®°å½•å™¨
            for meter in loss_meters.values():
                meter.reset()

            progress_bar = tqdm(
                train_loader, desc=f"[Epoch {epoch}/{self.args.epochs}] Training",
                dynamic_ncols=True, leave=True, total=total_batches
            )

            # è®°å½•Epochåˆå§‹çŠ¶æ€ (LR & Loss Weights) -> ä»…å†™å…¥è°ƒè¯•æ—¥å¿—ï¼Œä¸æ˜¾ç¤ºåœ¨ç»ˆç«¯
            if self.monitor:
                current_lrs = [pg['lr'] for pg in optimizer.param_groups]
                lr_str = ", ".join([f"{lr:.2e}" for lr in current_lrs])
                
                # è·å–å½“å‰Lossæƒé‡
                weight_str = ", ".join([f"{k}={v:.2f}" for k, v in self.combined_loss.weights.items() if v > 0])
                
                self.monitor.debug_logger.info(f"Epoch {epoch} Start | LRs: [{lr_str}] | Active Weights: [{weight_str}]")

            for i, inputs in enumerate(progress_bar):
                # ã€æ–°å¢ã€‘å­¦ä¹ ç‡é¢„çƒ­
                if stage_changed and global_step < warmup_steps:
                    for param_group in optimizer.param_groups:
                        base_lr = param_group.get('initial_lr', param_group['lr'])
                        warmup_lr = self._get_warmup_lr(base_lr, global_step, warmup_steps)
                        param_group['lr'] = warmup_lr
                
                optimizer.zero_grad()
                loss_dict = self.run(inputs, epoch, i, total_batches)
                loss = loss_dict['total']

                if self.scaler:
                    self.scaler.scale(loss).backward()
                    
                    # [Fix] Check for gradients BEFORE unscale to prevent scaler errors
                    has_grads = any(p.grad is not None for group in optimizer.param_groups for p in group['params'])
                    
                    if has_grads:
                        # ã€ä¿®æ”¹ã€‘ä½¿ç”¨åˆ†å±‚æ¢¯åº¦è£å‰ª
                        self.scaler.unscale_(optimizer)
                        self.clip_grad_norm_by_layer(self.model, max_norm=5.0)

                        # è®°å½•æ¢¯åº¦ä¿¡æ¯ï¼ˆæ¯100ä¸ªbatchï¼‰
                        if self.monitor and i % 100 == 0:
                            # log_gradients ç°åœ¨åŒ…å«äº†åŸæ¥çš„ flow analysis åŠŸèƒ½
                            self.monitor.log_gradients(self.model, f"epoch_{epoch}_batch_{i}")

                        self.scaler.step(optimizer)
                        self.scaler.update()
                    else:
                        if self.monitor: self.monitor.debug_logger.warning(f"âš ï¸  Skipping step at epoch {epoch} batch {i}: No gradients found (likely disconnected graph or unused params).")
                        # Debug info for first occurrence
                        if i == 0:
                            trainable_params = [n for n, p in self.model.named_parameters() if p.requires_grad]
                            if self.monitor: self.monitor.debug_logger.warning(f"Trainable params count: {len(trainable_params)}")
                            if self.monitor: self.monitor.debug_logger.warning("Sample trainable params with None grad:")
                            count = 0
                            for n, p in self.model.named_parameters():
                                if p.requires_grad and p.grad is None:
                                    if self.monitor: self.monitor.debug_logger.warning(f"  - {n}")
                                    count += 1
                                    if count > 10: break
                else:
                    loss.backward()
                    
                    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨åˆ†å±‚æ¢¯åº¦è£å‰ª
                    self.clip_grad_norm_by_layer(self.model, max_norm=5.0)

                    # è®°å½•æ¢¯åº¦ä¿¡æ¯ï¼ˆæ¯100ä¸ªbatchï¼‰
                    if self.monitor and i % 100 == 0:
                        self.monitor.log_gradients(self.model, f"epoch_{epoch}_batch_{i}")

                    optimizer.step()

                # æ›´æ–°æŸå¤±è®°å½•
                for key, val in loss_dict.items():
                    if key in loss_meters:
                        loss_meters[key].update(val.item() if isinstance(val, torch.Tensor) else val)
                
                # è®°å½•è¯¦ç»†æŸå¤±åˆ†è§£ï¼ˆæ¯100ä¸ªbatchï¼‰
                if self.monitor and i % 100 == 0:
                    self.monitor.log_loss_breakdown(loss_dict, epoch, i)

                # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
                if self.monitor and i % 200 == 0:  # æ¯200ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                    current_lr = optimizer.param_groups[0]['lr']
                    self.monitor.log_batch_info(epoch, i, total_batches,
                                              {k: v.avg for k, v in loss_meters.items()},
                                              current_lr, print_to_console=False)
                
                global_step += 1

            progress_bar.close()
            
            # åªåœ¨stageæœªæ”¹å˜æ—¶è°ƒç”¨lr_scheduler.step()
            if not stage_changed:
                lr_scheduler.step()

            # è®°å½•epochä¿¡æ¯
            if self.monitor:
                epoch_metrics = {k: v.avg for k, v in loss_meters.items()}
                self.monitor.log_epoch_info(epoch, self.args.epochs, epoch_metrics)

            # === æ¸…ç†æ˜¾å­˜ï¼Œå‡†å¤‡è¯„ä¼° ===
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¯ä¸ªepochç»“æŸåè¿›è¡Œè¯„ä¼°
            if query_loader and gallery_loader:
                # è¯„ä¼°æ¨¡å‹
                evaluator = Evaluator(self.model, args=self.args)
                metrics = evaluator.evaluate(
                    query_loader, gallery_loader, query_loader.dataset.data,
                    gallery_loader.dataset.data, checkpoint_path=None, epoch=epoch
                )

                current_mAP = metrics['mAP']

                # åŒæ—¶åœ¨ç»ˆç«¯å’Œæ—¥å¿—æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                print(f"\n{'='*60}")
                print(f"Epoch {epoch} Evaluation Results:")
                print(f"  mAP:    {metrics['mAP']:.4f}")
                print(f"  Rank-1: {metrics['rank1']:.4f}")
                print(f"  Rank-5: {metrics['rank5']:.4f}")
                print(f"  Rank-10: {metrics['rank10']:.4f}")
                print(f"{'='*60}\n")
                
                # åŒæ—¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                if self.monitor:
                    self.monitor.logger.info(f"Epoch {epoch}: mAP={metrics['mAP']:.4f}, R1={metrics['rank1']:.4f}, R5={metrics['rank5']:.4f}, R10={metrics['rank10']:.4f}")

                # ã€æ–°å¢ã€‘æ—©åœæ£€æŸ¥
                early_stopping(current_mAP)
                if early_stopping.early_stop:
                    if self.monitor:
                        self.monitor.logger.info(f"Training stopped early at epoch {epoch}")
                    break

                # ä¿å­˜æœ€ä¼˜æ£€æŸ¥ç‚¹
                if current_mAP > best_mAP:
                    best_mAP = current_mAP

                    # ç”Ÿæˆæœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„
                    if checkpoint_dir:
                        # ç¡®ä¿ checkpoint_dir æ˜¯ Path å¯¹è±¡
                        ckpt_dir_path = Path(checkpoint_dir)
                        
                        # åˆ›å»º model å­ç›®å½•
                        model_dir = ckpt_dir_path / 'model'
                        model_dir.mkdir(parents=True, exist_ok=True)

                        # è·å–æ•°æ®é›†çŸ­åç§°ç”¨äºæ–‡ä»¶å (ä¾‹å¦‚ cuhk, rstp, icfg)
                        dataset_short_name = self._get_dataset_name()
                        
                        # æ„å»ºå®Œæ•´è·¯å¾„: log/dataset_name/model/best_dataset.pth
                        new_best_checkpoint_path = str(model_dir / f"best_{dataset_short_name}.pth")

                        # åˆ é™¤æ—§çš„æœ€ä½³æ£€æŸ¥ç‚¹
                        if best_checkpoint_path and Path(best_checkpoint_path).exists():
                            try:
                                Path(best_checkpoint_path).unlink()
                                if self.monitor:
                                    self.monitor.logger.info(f"Removed old best checkpoint: {best_checkpoint_path}")
                            except OSError:
                                if self.monitor:
                                    self.monitor.logger.warning(f"Could not remove old best checkpoint: {best_checkpoint_path}")

                        # ä¿å­˜æ–°çš„æœ€ä½³æ£€æŸ¥ç‚¹
                        save_checkpoint({
                            'model': self.model.state_dict(),
                            'optimizer': optimizer.state_dict(),
                            'lr_scheduler': lr_scheduler.state_dict(),
                            'epoch': epoch,
                            'mAP': current_mAP
                        }, fpath=new_best_checkpoint_path)

                        best_checkpoint_path = new_best_checkpoint_path

                        if self.monitor:
                            self.monitor.debug_logger.debug(f"New best checkpoint saved: {best_checkpoint_path}, mAP: {best_mAP:.4f}")
                    else:
                        if self.monitor:
                            self.monitor.logger.warning("checkpoint_dir not provided, cannot save best checkpoint")

        # æ˜¾ç¤ºè®­ç»ƒå®Œæˆä¿¡æ¯ï¼ˆç»ˆç«¯+æ—¥å¿—ï¼‰
        print(f"\n{'='*60}")
        print(f"ğŸ‰ Training Completed!")
        print(f"   Best mAP: {best_mAP:.4f}")
        if best_checkpoint_path:
            print(f"   Best Model: {best_checkpoint_path}")
        print(f"{'='*60}\n")
        
        if self.monitor:
            self.monitor.logger.info(f"Training completed. Best mAP: {best_mAP:.4f}")

        # æ˜¾ç¤ºæœ€ç»ˆå¹³å‡æŸå¤±
        avg_losses = self._format_loss_display(loss_meters)
        if avg_losses:
            avg_loss_str = ', '.join(avg_losses)
            print(f"[Final Avg Loss]: {avg_loss_str}")
            if self.monitor:
                self.monitor.logger.info(f"[Final Avg Loss]: {avg_loss_str}")

        if best_checkpoint_path:
            if self.monitor: self.monitor.logger.info(f"Final best checkpoint: {best_checkpoint_path}, mAP: {best_mAP:.4f}")

    def _get_dataset_name(self):
        """è·å–æ•°æ®é›†åç§°ç”¨äºæ¨¡å‹æ–‡ä»¶å‘½å"""
        if hasattr(self.args, 'dataset_configs') and self.args.dataset_configs:
            dataset_name = self.args.dataset_configs[0]['name'].lower()
            if 'cuhk' in dataset_name:
                return 'cuhk'
            elif 'rstp' in dataset_name:
                return 'rstp'
            elif 'icfg' in dataset_name:
                return 'icfg'
            else:
                return dataset_name
        else:
            return 'unknown'
