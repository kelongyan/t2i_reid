# trainers/train_loop.py
import torch
import torch.nn as nn
from tqdm import tqdm
from pathlib import Path
from evaluators.evaluator import Evaluator
from utils.serialization import save_checkpoint

def create_discriminator_optimizer(model, lr=1e-4):
    # ä¸ºå¯¹æŠ—è§£è€¦æ¨¡å—ä¸­çš„åˆ¤åˆ«å™¨ï¼ˆå±æ€§åˆ¤åˆ«å™¨ä¸åŸŸåˆ¤åˆ«å™¨ï¼‰åˆ›å»ºç‹¬ç«‹ä¼˜åŒ–å™¨
    disc_params = []
    if hasattr(model, 'adversarial_decoupler'):
        disc_params.extend(model.adversarial_decoupler.attr_disc.parameters())
        if hasattr(model.adversarial_decoupler, 'domain_disc'):
            disc_params.extend(model.adversarial_decoupler.domain_disc.parameters())
    
    if len(disc_params) > 0:
        return torch.optim.Adam(disc_params, lr=lr, betas=(0.5, 0.999))
    return None


def curriculum_train_epoch(trainer, train_loader, optimizer, optimizer_disc, epoch, total_batches):
    # è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ä¸‹çš„å•ä¸ª Epoch è®­ç»ƒé€»è¾‘
    from utils.meters import AverageMeter
    
    trainer.model.train()
    loss_meters = {}
    
    # è·å–å¹¶æ›´æ–°å½“å‰é˜¶æ®µçš„æŸå¤±æƒé‡
    phase = trainer.curriculum.get_current_phase(epoch)
    weights = trainer.curriculum.get_loss_weights(epoch, trainer.performance_history)
    trainer.loss.update_weights(weights)
    
    # é˜¶æ®µä¿¡æ¯å±•ç¤ºä¸å¯¹æŠ—å‚æ•°æ›´æ–°
    trainer.curriculum.print_phase_summary(epoch)
    if hasattr(trainer.model, 'adversarial_decoupler'):
        progress = (epoch - 1) / trainer.args.epochs
        trainer.model.adversarial_decoupler.update_lambda(progress)
    
    # åŠ¨æ€è°ƒæ•´è¿›åº¦æ¡å®½åº¦ï¼šä¼˜å…ˆå°è¯•ä» STDIN (fd=0) è·å–å®½åº¦ï¼Œä»¥ç»•è¿‡ pipe/tee çš„é™åˆ¶
    import shutil
    import os
    try:
        # å°è¯•ä» stdin è·å–çœŸå®çš„ TTY å®½åº¦
        term_width = os.get_terminal_size(0).columns
    except OSError:
        # å¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚åå°è¿è¡Œï¼‰ï¼Œå›é€€åˆ° shutil æ£€æµ‹
        term_width = shutil.get_terminal_size((80, 20)).columns
    
    tqdm_width = term_width

    for key in weights.keys():
        if key not in loss_meters:
            loss_meters[key] = AverageMeter()
    if 'total' not in loss_meters:
        loss_meters['total'] = AverageMeter()
    
    progress_bar = tqdm(
        train_loader, 
        desc=f"[Phase {phase}] [Epoch {epoch}/{trainer.args.epochs}]",
        ncols=tqdm_width, 
        leave=True,
        total=total_batches
    )
    
    for batch_idx, inputs in enumerate(progress_bar):
        # ---- ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒç‰¹å¾æå–å™¨ï¼ˆä¸»æ¨¡å‹ï¼‰ ----
        optimizer.zero_grad()
        
        # è®¡ç®—ç‰¹å¾æå–æŸå¤±
        loss_dict = trainer.run(inputs, epoch, batch_idx, total_batches, training_phase='feature')
        loss = loss_dict['total']
        
        # å¼‚å¸¸æ£€æµ‹ä¸åå‘ä¼ æ’­
        if torch.isnan(loss).any() or torch.isinf(loss).any():
            if trainer.monitor:
                trainer.monitor.logger.error(f"âŒ Epoch {epoch} Batch {batch_idx} å‡ºç° NaN/Inf æŸå¤±ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
            continue
        
        if trainer.scaler:
            trainer.scaler.scale(loss).backward()
            has_grads = any(p.grad is not None for group in optimizer.param_groups for p in group['params'])
            if has_grads:
                trainer.scaler.unscale_(optimizer)
                trainer.clip_grad_norm_by_layer(trainer.model, max_norm=1.0)
                trainer.scaler.step(optimizer)
                trainer.scaler.update()
        else:
            loss.backward()
            trainer.clip_grad_norm_by_layer(trainer.model, max_norm=1.0)
            optimizer.step()
        
        # ---- ç¬¬äºŒæ­¥ï¼šè®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆä»…åœ¨è§£è€¦é˜¶æ®µå¼€å¯ï¼Œä¸”æŒ‰é¢‘ç‡è§¦å‘ï¼‰ ----
        if phase >= 2 and optimizer_disc is not None:
            if trainer.curriculum.should_train_discriminator(epoch, batch_idx, total_batches):
                optimizer_disc.zero_grad()
                loss_dict_disc = trainer.run(inputs, epoch, batch_idx, total_batches, training_phase='discriminator')
                loss_disc = loss_dict_disc['total']
                
                if not (torch.isnan(loss_disc).any() or torch.isinf(loss_disc).any()):
                    loss_disc.backward()
                    optimizer_disc.step()
                    for key in ['discriminator_attr', 'discriminator_domain']:
                        if key in loss_dict_disc and key in loss_meters:
                            loss_meters[key].update(loss_dict_disc[key].item())
        
        # æ›´æ–°æŸå¤±ç»Ÿè®¡ä¸è¿›åº¦æ¡å±•ç¤º
        for key, val in loss_dict.items():
            if key in loss_meters:
                loss_meters[key].update(val.item() if isinstance(val, torch.Tensor) else val)
        
        display_loss = loss.item()
        progress_bar.set_postfix_str(f"loss: {display_loss:.4f}")
    
    progress_bar.close()
    return loss_meters


def train_with_curriculum(trainer, train_loader, query_loader, gallery_loader, checkpoint_dir):
    # å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹æ§åˆ¶å™¨
    from trainers.trainer import EarlyStopping
    
    # å®ä¾‹åŒ–ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        [p for p in trainer.model.parameters() if p.requires_grad],
        lr=trainer.args.lr,
        weight_decay=trainer.args.weight_decay
    )
    optimizer_disc = create_discriminator_optimizer(trainer.model, lr=trainer.args.lr * 0.5)
    
    from torch.optim.lr_scheduler import CosineAnnealingLR
    lr_scheduler = CosineAnnealingLR(optimizer, T_max=trainer.args.epochs)
    early_stopping = EarlyStopping(patience=20, min_delta=0.001, logger=trainer.monitor)
    
    best_mAP = 0.0
    best_checkpoint_path = None
    total_batches = len(train_loader)
    
    # éå†æ¯ä¸ª Epoch
    for epoch in range(1, trainer.args.epochs + 1):
        loss_meters = curriculum_train_epoch(
            trainer, train_loader, optimizer, optimizer_disc, epoch, total_batches
        )
        
        # å­¦ä¹ ç‡åŠ¨æ€è°ƒæ•´
        phase = trainer.curriculum.get_current_phase(epoch)
        lr_mult = trainer.curriculum.get_learning_rate_multiplier(epoch)
        lr_scheduler.step()
        for param_group in optimizer.param_groups:
            param_group['lr'] *= lr_mult
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å®šæœŸæ‰§è¡Œæ¨¡å‹è¯„ä¼°
        if query_loader and gallery_loader:
            evaluator = Evaluator(trainer.model, args=trainer.args)
            metrics = evaluator.evaluate(
                query_loader, gallery_loader,
                query_loader.dataset.data, gallery_loader.dataset.data,
                checkpoint_path=None, epoch=epoch
            )
            
            current_mAP, current_rank1 = metrics['mAP'], metrics['rank1']
            print(f"\n{'='*60}\nğŸ“Š Epoch {epoch} [é˜¶æ®µ {phase}] è¯„ä¼°ç»“æœ:\n"
                  f"  Rank-1: {metrics['rank1']:.3f} | Rank-5: {metrics['rank5']:.3f} | Rank-10: {metrics['rank10']:.3f} | mAP: {metrics['mAP']:.3f}\n{'='*60}\n")
            
            trainer.performance_history.append({
                'epoch': epoch, 'mAP': current_mAP, 'rank1': current_rank1,
                'rank5': metrics['rank5'], 'rank10': metrics['rank10']
            })
            
            # æ—©åœæ£€æŸ¥ä¸æœ€ä½³æ¨¡å‹ä¿å­˜
            early_stopping(current_mAP)
            if early_stopping.early_stop:
                if trainer.monitor: trainer.monitor.logger.info(f"æ€§èƒ½è¿ç»­æœªæå‡ï¼Œåœ¨ Epoch {epoch} è§¦å‘æ—©åœ")
                break
            
            if current_mAP > best_mAP:
                best_mAP = current_mAP
                if checkpoint_dir:
                    model_dir = Path(checkpoint_dir) / 'model'
                    model_dir.mkdir(parents=True, exist_ok=True)
                    dataset_name = trainer._get_dataset_name()
                    new_best_path = str(model_dir / f"best_{dataset_name}.pth")
                    
                    if best_checkpoint_path and Path(best_checkpoint_path).exists():
                        Path(best_checkpoint_path).unlink()
                    
                    save_checkpoint({
                        'model': trainer.model.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'lr_scheduler': lr_scheduler.state_dict(),
                        'epoch': epoch, 'mAP': current_mAP, 'phase': phase
                    }, fpath=new_best_path)
                    
                    best_checkpoint_path = new_best_path
                    if trainer.monitor:
                        trainer.monitor.logger.info(f"âœ… åˆ·æ–°æœ€ä½³è®°å½•: mAP={best_mAP:.3f}, æ¨¡å‹å·²ä¿å­˜è‡³ {best_checkpoint_path}")
        
        # é˜¶æ®µè‡ªåŠ¨è¿‡æ¸¡æ£€æµ‹
        if trainer.curriculum.should_transition_phase(epoch, trainer.performance_history):
            if trainer.monitor: trainer.monitor.logger.info(f"ğŸš€ æ€§èƒ½è¾¾æ ‡ï¼Œåœ¨ Epoch {epoch} è§¦å‘é˜¶æ®µè‡ªåŠ¨è¿‡æ¸¡")
    
    # è®­ç»ƒç»“æŸæ€»ç»“
    import shutil
    width = min(max(shutil.get_terminal_size((80, 20)).columns, 80), 100)
    print(f"\n{'='*width}\nğŸ‰ è®­ç»ƒä»»åŠ¡åœ†æ»¡å®Œæˆï¼\n   æœ€ä½³ mAP æŒ‡æ ‡: {best_mAP:.4f}\n"
          f"   æ¨¡å‹æ£€æŸ¥ç‚¹: {best_checkpoint_path}\n{'='*width}\n")
    
    return best_mAP, best_checkpoint_path
