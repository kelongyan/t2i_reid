# trainers/train_loop.py
"""
è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå¾ªç¯æ ¸å¿ƒé€»è¾‘
ç”¨äºæ›¿æ¢trainer.pyä¸­çš„trainæ–¹æ³•
"""

import torch
import torch.nn as nn
from tqdm import tqdm
from pathlib import Path
from evaluators.evaluator import Evaluator
from utils.serialization import save_checkpoint


def create_discriminator_optimizer(model, lr=1e-4):
    """ä¸ºåˆ¤åˆ«å™¨åˆ›å»ºç‹¬ç«‹çš„ä¼˜åŒ–å™¨"""
    disc_params = []
    if hasattr(model, 'adversarial_decoupler'):
        disc_params.extend(model.adversarial_decoupler.attr_disc.parameters())
        if hasattr(model.adversarial_decoupler, 'domain_disc'):
            disc_params.extend(model.adversarial_decoupler.domain_disc.parameters())
    
    if len(disc_params) > 0:
        return torch.optim.Adam(disc_params, lr=lr, betas=(0.5, 0.999))
    return None


def curriculum_train_epoch(trainer, train_loader, optimizer, optimizer_disc, epoch, total_batches):
    """
    è¯¾ç¨‹å­¦ä¹ å•ä¸ªepochè®­ç»ƒ
    
    Args:
        trainer: Trainerå®ä¾‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        optimizer: ä¸»ä¼˜åŒ–å™¨ï¼ˆç‰¹å¾æå–å™¨ï¼‰
        optimizer_disc: åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        total_batches: æ€»batchæ•°
    
    Returns:
        loss_meters: dict of AverageMeter
    """
    from utils.meters import AverageMeter
    
    trainer.model.train()
    loss_meters = {}
    
    # è·å–å½“å‰é˜¶æ®µé…ç½®
    phase = trainer.curriculum.get_current_phase(epoch)
    weights = trainer.curriculum.get_loss_weights(epoch, trainer.performance_history)
    
    # æ›´æ–°Lossæƒé‡
    trainer.loss.update_weights(weights)
    
    # æ‰“å°é˜¶æ®µæ‘˜è¦ï¼ˆæ¯ä¸ªepochå¼€å§‹æ—¶ï¼‰
    trainer.curriculum.print_phase_summary(epoch)
    
    # æ›´æ–°å¯¹æŠ—æ¨¡å—çš„lambda
    if hasattr(trainer.model, 'adversarial_decoupler'):
        progress = (epoch - 1) / trainer.args.epochs
        trainer.model.adversarial_decoupler.update_lambda(progress)
    
    # åˆå§‹åŒ–loss meters
    for key in weights.keys():
        if key not in loss_meters:
            loss_meters[key] = AverageMeter()
    if 'total' not in loss_meters:
        loss_meters['total'] = AverageMeter()
    
    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(
        train_loader, 
        desc=f"[Phase {phase}] [Epoch {epoch}/{trainer.args.epochs}]",
        dynamic_ncols=True, 
        leave=True,
        total=total_batches
    )
    
    for batch_idx, inputs in enumerate(progress_bar):
        # ==== Step 1: è®­ç»ƒç‰¹å¾æå–å™¨ ====
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­ + è®¡ç®—æŸå¤±ï¼ˆtraining_phase='feature'ï¼‰
        loss_dict = trainer.run(inputs, epoch, batch_idx, total_batches, training_phase='feature')
        loss = loss_dict['total']
        
        # NaNæ£€æµ‹
        if torch.isnan(loss).any() or torch.isinf(loss).any():
            if trainer.monitor:
                trainer.monitor.logger.error(f"âŒ NaN/Inf loss at E{epoch} B{batch_idx}, skipping")
            continue
        
        # åå‘ä¼ æ’­
        if trainer.scaler:
            trainer.scaler.scale(loss).backward()
            
            # æ£€æŸ¥æ¢¯åº¦
            has_grads = any(p.grad is not None for group in optimizer.param_groups for p in group['params'])
            
            if has_grads:
                trainer.scaler.unscale_(optimizer)
                trainer.clip_grad_norm_by_layer(trainer.model, max_norm=1.0)  # ğŸ”¥ é™ä½åˆ°1.0
                trainer.scaler.step(optimizer)
                trainer.scaler.update()
        else:
            loss.backward()
            trainer.clip_grad_norm_by_layer(trainer.model, max_norm=1.0)
            optimizer.step()
        
        # ==== Step 2: è®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆPhase 2/3ï¼Œæ¯2ä¸ªbatchï¼‰====
        if phase >= 2 and optimizer_disc is not None:
            if trainer.curriculum.should_train_discriminator(epoch, batch_idx, total_batches):
                optimizer_disc.zero_grad()
                
                # é‡æ–°å‰å‘ä¼ æ’­ï¼ˆtraining_phase='discriminator'ï¼‰
                loss_dict_disc = trainer.run(inputs, epoch, batch_idx, total_batches, training_phase='discriminator')
                loss_disc = loss_dict_disc['total']
                
                if not (torch.isnan(loss_disc).any() or torch.isinf(loss_disc).any()):
                    loss_disc.backward()
                    optimizer_disc.step()
                    
                    # è®°å½•åˆ¤åˆ«å™¨æŸå¤±
                    for key in ['discriminator_attr', 'discriminator_domain']:
                        if key in loss_dict_disc and key in loss_meters:
                            loss_meters[key].update(loss_dict_disc[key].item())
        
        # ==== Step 3: æ›´æ–°loss meters ====
        for key, val in loss_dict.items():
            if key in loss_meters:
                if isinstance(val, torch.Tensor):
                    loss_meters[key].update(val.item())
                else:
                    loss_meters[key].update(val)
        
        # ==== Step 4: æ›´æ–°è¿›åº¦æ¡ ====
        # åªæ˜¾ç¤ºä¸»è¦æŸå¤±
        progress_str = f"Loss: {loss.item():.4f}"
        if 'id_triplet' in loss_dict:
            progress_str += f" | Triplet: {loss_dict['id_triplet'].item():.4f}"
        progress_bar.set_postfix_str(progress_str)
    
    progress_bar.close()
    return loss_meters


def train_with_curriculum(trainer, train_loader, query_loader, gallery_loader, checkpoint_dir):
    """
    å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹
    
    Args:
        trainer: Trainerå®ä¾‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        query_loader: æŸ¥è¯¢é›†åŠ è½½å™¨
        gallery_loader: å›¾åº“åŠ è½½å™¨
        checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    """
    from trainers.trainer import EarlyStopping
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        [p for p in trainer.model.parameters() if p.requires_grad],
        lr=trainer.args.lr,
        weight_decay=trainer.args.weight_decay
    )
    
    # åˆ›å»ºåˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
    optimizer_disc = create_discriminator_optimizer(trainer.model, lr=trainer.args.lr * 0.5)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    from torch.optim.lr_scheduler import CosineAnnealingLR
    lr_scheduler = CosineAnnealingLR(optimizer, T_max=trainer.args.epochs)
    
    # æ—©åœ
    early_stopping = EarlyStopping(patience=20, min_delta=0.001, logger=trainer.monitor)
    
    # è®­ç»ƒçŠ¶æ€
    best_mAP = 0.0
    best_checkpoint_path = None
    total_batches = len(train_loader)
    
    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(1, trainer.args.epochs + 1):
        # è®­ç»ƒä¸€ä¸ªepoch
        loss_meters = curriculum_train_epoch(
            trainer, train_loader, optimizer, optimizer_disc, epoch, total_batches
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ ¹æ®å½“å‰é˜¶æ®µåŠ¨æ€è°ƒæ•´ï¼‰
        phase = trainer.curriculum.get_current_phase(epoch)
        lr_mult = trainer.curriculum.get_learning_rate_multiplier(epoch)
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr_scheduler.step()
        for param_group in optimizer.param_groups:
            param_group['lr'] *= lr_mult
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ==== è¯„ä¼° ====
        if query_loader and gallery_loader:
            evaluator = Evaluator(trainer.model, args=trainer.args)
            metrics = evaluator.evaluate(
                query_loader, gallery_loader,
                query_loader.dataset.data,
                gallery_loader.dataset.data,
                checkpoint_path=None,
                epoch=epoch
            )
            
            current_mAP = metrics['mAP']
            current_rank1 = metrics['rank1']
            
            # æ‰“å°è¯„ä¼°ç»“æœ
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Epoch {epoch} [Phase {phase}] Evaluation:")
            print(f"  mAP:     {metrics['mAP']:.4f}")
            print(f"  Rank-1:  {metrics['rank1']:.4f}")
            print(f"  Rank-5:  {metrics['rank5']:.4f}")
            print(f"  Rank-10: {metrics['rank10']:.4f}")
            print(f"{'='*60}\n")
            
            # è®°å½•æ€§èƒ½å†å²
            trainer.performance_history.append({
                'epoch': epoch,
                'mAP': current_mAP,
                'rank1': current_rank1,
                'rank5': metrics['rank5'],
                'rank10': metrics['rank10']
            })
            
            # æ—©åœæ£€æŸ¥
            early_stopping(current_mAP)
            if early_stopping.early_stop:
                if trainer.monitor:
                    trainer.monitor.logger.info(f"Early stopping at epoch {epoch}")
                break
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_mAP > best_mAP:
                best_mAP = current_mAP
                
                if checkpoint_dir:
                    ckpt_dir_path = Path(checkpoint_dir)
                    model_dir = ckpt_dir_path / 'model'
                    model_dir.mkdir(parents=True, exist_ok=True)
                    
                    dataset_name = trainer._get_dataset_name()
                    new_best_checkpoint_path = str(model_dir / f"best_{dataset_name}.pth")
                    
                    # åˆ é™¤æ—§checkpoint
                    if best_checkpoint_path and Path(best_checkpoint_path).exists():
                        Path(best_checkpoint_path).unlink()
                    
                    # ä¿å­˜æ–°checkpoint
                    save_checkpoint({
                        'model': trainer.model.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'lr_scheduler': lr_scheduler.state_dict(),
                        'epoch': epoch,
                        'mAP': current_mAP,
                        'phase': phase
                    }, fpath=new_best_checkpoint_path)
                    
                    best_checkpoint_path = new_best_checkpoint_path
                    
                    if trainer.monitor:
                        trainer.monitor.logger.info(f"âœ… New best: mAP={best_mAP:.4f}, saved to {best_checkpoint_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰è¿‡æ¸¡é˜¶æ®µ
        if trainer.curriculum.should_transition_phase(epoch, trainer.performance_history):
            if trainer.monitor:
                trainer.monitor.logger.info(f"ğŸš€ Phase transition triggered at epoch {epoch}")
    
    # è®­ç»ƒå®Œæˆ
    print(f"\n{'='*70}")
    print(f"ğŸ‰ Training Completed!")
    print(f"   Best mAP: {best_mAP:.4f}")
    if best_checkpoint_path:
        print(f"   Best Model: {best_checkpoint_path}")
    print(f"{'='*70}\n")
    
    return best_mAP, best_checkpoint_path
