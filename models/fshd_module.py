# models/fshd_module.py
"""
FSHD-Netå®Œæ•´æ¨¡å—
æ•´åˆé¢‘åŸŸåˆ†è§£ã€å¼‚æ„åŒæµã€é¢‘åŸŸå¼•å¯¼æ³¨æ„åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .frequency_module import get_frequency_splitter
from .hybrid_stream import HybridDualStream, FrequencyGuidedAttention


class FSHDModule(nn.Module):
    """
    Frequency-Spatial Hybrid Decoupling Module
    
    å®Œæ•´æµç¨‹ï¼š
    1. é¢‘åŸŸåˆ†è§£ï¼šDCT â†’ ä½é¢‘/é«˜é¢‘ç‰¹å¾
    2. é¢‘åŸŸå¼•å¯¼æ³¨æ„åŠ›ï¼šä½é¢‘â†’IDåˆ†æ”¯ï¼Œé«˜é¢‘â†’Attråˆ†æ”¯
    3. å¼‚æ„åŒæµå»ºæ¨¡ï¼šMamba(ID) + Multi-scale CNN(Attr)
    4. å¯¹ç§°é—¨æ§ï¼šç‹¬ç«‹é—¨æ§æœºåˆ¶
    """
    
    def __init__(self, dim=768, num_heads=8, d_state=16, d_conv=4, dropout=0.1,
                 img_size=(14, 14), use_multi_scale_cnn=True, logger=None):
        """
        Args:
            dim: ç‰¹å¾ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_state: MambaçŠ¶æ€ç»´åº¦
            d_conv: Mambaå·ç§¯æ ¸å¤§å°
            dropout: Dropoutæ¯”ç‡
            img_size: å›¾åƒpatch gridå°ºå¯¸
            use_multi_scale_cnn: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦CNN
            logger: æ—¥å¿—è®°å½•å™¨
        """
        super().__init__()
        self.logger = logger
        
        # === é˜¶æ®µ1ï¼šé¢‘åŸŸåˆ†è§£ (å›ºå®šä¸ºDCT) ===
        self.freq_splitter = get_frequency_splitter(
            dim=dim,
            img_size=img_size
        )
        if logger:
            logger.debug_logger.info(f"âœ… FSHD: Using DCT frequency splitter")
        
        # === é˜¶æ®µ2ï¼šé¢‘åŸŸå¼•å¯¼æ³¨æ„åŠ› ===
        self.freq_guided_attn = FrequencyGuidedAttention(
            dim=dim,
            num_heads=num_heads,
            dropout=dropout
        )
        if logger:
            logger.debug_logger.info("âœ… FSHD: Frequency-guided attention initialized")
        
        # === é˜¶æ®µ3ï¼šå¼‚æ„åŒæµ ===
        self.hybrid_stream = HybridDualStream(
            dim=dim,
            d_state=d_state,
            d_conv=d_conv,
            expand=2,
            use_multi_scale_cnn=use_multi_scale_cnn,
            logger=logger
        )
        if logger:
            stream_type = "Multi-scale CNN" if use_multi_scale_cnn else "Lightweight Mamba"
            logger.debug_logger.info(f"âœ… FSHD: Hybrid stream with {stream_type}")
        
        # === é˜¶æ®µ4ï¼šå¯¹ç§°é—¨æ§ ===
        self.gate_id = nn.Sequential(
            nn.Linear(dim * 2, dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim // 4, dim),
            nn.Sigmoid()
        )
        
        self.gate_attr = nn.Sequential(
            nn.Linear(dim * 2, dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim // 4, dim),
            nn.Sigmoid()
        )
        
        # å…¨å±€æ± åŒ–
        self.pool_id = nn.AdaptiveAvgPool1d(1)
        self.pool_attr = nn.AdaptiveAvgPool1d(1)
        
        if logger:
            logger.debug_logger.info("=" * 60)
            logger.debug_logger.info("FSHD-Net Architecture Summary:")
            logger.debug_logger.info(f"  1. Frequency Decomposition: DCT")
            logger.debug_logger.info(f"  2. Attention: Frequency-Guided + Soft Orthogonal")
            logger.debug_logger.info(f"  3. Dual Stream: Mamba(ID) + {'Multi-CNN' if use_multi_scale_cnn else 'Light-Mamba'}(Attr)")
            logger.debug_logger.info(f"  4. Gating: Symmetric Independent Gates")
            logger.debug_logger.info("=" * 60)
    
    def forward(self, x, return_attention=False, return_freq_info=False):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, N, D]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›å›¾
            return_freq_info: æ˜¯å¦è¿”å›é¢‘åŸŸä¿¡æ¯
        Returns:
            å¦‚æœ return_attention=False and return_freq_info=False:
                (id_feat, attr_feat, gate_stats, original_feat)
            å¦‚æœ return_attention=True:
                + (id_attn_map, attr_attn_map)
            å¦‚æœ return_freq_info=True:
                + (freq_info,)
        """
        B, N, D = x.shape
        
        # ä¿å­˜åŸå§‹ç‰¹å¾ï¼ˆç”¨äºé‡æ„æŸå¤±ï¼‰
        original_feat = self.pool_id(x.transpose(1, 2)).squeeze(-1)  # [B, D]
        
        # === é˜¶æ®µ1ï¼šé¢‘åŸŸåˆ†è§£ ===
        # ç¡®å®šCLS tokenä½ç½®ï¼ˆæ ¹æ®åºåˆ—é•¿åº¦åˆ¤æ–­æ˜¯Vimè¿˜æ˜¯ViTï¼‰
        # Vim: mid-token (N//2), ViT: first token (0)
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾å¦‚æœNæ˜¯å¥‡æ•°ï¼Œåˆ™ä¸ºVimï¼ˆmid-tokenï¼‰
        cls_token_idx = N // 2 if N % 2 == 1 else 0
        
        low_freq_seq, high_freq_seq, freq_info = self.freq_splitter(x, cls_token_idx=cls_token_idx)
        
        # åŸå§‹ç‰¹å¾ + é¢‘åŸŸå¢å¼ºï¼ˆåŠ æƒèåˆï¼‰
        # è¿™æ ·ä¿è¯å³ä½¿é¢‘åŸŸåˆ†è§£å¤±è´¥ï¼Œä¹Ÿä¸ä¼šç ´ååŸæœ‰ç‰¹å¾
        x_enhanced = x + low_freq_seq + high_freq_seq
        
        # === é˜¶æ®µ2ï¼šé¢‘åŸŸå¼•å¯¼æ³¨æ„åŠ› ===
        if return_attention:
            id_seq, attr_seq, id_attn_map, attr_attn_map = self.freq_guided_attn(
                x_enhanced, 
                low_freq_feat=low_freq_seq,
                high_freq_feat=high_freq_seq,
                return_attention=True
            )
        else:
            id_seq, attr_seq = self.freq_guided_attn(
                x_enhanced,
                low_freq_feat=low_freq_seq,
                high_freq_feat=high_freq_seq,
                return_attention=False
            )
            id_attn_map, attr_attn_map = None, None
        
        # === é˜¶æ®µ3ï¼šå¼‚æ„åŒæµå»ºæ¨¡ ===
        # IDæµè¾“å…¥ï¼šåŸå§‹id_seq + ä½é¢‘å¢å¼º
        # Attræµè¾“å…¥ï¼šåŸå§‹attr_seq + é«˜é¢‘å¢å¼º
        id_seq_filtered, attr_seq_filtered = self.hybrid_stream(id_seq, attr_seq)
        
        # === é˜¶æ®µ4ï¼šå…¨å±€æ± åŒ– ===
        id_feat = self.pool_id(id_seq_filtered.transpose(1, 2)).squeeze(-1)      # [B, D]
        attr_feat = self.pool_attr(attr_seq_filtered.transpose(1, 2)).squeeze(-1) # [B, D]
        
        # === é˜¶æ®µ5ï¼šå¯¹ç§°é—¨æ§ï¼ˆğŸ”¥ ä¿®å¤ç‰ˆï¼šæ”¾å®½çº¦æŸï¼‰===
        concat_feat = torch.cat([id_feat, attr_feat], dim=-1)  # [B, D*2]
        
        gate_id = self.gate_id(concat_feat)      # [B, D]
        gate_attr = self.gate_attr(concat_feat)  # [B, D]
        
        # ğŸ”¥ æ”¾å®½é—¨æ§çº¦æŸï¼š[0.2, 0.8] â†’ [0.1, 0.95]
        # ä½¿ç”¨æ›´å®½æ¾çš„clampï¼Œå…è®¸é—¨æ§æœ‰æ›´å¤§çš„è¡¨è¾¾ç©ºé—´
        gate_id = torch.clamp(gate_id, min=0.1, max=0.95)
        gate_attr = torch.clamp(gate_attr, min=0.1, max=0.95)
        
        # åº”ç”¨é—¨æ§
        id_feat_gated = gate_id * id_feat
        attr_feat_gated = gate_attr * attr_feat
        
        # === é—¨æ§ç»Ÿè®¡ä¿¡æ¯ ===
        # è®¡ç®—é¢‘åŸŸèƒ½é‡æ¯”ç‡ (r_E) ç”¨äº SAMG
        # ä½¿ç”¨ Parseval å®šç†ï¼šæ—¶åŸŸèƒ½é‡ç­‰äºé¢‘åŸŸèƒ½é‡
        # r_E = Energy_High / Energy_Total
        energy_high = high_freq_seq.norm(p=2, dim=-1).mean(dim=1) # [B]
        energy_total = x.norm(p=2, dim=-1).mean(dim=1) + 1e-8      # [B]
        r_E = energy_high / energy_total                           # [B]

        gate_stats = {
            'gate_id_mean': gate_id.mean().item(),
            'gate_id_std': gate_id.std().item(),
            'gate_attr_mean': gate_attr.mean().item(),
            'gate_attr_std': gate_attr.std().item(),
            'diversity': torch.abs(gate_id - gate_attr).mean().item(),
            'freq_type': 'dct',
            'low_freq_energy': freq_info.get('freq_magnitude', torch.tensor(0.0)).mean().item() if 'freq_magnitude' in freq_info else 0.0,
            'energy_ratio': r_E  # [B] Tensor, not item, passed to fusion
        }
        
        # === æ„å»ºè¿”å›å€¼ ===
        base_outputs = (id_feat_gated, attr_feat_gated, gate_stats, original_feat)
        
        if return_attention:
            base_outputs = base_outputs + (id_attn_map, attr_attn_map)
        
        if return_freq_info:
            # å¢å¼ºfreq_infoï¼Œæ·»åŠ æ›´å¤šå¯è§†åŒ–ä¿¡æ¯
            freq_info['id_seq'] = id_seq.detach()
            freq_info['attr_seq'] = attr_seq.detach()
            freq_info['id_feat'] = id_feat_gated.detach()
            freq_info['attr_feat'] = attr_feat_gated.detach()
            base_outputs = base_outputs + (freq_info,)
        
        return base_outputs


# å…¼å®¹æ€§åˆ«å
GS3Module = FSHDModule
SymmetricGS3Module = FSHDModule