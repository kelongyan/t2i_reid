# models/semantic_guidance.py
"""
CLIPè¯­ä¹‰å¼•å¯¼æ¨¡å—
åˆ©ç”¨CLIPçš„è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¼•å¯¼IDå’ŒAttributeç‰¹å¾çš„è¯­ä¹‰åˆ†ç¦»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SemanticGuidedDecoupling(nn.Module):
    """
    è¯­ä¹‰å¼•å¯¼çš„ç‰¹å¾è§£è€¦æ¨¡å—
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - åˆ©ç”¨CLIP Text Encoderé¢„å…ˆå®šä¹‰çš„IDå’ŒAttribute Prompts
    - é€šè¿‡è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œè®©è§†è§‰ç‰¹å¾å‘å¯¹åº”çš„è¯­ä¹‰ç©ºé—´é æ‹¢
    - IDç‰¹å¾ â†’ "a person", "pedestrian" ç­‰èº«ä»½ç›¸å…³æè¿°
    - Attrç‰¹å¾ â†’ "red clothes", "backpack" ç­‰å±æ€§ç›¸å…³æè¿°
    """
    
    def __init__(self, text_encoder, tokenizer, dim=768, logger=None):
        """
        Args:
            text_encoder: CLIP Text Encoderå®ä¾‹
            tokenizer: CLIP Tokenizerå®ä¾‹
            dim (int): ç‰¹å¾ç»´åº¦ï¼ˆéœ€ä¸ID/Attrç‰¹å¾å¯¹é½ï¼‰
            logger: TrainingMonitorå®ä¾‹
        """
        super().__init__()
        self.text_encoder = text_encoder
        self.tokenizer = tokenizer
        self.dim = dim
        self.logger = logger
        
        # === ğŸ”¥ æ”¹è¿›çš„Promptæ¨¡æ¿ï¼ˆæ›´å…·ä½“ã€æ›´å¤šæ ·åŒ–ï¼‰===
        # ID Prompts: å¼ºè°ƒèº«ä»½ç»“æ„ã€ä½“æ€ã€ä¸å˜ç‰¹å¾
        self.id_prompts = [
            # ç»“æ„ç±»
            "person's body structure",
            "human silhouette",
            "pedestrian figure",
            # åŠ¨ä½œç±»
            "person walking",
            "individual standing",
            # æŠ½è±¡èº«ä»½
            "unique person identity",
            "pedestrian appearance",
        ]
        
        # Attribute Prompts: ç»†ç²’åº¦æœè£…æè¿°
        self.attr_prompts = [
            # ä¸Šè¡£é¢œè‰²
            "red shirt", "blue shirt", "black shirt", "white shirt",
            "gray shirt", "yellow shirt", "green shirt", "pink shirt",
            # ä¸‹è£…é¢œè‰²
            "blue jeans", "black pants", "gray pants", "white pants",
            # é£æ ¼
            "casual clothes", "formal attire", "sportswear",
            # é…é¥°
            "wearing backpack", "carrying handbag", "wearing hat",
        ]
        
        # é¢„è®¡ç®—å¹¶ç¼“å­˜CLIP Embeddingsï¼ˆé¿å…é‡å¤ç¼–ç ï¼‰
        self.register_buffer('id_prompt_embeds', torch.zeros(len(self.id_prompts), dim))
        self.register_buffer('attr_prompt_embeds', torch.zeros(len(self.attr_prompts), dim))
        self._initialized = False
        
        # æŠ•å½±å±‚ï¼šå°†CLIPè¾“å‡ºç»´åº¦ï¼ˆ512ï¼‰æ˜ å°„åˆ°ç³»ç»Ÿç»´åº¦ï¼ˆ768ï¼‰
        # è¿™ä¸ä¸»æ¨¡å‹çš„text_projä¿æŒä¸€è‡´
        clip_dim = text_encoder.config.hidden_size
        if clip_dim != dim:
            self.prompt_proj = nn.Sequential(
                nn.Linear(clip_dim, dim),
                nn.LayerNorm(dim),
                nn.GELU()
            )
        else:
            self.prompt_proj = nn.Identity()
    
    def _initialize_prompt_embeddings(self, device):
        """
        åˆå§‹åŒ–Prompt Embeddingsï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œï¼‰
        ä½¿ç”¨é¢„è®­ç»ƒçš„CLIP Text Encoderç¼–ç å›ºå®šPrompts
        """
        if self._initialized:
            return
        
        with torch.no_grad():
            # ç¼–ç ID Prompts
            id_tokens = self.tokenizer(
                self.id_prompts,
                padding='max_length',
                max_length=77,
                truncation=True,
                return_tensors='pt'
            ).to(device)
            
            id_outputs = self.text_encoder(**id_tokens)
            # ä½¿ç”¨pooler_outputï¼ˆ[CLS] tokenï¼‰æˆ–last_hidden_stateçš„å‡å€¼
            id_embeds_raw = id_outputs.last_hidden_state.mean(dim=1)  # [num_prompts, 512]
            id_embeds = self.prompt_proj(id_embeds_raw)  # [num_prompts, 768]
            self.id_prompt_embeds.copy_(F.normalize(id_embeds, dim=-1))
            
            # ç¼–ç Attr Prompts
            attr_tokens = self.tokenizer(
                self.attr_prompts,
                padding='max_length',
                max_length=77,
                truncation=True,
                return_tensors='pt'
            ).to(device)
            
            attr_outputs = self.text_encoder(**attr_tokens)
            attr_embeds_raw = attr_outputs.last_hidden_state.mean(dim=1)
            attr_embeds = self.prompt_proj(attr_embeds_raw)
            self.attr_prompt_embeds.copy_(F.normalize(attr_embeds, dim=-1))
        
        self._initialized = True
        
        if self.logger:
            self.logger.debug_logger.info(
                f"âœ… Semantic Guidance Initialized: "
                f"{len(self.id_prompts)} ID Prompts, {len(self.attr_prompts)} Attr Prompts"
            )
    
    def compute_semantic_alignment_loss(self, id_feat, attr_feat):
        """
        è®¡ç®—è¯­ä¹‰å¯¹é½æŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        ä¿®å¤ï¼š
        1. ä½¿ç”¨L2è·ç¦»æ›¿ä»£è´Ÿå¯¹æ•°ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
        2. æ·»åŠ NaNæ£€æµ‹
        3. é™ä½æŸå¤±æ•æ„Ÿåº¦
        
        ç›®æ ‡ï¼š
        - IDç‰¹å¾åº”è¯¥ä¸ID Promptsæ›´ç›¸ä¼¼
        - Attrç‰¹å¾åº”è¯¥ä¸Attr Promptsæ›´ç›¸ä¼¼
        
        Args:
            id_feat: IDç‰¹å¾ [batch_size, dim]
            attr_feat: Attrç‰¹å¾ [batch_size, dim]
            
        Returns:
            loss: è¯­ä¹‰å¯¹é½æŸå¤±ï¼ˆæ ‡é‡ï¼‰
        """
        # åˆå§‹åŒ–Prompt Embeddingsï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼‰
        if not self._initialized:
            self._initialize_prompt_embeddings(id_feat.device)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        id_feat_norm = F.normalize(id_feat, dim=-1, eps=1e-8)      # [B, dim]
        attr_feat_norm = F.normalize(attr_feat, dim=-1, eps=1e-8)  # [B, dim]
        
        # NaNæ£€æµ‹
        if torch.isnan(id_feat_norm).any() or torch.isnan(attr_feat_norm).any():
            return torch.tensor(0.0, device=id_feat.device, requires_grad=True)
        
        # === IDç‰¹å¾ä¸ID Promptsçš„å¯¹é½ ===
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ [B, num_id_prompts]
        id_sim = torch.matmul(id_feat_norm, self.id_prompt_embeds.t())
        # å–æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆæœ€æ¥è¿‘çš„promptï¼‰
        id_max_sim, _ = torch.max(id_sim, dim=1)  # [B]
        
        # === Attrç‰¹å¾ä¸Attr Promptsçš„å¯¹é½ ===
        attr_sim = torch.matmul(attr_feat_norm, self.attr_prompt_embeds.t())
        attr_max_sim, _ = torch.max(attr_sim, dim=1)  # [B]
        
        # === æŸå¤±ï¼šä½¿ç”¨L2è·ç¦»æ›¿ä»£è´Ÿå¯¹æ•° ===
        # ç›¸ä¼¼åº¦è¶Šé«˜ï¼ˆæ¥è¿‘1ï¼‰æŸå¤±è¶Šå°
        loss_id = (1.0 - id_max_sim).mean()
        loss_attr = (1.0 - attr_max_sim).mean()
        
        # NaNæ£€æµ‹
        if torch.isnan(loss_id).any() or torch.isnan(loss_attr).any():
            return torch.tensor(0.0, device=id_feat.device, requires_grad=True)
        
        # æ€»æŸå¤±
        total_loss = loss_id + loss_attr
        
        # è°ƒè¯•ä¿¡æ¯
        if self.logger and hasattr(self, '_log_counter'):
            self._log_counter = getattr(self, '_log_counter', 0) + 1
            if self._log_counter % 200 == 0:
                self.logger.debug_logger.debug(
                    f"Semantic Alignment: ID_sim={id_max_sim.mean():.4f}, "
                    f"Attr_sim={attr_max_sim.mean():.4f}, Loss={total_loss.item():.6f}"
                )
        
        return total_loss
    
    def compute_cross_separation_loss(self, id_feat, attr_feat):
        """
        è®¡ç®—äº¤å‰åˆ†ç¦»æŸå¤±ï¼ˆå¯é€‰ï¼‰
        
        ç›®æ ‡ï¼š
        - IDç‰¹å¾åº”è¿œç¦»Attr Prompts
        - Attrç‰¹å¾åº”è¿œç¦»ID Prompts
        
        è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©çº¦æŸï¼Œå¢å¼ºä¸¤ä¸ªç‰¹å¾ç©ºé—´çš„åˆ†ç¦»
        """
        if not self._initialized:
            self._initialize_prompt_embeddings(id_feat.device)
        
        id_feat_norm = F.normalize(id_feat, dim=-1, eps=1e-8)
        attr_feat_norm = F.normalize(attr_feat, dim=-1, eps=1e-8)
        
        # IDç‰¹å¾ä¸Attr Promptsçš„ç›¸ä¼¼åº¦ï¼ˆåº”è¯¥ä½ï¼‰
        id_to_attr_sim = torch.matmul(id_feat_norm, self.attr_prompt_embeds.t())
        id_to_attr_max, _ = torch.max(id_to_attr_sim, dim=1)
        
        # Attrç‰¹å¾ä¸ID Promptsçš„ç›¸ä¼¼åº¦ï¼ˆåº”è¯¥ä½ï¼‰
        attr_to_id_sim = torch.matmul(attr_feat_norm, self.id_prompt_embeds.t())
        attr_to_id_max, _ = torch.max(attr_to_id_sim, dim=1)
        
        # æŸå¤±ï¼šç›¸ä¼¼åº¦è¶Šé«˜æƒ©ç½šè¶Šå¤§
        loss = id_to_attr_max.mean() + attr_to_id_max.mean()
        
        return loss
    
    def forward(self, id_feat, attr_feat, use_cross_separation=False):
        """
        å‰å‘ä¼ æ’­ï¼šè®¡ç®—è¯­ä¹‰å¼•å¯¼æŸå¤±
        
        Args:
            id_feat: IDç‰¹å¾ [B, dim]
            attr_feat: Attrç‰¹å¾ [B, dim]
            use_cross_separation: æ˜¯å¦ä½¿ç”¨äº¤å‰åˆ†ç¦»æŸå¤±
            
        Returns:
            loss: è¯­ä¹‰å¼•å¯¼æŸå¤±
        """
        # ä¸»æŸå¤±ï¼šè¯­ä¹‰å¯¹é½
        align_loss = self.compute_semantic_alignment_loss(id_feat, attr_feat)
        
        # å¯é€‰ï¼šäº¤å‰åˆ†ç¦»æŸå¤±
        if use_cross_separation:
            sep_loss = self.compute_cross_separation_loss(id_feat, attr_feat)
            return align_loss + 0.5 * sep_loss  # æƒé‡å¯è°ƒ
        else:
            return align_loss
