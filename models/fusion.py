import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from mamba_ssm import Mamba
except ImportError:
    Mamba = None

class EnhancedMambaFusion(nn.Module):
    """ä¼˜åŒ–åçš„ Mamba SSM èåˆæ¨¡å—ï¼Œé—¨æ§æœºåˆ¶å‰ç½®ï¼Œç”¨äºé«˜æ•ˆæ•´åˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ã€‚"""
    
    def __init__(self, dim, d_state=16, d_conv=4, num_layers=3, output_dim=256, dropout=0.1, logger=None):
        super().__init__()
        self.logger = logger
        # æ¨¡æ€å¯¹é½å±‚
        self.image_align = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim)
        )
        self.text_align = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.LayerNorm(dim)
        )
        
        # å‰ç½®é—¨æ§æœºåˆ¶
        self.gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim // 2, 2),
            nn.Softmax(dim=-1)
        )
        
        # å¤šå±‚ Mamba SSM
        if Mamba is None:
             raise ImportError("Mamba module not found")

        self.mamba_layers = nn.ModuleList([
            Mamba(
                d_model=dim * 2,
                d_state=d_state,
                d_conv=d_conv,
                expand=2
            ) for _ in range(num_layers)
        ])
        self.mamba_norms = nn.ModuleList([nn.LayerNorm(dim * 2) for _ in range(num_layers)])
        
        # è¾“å‡ºæŠ•å½±
        self.fc = nn.Linear(dim * 2, output_dim)
        self.norm_final = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, image_features, text_features):
        if torch.isnan(image_features).any() or torch.isnan(text_features).any():
            if self.logger:
                self.logger.debug_logger.warning("âš ï¸  EnhancedMambaFusion: Input contains NaN, returning zeros")
            batch_size = image_features.size(0)
            device = image_features.device
            return torch.zeros(batch_size, self.fc.out_features, device=device), \
                   torch.ones(batch_size, 2, device=device) * 0.5
        
        image_features = self.image_align(image_features)
        text_features = self.text_align(text_features)
        
        if torch.isnan(image_features).any() or torch.isnan(text_features).any():
            image_features = torch.where(torch.isnan(image_features), torch.zeros_like(image_features), image_features)
            text_features = torch.where(torch.isnan(text_features), torch.zeros_like(text_features), text_features)
        
        concat_features = torch.cat([image_features, text_features], dim=-1)
        gate_weights = self.gate(concat_features)
        
        if torch.isnan(gate_weights).any():
            gate_weights = torch.ones_like(gate_weights) * 0.5
        
        image_weight, text_weight = gate_weights[:, 0:1], gate_weights[:, 1:2]
        
        weighted_image = image_weight * image_features
        weighted_text = text_weight * text_features
        weighted_features = torch.cat([weighted_image, weighted_text], dim=-1)
        weighted_features = weighted_features.unsqueeze(1)
        
        mamba_output = weighted_features
        for mamba, norm in zip(self.mamba_layers, self.mamba_norms):
            residual = mamba_output
            mamba_output = mamba(mamba_output)
            
            if torch.isnan(mamba_output).any():
                mamba_output = residual
            
            mamba_output = norm(mamba_output + residual)
        
        mamba_output = mamba_output.squeeze(1)
        
        fused_features = self.fc(mamba_output)
        fused_features = self.dropout(fused_features)
        fused_features = self.norm_final(fused_features)
        
        if torch.isnan(fused_features).any():
            fused_features = torch.zeros_like(fused_features)
        
        return fused_features, gate_weights

class SCAG_Gate(nn.Module):
    """
    è¯­ä¹‰ç½®ä¿¡åº¦æ„ŸçŸ¥é—¨æ§ (Semantic-Confidence Aware Gating)

    æ ¸å¿ƒåˆ›æ–°ï¼š
    - ä½¿ç”¨å†²çªåˆ†æ•° (conflict_score) æ¥è¡¡é‡è§£è€¦è´¨é‡
    - conflict_score é«˜ â†’ è§£è€¦å¤±è´¥ â†’ é™ä½å›¾åƒæƒé‡ â†’ "å¼ƒå›¾ä¿æ–‡"
    - conflict_score ä½ â†’ è§£è€¦æˆåŠŸ â†’ æå‡å›¾åƒæƒé‡ â†’ "å›¾æ–‡å¹¶é‡"
    """
    def __init__(self, dim, temperature=1.0):
        super().__init__()
        self.temperature = temperature  # ğŸ”¥ ä»0.1å¢åŠ åˆ°1.0ï¼Œè®©é—¨æ§æ›´å¹³æ»‘

        # Confidence MLP: å°†å†²çªåˆ†æ•°è½¬æ¢ä¸ºç½®ä¿¡åº¦æƒé‡çš„ä¿®æ­£é‡
        self.confidence_mlp = nn.Sequential(
            nn.Linear(1, dim // 4),
            nn.LayerNorm(dim // 4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(dim // 4, dim // 2),
            nn.LayerNorm(dim // 2),
            nn.ReLU(),
            nn.Linear(dim // 2, dim)
            # ç§»é™¤æœ€åçš„ Sigmoidï¼Œæ”¹åœ¨ forward ä¸­ç»Ÿä¸€å¤„ç†
        )

        # äº’æŸ¥è¯¢åˆ†æ”¯ (ä¿ç•™åŸ SAMG çš„é€»è¾‘ï¼Œç”¨äºç‰¹å¾äº¤äº’)
        self.mutual_proj = nn.Linear(dim * 2, dim * 2)

        self._init_weights()

    def _init_weights(self):
        """ğŸ”¥ åˆå§‹åŒ–æƒé‡"""
        for m in self.confidence_mlp.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # æœ€åä¸€å±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘0ï¼Œä½¿å¾—åˆå§‹çŠ¶æ€ä¸»è¦ç”± conflict_score å†³å®š
        if isinstance(self.confidence_mlp[-1], nn.Linear):
            nn.init.constant_(self.confidence_mlp[-1].weight, 0)
            nn.init.constant_(self.confidence_mlp[-1].bias, 0)

        nn.init.xavier_normal_(self.mutual_proj.weight, gain=1.0)
        if self.mutual_proj.bias is not None:
            nn.init.constant_(self.mutual_proj.bias, 0)

    def forward(self, id_feat, attr_feat, conflict_score):
        """
        Args:
            id_feat: [B, D] ID ç‰¹å¾
            attr_feat: [B, D] Attr ç‰¹å¾
            conflict_score: [B] å†²çªåˆ†æ•° (æ¥è‡ª AH-Net Module)

        Returns:
            id_out: [B, D] é—¨æ§åçš„ ID ç‰¹å¾
            attr_out: [B, D] é—¨æ§åçš„ Attr ç‰¹å¾
            confidence_weight: [B, D] ç½®ä¿¡åº¦æƒé‡
        """
        # ğŸ”¥ æ·»åŠ NaNæ£€æµ‹ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        if torch.isnan(id_feat).any() or torch.isnan(attr_feat).any():
            # å¦‚æœè¾“å…¥æœ‰NaNï¼Œè¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            B, D = id_feat.shape
            device = id_feat.device
            return (
                torch.zeros_like(id_feat),
                torch.zeros_like(attr_feat),
                torch.ones(B, D, device=device) * 0.5
            )
        
        if conflict_score.dim() == 1:
            conflict_score = conflict_score.unsqueeze(1)  # [B, 1]
        
        # 1. è®¡ç®—ç½®ä¿¡åº¦æƒé‡
        # åŸºç¡€é€»è¾‘ï¼šConflict é«˜ -> Confidence ä½
        # Base Confidence: 1.0 - conflict_score
        base_confidence = 1.0 - conflict_score
        
        # MLP å­¦ä¹ çš„æ˜¯é’ˆå¯¹æ¯ä¸ªç»´åº¦çš„ç²¾ç»†è°ƒæ•´ (Residual correction)
        delta_confidence = self.confidence_mlp(conflict_score) # [B, D]
        
        # ğŸ”¥ æ·»åŠ ä¸­é—´ç»“æœNaNæ£€æµ‹
        if torch.isnan(delta_confidence).any():
            delta_confidence = torch.zeros_like(delta_confidence)
        
        # ç»“åˆå¹¶åº”ç”¨æ¸©åº¦ç¼©æ”¾
        # Logit = (Base + Delta) / T
        # Sigmoid(Logit) å°†åœ¨ 0 æˆ– 1 é™„è¿‘é¥±å’Œï¼Œé¿å…åœç•™åœ¨ 0.5
        logits = (base_confidence + delta_confidence) / self.temperature
        
        # ğŸ”¥ é™åˆ¶logitèŒƒå›´ï¼Œé˜²æ­¢sigmoidæ•°å€¼ä¸ç¨³å®š
        logits = torch.clamp(logits, min=-10, max=10)
        confidence_weight = torch.sigmoid(logits)  # [B, D]
        
        # 2. äº’æŸ¥è¯¢é—¨æ§
        joint_feat = torch.cat([id_feat, attr_feat], dim=-1)  # [B, 2D]
        raw_gates = self.mutual_proj(joint_feat)  # [B, 2D]
        
        # ğŸ”¥ æ·»åŠ NaNæ£€æµ‹
        if torch.isnan(raw_gates).any():
            raw_gates = torch.zeros_like(raw_gates)
        
        raw_gates = raw_gates.view(-1, 2, id_feat.shape[-1])  # [B, 2, D]
        gates = F.softmax(raw_gates, dim=1)  # [B, 2, D]

        g_id = gates[:, 0, :]     # [B, D]
        g_attr = gates[:, 1, :]   # [B, D]

        # 3. åº”ç”¨ç½®ä¿¡åº¦è°ƒèŠ‚ + ğŸ”¥ æ·»åŠ Residual Connection
        # åŸå§‹æ–¹æ¡ˆï¼šå®Œå…¨æ›¿æ¢ç‰¹å¾
        # æ”¹è¿›æ–¹æ¡ˆï¼šä¿ç•™éƒ¨åˆ†åŸå§‹ç‰¹å¾ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
        alpha = 0.7  # Residualæƒé‡
        id_out = alpha * (id_feat * g_id * confidence_weight) + (1 - alpha) * id_feat
        attr_out = alpha * (attr_feat * g_attr * confidence_weight) + (1 - alpha) * attr_feat

        return id_out, attr_out, confidence_weight

class RCSM_Fusion(nn.Module):
    """
    æ®‹å·®äº¤å‰æ‰«æ Mamba èåˆ (Residual Cross-Scan Mamba Fusion)
    """
    def __init__(self, dim, output_dim=256, d_state=16, d_conv=4, dropout=0.1):
        super().__init__()
        if Mamba is None:
             raise ImportError("Mamba module not found")
             
        self.dim = dim
        self.ln = nn.LayerNorm(dim)
        
        self.mamba_fwd = Mamba(d_model=dim, d_state=d_state, d_conv=d_conv, expand=2)
        self.mamba_bwd = Mamba(d_model=dim, d_state=d_state, d_conv=d_conv, expand=2)
        
        self.dropout = nn.Dropout(dropout)
        self.injection_proj = nn.Linear(dim, dim)
        
        self.bottleneck = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )
        
        self.out_proj = nn.Linear(dim, output_dim)
        self.out_norm = nn.LayerNorm(output_dim)

    def forward(self, img_id, img_attr, txt_id, txt_attr):
        B, D = img_id.shape
        
        # Interleaved Construction: [Img_ID, Img_Attr, Txt_ID, Txt_Attr]
        s_inter = torch.stack([img_id, img_attr, txt_id, txt_attr], dim=1) # [B, 4, D] 
        
        x_norm = self.ln(s_inter)
        
        s_fwd = self.mamba_fwd(x_norm)
        s_rev = x_norm.flip(dims=[1])
        s_bwd = self.mamba_bwd(s_rev).flip(dims=[1])
        
        s_mamba = s_inter + self.dropout(s_fwd + s_bwd)
        
        s_shortcut = self.injection_proj(s_inter)
        s_fused = F.layer_norm(s_mamba + s_shortcut, (D,))
        
        s_out = self.bottleneck(s_fused)
        fused_feat = s_out.mean(dim=1)
        
        out = self.out_proj(fused_feat)
        out = self.out_norm(out)
        
        return out

class ScagRcsmFusion(nn.Module):
    """
    S-CAG + RCSM èåˆæ¨¡å—

    è¯­ä¹‰ç½®ä¿¡åº¦æ„ŸçŸ¥é—¨æ§ (S-CAG) + æ®‹å·®äº¤å‰æ‰«æ Mamba (RCSM)
    è¿™æ˜¯æ–¹æ¡ˆä¹¦çš„å®Œæ•´å®ç°ï¼Œæ›¿ä»£äº†åŸæœ‰çš„ SAMG+RCSM æ¶æ„ã€‚

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    1. åŸºäºè¯­ä¹‰è´¨é‡ (conflict_score) è€Œéç‰©ç†ç»Ÿè®¡ (energy_ratio)
    2. å…·å¤‡"è‡ªçŸ¥ä¹‹æ˜"ï¼ŒçŸ¥é“ä½•æ—¶è¯¥ä¿¡å›¾ç‰‡ï¼Œä½•æ—¶è¯¥ä¿¡æ–‡å­—
    """
    def __init__(self, dim=768, output_dim=256, **kwargs):
        super().__init__()
        self.scag = SCAG_Gate(dim)
        self.rcsm = RCSM_Fusion(dim, output_dim=output_dim, **kwargs)
        
        # ğŸ”¥ æ·»åŠ ç»´åº¦æŠ•å½±ç”¨äºresidual connection
        self.residual_proj = nn.Linear(dim, output_dim)

    def forward(self, img_id, img_attr, txt_id, txt_attr, conflict_score):
        """
        Args:
            conflict_score: [B] å†²çªåˆ†æ•° (æ¥è‡ª AH-Net Module)
                              æ›¿ä»£äº†æ—§çš„ energy_ratio å‚æ•°
        """
        # ä½¿ç”¨ S-CAG é—¨æ§ï¼ŒåŸºäºå†²çªåˆ†æ•°è°ƒèŠ‚å›¾åƒç‰¹å¾
        img_id_gated, img_attr_gated, confidence_weight = self.scag(
            img_id, img_attr, conflict_score
        )

        # RCSM èåˆ
        fused_embeds = self.rcsm(img_id_gated, img_attr_gated, txt_id, txt_attr)
        
        # ğŸ”¥ æ·»åŠ Multi-modal Residual Connection
        # æŠ•å½±img_idåˆ°è¾“å‡ºç»´åº¦ï¼Œç„¶ååŠ æƒèåˆ
        residual_weight = 0.3
        img_id_proj = self.residual_proj(img_id)
        fused_embeds = (1 - residual_weight) * fused_embeds + residual_weight * img_id_proj

        return fused_embeds, confidence_weight

def get_fusion_module(config):
    """
    åŠ¨æ€åˆ›å»ºèåˆæ¨¡å—ã€‚

    æ”¯æŒçš„ç±»å‹ï¼š
    - 'scag_rcsm': S-CAG + RCSM (æ–¹æ¡ˆä¹¦æ¨èï¼Œæœ€æ–°)
    - 'enhanced_mamba': Enhanced Mamba Fusion (å¤‡ç”¨)
    """
    fusion_type = config.get("type")

    if fusion_type == "scag_rcsm":
        # æ–¹æ¡ˆä¹¦çš„å®Œæ•´å®ç°
        valid_params = {k: v for k, v in config.items() if k in ['dim', 'd_state', 'd_conv', 'output_dim', 'dropout']}
        return ScagRcsmFusion(**valid_params)

    elif fusion_type == "samg_rcsm":
        # å…¼å®¹æ—§å‘½åï¼Œå†…éƒ¨ä½¿ç”¨ S-CAG
        valid_params = {k: v for k, v in config.items() if k in ['dim', 'd_state', 'd_conv', 'output_dim', 'dropout']}
        return ScagRcsmFusion(**valid_params)

    elif fusion_type == "enhanced_mamba":
        valid_params = {k: v for k, v in config.items() if k in ['dim', 'd_state', 'd_conv', 'num_layers', 'output_dim', 'dropout']}
        return EnhancedMambaFusion(**valid_params)

    else:
        raise ValueError(f"Unknown fusion type: {fusion_type}. Supported: ['scag_rcsm', 'samg_rcsm', 'enhanced_mamba']")
