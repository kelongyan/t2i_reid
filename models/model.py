# src/models/model.py
import math
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPTokenizer, CLIPTextModel, ViTModel
from safetensors.torch import load_file
from utils.serialization import copy_state_dict
from .fusion import get_fusion_module, SamgRcsmFusion
from .fshd_module import FSHDModule  # æ–°çš„FSHDæ¨¡å—ï¼ˆé¢‘åŸŸ-ç©ºåŸŸè”åˆè§£è€¦ï¼‰
from .semantic_guidance import SemanticGuidedDecoupling  # æ–°å¢CLIPè¯­ä¹‰å¼•å¯¼
from .vim import VisionMamba

# å°è¯•å¯¼å…¥ Mamba
try:
    from mamba_ssm import Mamba
    HAS_MAMBA = True
except ImportError:
    Mamba = None
    HAS_MAMBA = False

# è®¾ç½®transformersåº“æ—¥å¿—çº§åˆ«
import logging as _logging
import warnings
_logging.getLogger("transformers").setLevel(_logging.ERROR)
warnings.filterwarnings('ignore', category=UserWarning, module='transformers')


def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=(), mid_cls=True, logger=None):
    """
    Rescale the grid of position embeddings when loading from state_dict. Adapted from DEIT/Vim.
    
    Args:
        posemb: Pretrained position embedding tensor. [1, 197, 384]
        posemb_new: Target position embedding tensor. [1, 129, 384]
        num_tokens: Number of special tokens (CLS, etc.).
        gs_new: Target grid size (h, w).
        mid_cls: Whether the CLS token is in the middle (Vim style).
        logger: Logger instance for debugging
    """
    if logger:
        debug_logger.debug(f"Resizing position embedding: {posemb.shape} -> {posemb_new.shape}")
    
    ntok_new = posemb_new.shape[1]
    
    # 1. è§£æè¾“å…¥æƒé‡ (Handle Input CLS position)
    # å¦‚æœåŠ è½½çš„æ˜¯ vim_midclstok æƒé‡ï¼ŒCLS token é€šå¸¸å·²ç»åœ¨ä¸­é—´äº†
    if num_tokens:
        if mid_cls:
            # å‡è®¾é¢„è®­ç»ƒæƒé‡ä¹Ÿæ˜¯ Mid-CLS ç»“æ„
            old_cls_idx = posemb.shape[1] // 2
            posemb_tok = posemb[:, old_cls_idx:old_cls_idx+num_tokens]
            # æ‹¼æ¥é™¤äº† CLS ä»¥å¤–çš„éƒ¨åˆ†
            posemb_grid = torch.cat([posemb[:, :old_cls_idx], posemb[:, old_cls_idx+num_tokens:]], dim=1)
        else:
            # æ ‡å‡† ViT ç»“æ„ [CLS, Grid]
            posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[:, num_tokens:]
            
        ntok_new -= num_tokens
    else:
        posemb_tok, posemb_grid = posemb[:, :0], posemb
    
    gs_old = int(math.sqrt(len(posemb_grid[0])))
    
    if ntok_new != len(posemb_grid[0]):
        if logger:
            debug_logger.debug(f'Position embedding grid resize from {gs_old}x{gs_old} to {gs_new[0]}x{gs_new[1]}')
        posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
        posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)
        posemb_grid = posemb_grid.permute(0, 2, 3, 1).flatten(1, 2)
        
        # 2. ç»„è£…è¾“å‡ºæƒé‡ (Re-assemble Output)
        if mid_cls:
            # === å…³é”®ä¿®å¤ï¼šå°† CLS æ”¾å›æ–°åºåˆ—çš„ä¸­é—´ ===
            new_cls_idx = posemb_grid.shape[1] // 2
            posemb = torch.cat([
                posemb_grid[:, :new_cls_idx], 
                posemb_tok, 
                posemb_grid[:, new_cls_idx:]
            ], dim=1)
        else:
            posemb = torch.cat([posemb_tok, posemb_grid], dim=1)
            
    return posemb


class ResidualBottleneck1d(nn.Module):
    """
    Stage 1: å±€éƒ¨æ®‹å·®ç“¶é¢ˆæ¨¡å— (Local Residual Bottleneck)
    ç»“æ„: 1x1 Conv (Squeeze) -> 3x3 Conv (Process) -> 1x1 Conv (Expand)
    """
    def __init__(self, dim, reduction=4):
        super().__init__()
        hidden_dim = dim // reduction
        
        self.squeeze = nn.Sequential(
            nn.Conv1d(dim, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm1d(hidden_dim),
            nn.SiLU(inplace=True)
        )
        
        self.process = nn.Sequential(
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm1d(hidden_dim),
            nn.SiLU(inplace=True)
        )
        
        self.expand = nn.Sequential(
            nn.Conv1d(hidden_dim, dim, kernel_size=1, bias=False),
            nn.BatchNorm1d(dim)
        )
        
        # è¿™é‡Œçš„æ®‹å·®è¿æ¥åœ¨ forward ä¸­å¤„ç†

    def forward(self, x):
        # Input: [B, D, L]
        identity = x
        out = self.squeeze(x)
        out = self.process(out)
        out = self.expand(out)
        return identity + out

class GatedMambaBlock(nn.Module):
    """
    Stage 2: é—¨æ§é•¿ç¨‹ Mamba æ¨¡å— (Gated Residual Mamba)
    ç»“æ„: Mamba -> Gating (Sigmoid) -> Residual
    """
    def __init__(self, dim, d_state=16, d_conv=4, expand=2, dropout=0.1):
        super().__init__()
        if Mamba is None:
            raise ImportError("Mamba module not found. Please install mamba-ssm.")
            
        self.mamba = Mamba(
            d_model=dim,
            d_state=d_state,
            d_conv=d_conv,
            expand=expand
        )
        
        # è‡ªé€‚åº”é—¨æ§ç”Ÿæˆå™¨
        self.gate_fc = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Input: [B, L, D]
        residual = x
        x_norm = self.norm(x)
        
        # Mamba å¤„ç†
        mamba_out = self.mamba(x_norm)
        
        # é—¨æ§æœºåˆ¶: G = Sigmoid(Linear(x))
        # ä½¿ç”¨åŸå§‹è¾“å…¥ x (æˆ–è€… x_norm) æ¥è®¡ç®—é—¨æ§
        gate = torch.sigmoid(self.gate_fc(x_norm))
        
        # è°ƒåˆ¶
        out = mamba_out * gate
        out = self.dropout(out)
        
        return residual + out

class PyramidTextEncoder(nn.Module):
    """
    ä¸²è¡Œæ¸è¿›å¼é‡‘å­—å¡”æ–‡æœ¬ç¼–ç å™¨ (Serial Progressive Pyramid Text Encoder)
    
    Architecture:
    Input (CLIP) -> [Stage 1: CNN Bottleneck] -> Attr Head (MaxPool)
                                     |
                                     v
                   [Stage 2: Gated Mamba] -> Input Injection -> ID Head (AvgPool)
    """
    def __init__(self, dim=768):
        super().__init__()
        self.dim = dim
        
        # === Stage 1: Local Attribute Extraction (CNN) ===
        self.stage1_bottleneck = ResidualBottleneck1d(dim, reduction=4)
        
        # Attr Head: LayerNorm -> MaxPool
        # ç”¨äºæå–æœ€æ˜¾è‘—çš„å±€éƒ¨ç‰¹å¾ (Keyphrase Activation)
        self.attr_norm = nn.LayerNorm(dim)
        
        # === Stage 2: Global Context Modeling (Mamba) ===
        self.stage2_mamba = GatedMambaBlock(dim=dim)
        
        # ID Head: LayerNorm -> AvgPool -> BNNeck
        # ç”¨äºèšåˆå…¨å±€èº«ä»½ç‰¹å¾
        self.id_norm = nn.LayerNorm(dim)
        self.id_bn = nn.BatchNorm1d(dim)
        nn.init.constant_(self.id_bn.weight, 1.0)
        nn.init.constant_(self.id_bn.bias, 0.0)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input text embeddings from CLIP. Shape [B, L, D].
                              Expect L=77, D=768.
        Returns:
            dict: {
                'feat_attr': [B, D], # å±æ€§ç‰¹å¾
                'feat_id': [B, D],   # èº«ä»½ç‰¹å¾
                'feat_id_bn': [B, D] # ç»è¿‡BNNeckçš„èº«ä»½ç‰¹å¾(ç”¨äºåˆ†ç±»)
            }
        """
        B, L, D = x.shape
        
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äº Input Injection
        x_raw = x
        
        # =================================================
        # Stage 1: Local Processing (CNN)
        # =================================================
        # Permute for Conv1d: [B, L, D] -> [B, D, L]
        x_cnn_in = x.transpose(1, 2)
        
        # Bottleneck processing
        x_cnn_out = self.stage1_bottleneck(x_cnn_in) # [B, D, L]
        
        # Permute back: [B, D, L] -> [B, L, D]
        x_stage1 = x_cnn_out.transpose(1, 2)
        
        # --- Attr Head Output ---
        # Norm -> MaxPool
        # MaxPool along sequence dim (dim=1)
        # x_stage1: [B, L, D]
        feat_attr = self.attr_norm(x_stage1)
        feat_attr = feat_attr.transpose(1, 2) # [B, D, L]
        feat_attr = F.adaptive_max_pool1d(feat_attr, 1).squeeze(2) # [B, D]
        
        # =================================================
        # Stage 2: Global Processing (Mamba)
        # =================================================
        # Mamba takes [B, L, D]
        x_mamba_out = self.stage2_mamba(x_stage1)
        
        # =================================================
        # Global Input Injection
        # =================================================
        # Feature Fusion: Stage 2 Output + Original Input
        # é˜²æ­¢æ·±å±‚é—å¿˜ï¼Œä¿ç•™åŸå§‹è¯­ä¹‰ç©ºé—´
        x_final = x_mamba_out + x_raw
        
        # --- ID Head Output ---
        # Norm -> AvgPool -> BNNeck
        x_final_norm = self.id_norm(x_final)
        x_final_norm = x_final_norm.transpose(1, 2) # [B, D, L]
        
        # AvgPool along sequence dim
        feat_id = F.adaptive_avg_pool1d(x_final_norm, 1).squeeze(2) # [B, D]
        
        # BNNeck (Classification Friendly)
        feat_id_bn = self.id_bn(feat_id)
        
        return {
            'feat_attr': feat_attr,
            'feat_id': feat_id,
            'feat_id_bn': feat_id_bn
        }


class DisentangleModule(nn.Module):
    """
    ç‰¹å¾åˆ†ç¦»æ¨¡å—çš„åŸºç±»ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
    å®é™…ä½¿ç”¨ä¸­å»ºè®®ç›´æ¥ä½¿ç”¨ GS3Module/FSHDModule
    """
    def __init__(self, dim):
        """
        ç®€åŒ–çš„ç‰¹å¾åˆ†ç¦»æ¨¡å—ï¼ˆæ¶ˆèå®éªŒç‰ˆæœ¬ï¼‰ï¼Œç§»é™¤å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

        Args:
            dim (int): è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼ˆæ¯ä¸ª token çš„ç»´åº¦ï¼‰ã€‚
        """
        super().__init__()
        # ç®€åŒ–ä¸ºåŸºæœ¬çš„çº¿æ€§å˜æ¢
        self.id_linear = nn.Linear(dim, dim)
        self.cloth_linear = nn.Linear(dim, dim)
        
        # ä¿ç•™é—¨æ§æœºåˆ¶ä»¥ç»´æŒæ¥å£ä¸€è‡´æ€§
        self.gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )

    def forward(self, x, return_attention=False):
        """
        ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œç§»é™¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨åŸºæœ¬çš„çº¿æ€§å˜æ¢å’Œæ± åŒ–ã€‚

        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, dim]ã€‚
            return_attention (bool): æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ã€‚

        Returns:
            tuple: (id_feat, cloth_feat, gate_stats, original_feat)
                   å¦‚æœæœ‰ return_attentionï¼Œè¿½åŠ  None
        """
        batch_size, seq_len, dim = x.size()

        # åŸå§‹ç‰¹å¾ç”¨äºé‡æ„æŸå¤± (Global Pooling)
        original_feat = x.mean(dim=1) # [batch_size, dim]

        # ç®€åŒ–çš„èº«ä»½ç‰¹å¾å¤„ç†ï¼šçº¿æ€§å˜æ¢ + å…¨å±€å¹³å‡æ± åŒ–
        id_feat = self.id_linear(x)  # [batch_size, seq_len, dim]
        id_feat = id_feat.mean(dim=1)  # [batch_size, dim]

        # ç®€åŒ–çš„æœè£…ç‰¹å¾å¤„ç†ï¼šçº¿æ€§å˜æ¢ + å…¨å±€å¹³å‡æ± åŒ–  
        cloth_feat = self.cloth_linear(x)  # [batch_size, seq_len, dim]
        cloth_feat = cloth_feat.mean(dim=1)  # [batch_size, dim]

        # ä¿æŒé—¨æ§æœºåˆ¶ä»¥ç»´æŒåŸæœ‰æ¥å£
        gate = self.gate(torch.cat([id_feat, cloth_feat], dim=-1))  # [batch_size, dim]
        id_feat = gate * id_feat
        cloth_feat = (1 - gate) * cloth_feat
        
        # æ„é€ å…¼å®¹çš„ gate_stats
        gate_stats = {
            'gate_id_mean': gate.mean().item(),
            'gate_attr_mean': (1-gate).mean().item(),
            'diversity': 0.0,
            # æä¾›é»˜è®¤çš„ energy_ratio ä»¥é˜² Fusion æ¨¡å—éœ€è¦
            'energy_ratio': torch.ones(batch_size, device=x.device) * 0.5 
        }
        
        # ç»Ÿä¸€è¿”å›æ¥å£ (4ä¸ªåŸºç¡€è¿”å›å€¼)
        base_outputs = (id_feat, cloth_feat, gate_stats, original_feat)
        
        if return_attention:
            return base_outputs + (None, None)
        else:
            return base_outputs


class Model(nn.Module):
    def __init__(self, net_config, logger=None):
        """
        æ–‡æœ¬-å›¾åƒè¡Œäººé‡è¯†åˆ«æ¨¡å‹ï¼ˆæ¶ˆèå®éªŒç‰ˆæœ¬ï¼‰ï¼Œç§»é™¤äº†å¤æ‚çš„è§£çº ç¼ æ¨¡å—ã€‚

        Args:
            net_config (dict): æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«BERTè·¯å¾„ã€ViTè·¯å¾„ã€èåˆæ¨¡å—é…ç½®ç­‰ã€‚
            logger: TrainingMonitorå®ä¾‹ï¼Œç”¨äºè®°å½•æ—¥å¿—
        """
        super().__init__()
        self.net_config = net_config
        self.logger = logger
        
        # === Upgrade: Switch to CLIP Text Encoder ===
        clip_base_path = Path(net_config.get('clip_pretrained', 'pretrained/openai/clip-vit-base-patch16'))
        vit_base_path = Path(net_config.get('vit_pretrained', 'pretrained/vit-base-patch16-224'))
        fusion_config = net_config.get('fusion', {})
        num_classes = net_config.get('num_classes', 8000)

        # éªŒè¯é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        if not clip_base_path.exists():
            # Fallback to searching in pretrained folder if exact path not found
            fallback = list(Path("pretrained").glob("**/clip-vit-base-patch16"))
            if fallback:
                clip_base_path = fallback[0]
                if self.logger:
                    self.debug_logger.warning(f"Exact CLIP path not found, using fallback: {clip_base_path}")
            else:
                 # Last resort: try checking parent directories if relative path issue
                if (Path.cwd() / "pretrained/clip-vit-base-patch16").exists():
                    clip_base_path = Path.cwd() / "pretrained/clip-vit-base-patch16"

        if not clip_base_path.exists() or not vit_base_path.exists():
             # If strictly not found, still raise error
            if not clip_base_path.exists():
                 raise FileNotFoundError(f"CLIP model path not found: {clip_base_path}")
            if not vit_base_path.exists():
                 raise FileNotFoundError(f"ViT model path not found: {vit_base_path}")

        # åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨ (CLIP)
        if self.logger:
            self.debug_logger.info(f"Loading CLIP Text Encoder from: {clip_base_path}")
        self.tokenizer = CLIPTokenizer.from_pretrained(str(clip_base_path))
        
        # åˆå§‹åŒ– CLIPTextModel (ä½¿ç”¨ safetensors åŠ è½½ä»¥è§„é¿å®‰å…¨æ£€æŸ¥)
        self.text_encoder = CLIPTextModel.from_pretrained(str(clip_base_path))
        
        # å°è¯•åŠ è½½ safetensors æƒé‡ (å¦‚æœæœ‰)
        safetensors_path = clip_base_path / "model.safetensors"
        if safetensors_path.exists():
            if self.logger:
                self.debug_logger.info(f"Loading CLIP weights from safetensors: {safetensors_path}")
            state_dict = load_file(str(safetensors_path))
            
            # CLIPModel åŒ…å« text_model. å‰ç¼€ï¼Œæˆ‘ä»¬éœ€è¦å‰¥ç¦»å®ƒä»¥é€‚é… CLIPTextModel
            new_state_dict = {}
            for k, v in state_dict.items():
                if k.startswith("text_model."):
                    new_key = k[len("text_model."):]  # remove "text_model." prefix
                    new_state_dict[new_key] = v
            
            if new_state_dict:
                missing, unexpected = self.text_encoder.load_state_dict(new_state_dict, strict=False)
                if missing and self.logger:
                    self.debug_logger.info(f"Loaded CLIP with missing keys (expected for visual part): {len(missing)}")
            else:
                if self.logger:
                    self.debug_logger.warning("No 'text_model.' keys found in safetensors. Assuming pure TextModel format.")
        
        self.clip_dim = self.text_encoder.config.hidden_size # é€šå¸¸æ˜¯ 512
        self.text_width = 768  # ç³»ç»Ÿç›®æ ‡ç»´åº¦ (é€‚é… G-S3/Vim)

        # === ç»´åº¦é€‚é…å±‚ (Adapter) ===
        # å°† CLIP çš„ 512 ç»´æ˜ å°„åˆ°ç³»ç»Ÿçš„ 768 ç»´
        # ä½¿ç”¨ Linear -> LayerNorm -> GELU å¢å¼ºè¡¨è¾¾èƒ½åŠ›
        if self.clip_dim != self.text_width:
            self.text_proj = nn.Sequential(
                nn.Linear(self.clip_dim, self.text_width),
                nn.LayerNorm(self.text_width),
                nn.GELU()
            )
            # åˆå§‹åŒ–
            nn.init.xavier_uniform_(self.text_proj[0].weight)
            nn.init.zeros_(self.text_proj[0].bias)
            if self.logger:
                self.debug_logger.info(f"Added CLIP Adapter: {self.clip_dim} -> {self.text_width} (Linear+LN+GELU)")
        else:
            self.text_proj = nn.Identity()

        # åˆå§‹åŒ–å›¾åƒç¼–ç å™¨
        self.vision_backbone_type = net_config.get('vision_backbone', 'vit')
        if self.vision_backbone_type == 'vim':
            # Vision Mamba (Vim-S)
            vim_pretrained_path = net_config.get('vim_pretrained', 'pretrained/Vision Mamba/vim_s_midclstok.pth')
            # è·å–å›¾åƒå°ºå¯¸ï¼Œé»˜è®¤ä¸º 224x224
            img_size = net_config.get('img_size', (224, 224))
            if isinstance(img_size, int):
                img_size = (img_size, img_size)
            
            # ä½¿ç”¨é…ç½®çš„å°ºå¯¸åˆå§‹åŒ– VisionMamba
            self.visual_encoder = VisionMamba(img_size=img_size, patch_size=16, embed_dim=384, depth=24)
            
            # åŠ è½½ Vim é¢„è®­ç»ƒæƒé‡
            if Path(vim_pretrained_path).exists():
                # weights_only=False ä»¥æ”¯æŒåŠ è½½åŒ…å« argparse.Namespace çš„ checkpoint
                checkpoint = torch.load(vim_pretrained_path, map_location='cpu', weights_only=False)
                # æå–æ¨¡å‹æƒé‡ (å¤„ç† checkpoint åŒ…å« 'model' é”®çš„æƒ…å†µ)
                state_dict = checkpoint.get('model', checkpoint)
                
                # === å…³é”®ä¿®å¤ï¼šä½ç½®ç¼–ç æ’å€¼ ===
                if 'pos_embed' in state_dict:
                    # è®¡ç®—æ–°çš„ Grid å°ºå¯¸
                    # img_size æ˜¯ (H, W)ï¼Œpatch_size æ˜¯ 16
                    grid_h = img_size[0] // 16
                    grid_w = img_size[1] // 16
                    
                    state_dict['pos_embed'] = resize_pos_embed(
                        state_dict['pos_embed'], 
                        self.visual_encoder.pos_embed, 
                        num_tokens=1, 
                        gs_new=(grid_h, grid_w),
                        logger=self.logger
                    )
                
                # åŠ è½½æƒé‡ (éä¸¥æ ¼æ¨¡å¼ï¼Œå…è®¸éƒ¨åˆ†ä¸åŒ¹é…ï¼Œå¦‚å¤´éƒ¨)
                missing, unexpected = self.visual_encoder.load_state_dict(state_dict, strict=False)
                if self.logger:
                    self.debug_logger.info(f"Loaded Vim backbone from {vim_pretrained_path}")
                    if missing:
                        self.debug_logger.warning(f"Missing keys in Vim: {missing}")
            else:
                if self.logger:
                    self.debug_logger.warning(f"Vim pretrained path not found: {vim_pretrained_path}. Using random init.")
            
            # æŠ•å½±å±‚ï¼šå°† Vim çš„ 384 ç»´æ˜ å°„åˆ° 768 ç»´ä»¥é€‚é…åç»­æ¨¡å—
            self.visual_proj = nn.Linear(384, self.text_width)
            if self.logger:
                self.debug_logger.info(f"Using Vision Mamba (Vim-S) backbone with projection (384->768), img_size={img_size}")
            
        else:
            # ViT-Base (é»˜è®¤)
            self.visual_encoder = ViTModel.from_pretrained(str(vit_base_path), weights_only=False)
            self.visual_proj = nn.Identity() # ViT è¾“å‡ºå·²ç»æ˜¯ 768ï¼Œæ— éœ€æŠ•å½±
            if self.logger:
                self.debug_logger.info("Using ViT-Base backbone")

        # === åˆå§‹åŒ–ç‰¹å¾åˆ†ç¦»æ¨¡å—ï¼šFSHDä¸ºé»˜è®¤æ¨¡å¼ ===
        disentangle_type = net_config.get('disentangle_type', 'fshd')
        gs3_config = net_config.get('gs3', {})
        
        if disentangle_type == 'fshd':
            # æ¿€è¿›æ–¹æ¡ˆï¼šFSHDæ¨¡å—ï¼ˆé¢‘åŸŸ-ç©ºåŸŸè”åˆè§£è€¦ï¼‰
            self.disentangle = FSHDModule(
                dim=self.text_width,
                num_heads=gs3_config.get('num_heads', 8),
                d_state=gs3_config.get('d_state', 16),
                d_conv=gs3_config.get('d_conv', 4),
                dropout=gs3_config.get('dropout', 0.1),
                img_size=gs3_config.get('img_size', (14, 14)),  # patch grid size
                use_multi_scale_cnn=gs3_config.get('use_multi_scale_cnn', True),
                logger=self.logger
            )
            if self.logger:
                self.debug_logger.info("ğŸ”¥ Using FSHD Module (Frequency-Spatial Hybrid Decoupling)")
                self.debug_logger.info(f"   - Multi-scale CNN: {gs3_config.get('use_multi_scale_cnn', True)}")
        else:
            # ç®€åŒ–ç‰ˆï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰
            self.disentangle = DisentangleModule(dim=self.text_width)
            if self.logger:
                self.debug_logger.info("Using simplified disentangle module")
        
        # === æ–°å¢ï¼šCLIPè¯­ä¹‰å¼•å¯¼æ¨¡å— ===
        # ä½¿ç”¨å·²åŠ è½½çš„CLIP Text Encoderå’ŒTokenizer
        self.semantic_guidance = SemanticGuidedDecoupling(
            text_encoder=self.text_encoder,
            tokenizer=self.tokenizer,
            dim=self.text_width,
            logger=self.logger
        )
        if self.logger:
            self.debug_logger.info("âœ… Initialized CLIP Semantic Guidance for ID/Attr decoupling")

        # ================================================================
        # === æ–¹æ¡ˆC (ä¼˜åŒ–ç‰ˆ)ï¼šBNNeck ç›‘ç£ - ä½¿ç”¨ BNNeck æ›¿ä»£æ·±å±‚æ®‹å·®åˆ†ç±»å™¨ ===
        # ================================================================
        
        # === åˆ†æ”¯1ï¼šä¸“ç”¨äºåˆ†ç±»çš„idç‰¹å¾å¤„ç† ===
        # ä½¿ç”¨ BNNeck (BatchNorm1d) è§„èŒƒåŒ–ç‰¹å¾åˆ†å¸ƒ
        # è¿™ä¸€æ­¥å°†ç‰¹å¾æ‹‰å‘è¶…çƒé¢ï¼Œæœ‰åˆ©äº CrossEntropy æ”¶æ•›ï¼ŒåŒæ—¶ä¸ç ´åæµå½¢ç»“æ„
        self.id_bn = nn.BatchNorm1d(self.text_width)
        # åˆå§‹åŒ– BNNeckï¼šweight=1, bias=0 (æ ‡å‡†åšæ³•)
        nn.init.constant_(self.id_bn.weight, 1.0)
        nn.init.constant_(self.id_bn.bias, 0.0)
        
        if self.logger:
            self.debug_logger.info("Using BNNeck (BatchNorm1d) for classification branch")

        # èº«ä»½åˆ†ç±»å™¨ï¼šä» text_width (768) ç›´æ¥æ˜ å°„åˆ°ç±»åˆ«æ•°
        # bias=False æ˜¯ ReID å¸¸è§ Trickï¼Œè®©æ¨¡å‹å…³æ³¨å‘é‡è§’åº¦è€Œéæ¨¡é•¿
        self.id_classifier = nn.Linear(self.text_width, num_classes, bias=False)
        # åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡
        nn.init.normal_(self.id_classifier.weight, std=0.02)
        
        # === åˆ†æ”¯2ï¼šä¸“ç”¨äºæ£€ç´¢çš„idç‰¹å¾å¤„ç†ï¼ˆä¿æŒåŸæœ‰è®¾è®¡ï¼‰===
        # å…±äº«MLPï¼šç”¨äºé™ç»´
        # === åˆ†æ”¯2ï¼šä¸“ç”¨äºæ£€ç´¢çš„idç‰¹å¾å¤„ç†ï¼ˆä¿æŒåŸæœ‰è®¾è®¡ï¼‰===
        # å…±äº«MLPï¼šç”¨äºé™ç»´
        self.shared_mlp = nn.Linear(self.text_width, 512)

        # å›¾åƒç‰¹å¾MLPï¼šå¤šå±‚æ˜ å°„
        self.image_mlp = nn.Sequential(
            nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(256, 256)
        )

        # æ–‡æœ¬ç‰¹å¾MLPï¼šå¤šå±‚æ˜ å°„
        self.text_mlp = nn.Sequential(
            nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(256, 256)
        )
        
        # === åˆ†æ”¯3ï¼šclothç‰¹å¾å¤„ç†ï¼ˆå…±äº«æ£€ç´¢åˆ†æ”¯çš„MLPï¼‰===
        # cloth_embedsä¹Ÿä½¿ç”¨shared_mlpå’Œimage_mlpè¿›è¡ŒæŠ•å½±
        # è¿™æ ·å¯ä»¥ç¡®ä¿clothç‰¹å¾å’Œidç‰¹å¾åœ¨åŒä¸€ç©ºé—´ä¸­å¯¹æ¯”
        
        if self.logger:
            self.debug_logger.info("=" * 60)
            self.debug_logger.info("Branch Decoupling Architecture (Optimized):")
            self.debug_logger.info(f"  - Classification Branch: {self.text_width} â†’ BNNeck(768) â†’ {num_classes}")
            self.debug_logger.info(f"  - Retrieval Branch: {self.text_width} â†’ 512 â†’ 256")
            self.debug_logger.info(f"  - Total Classifier Params: ~{self._count_classifier_params() / 1e6:.2f}M")
            self.debug_logger.info("=" * 60)

        # === æ–‡æœ¬ç¼–ç å™¨å‡çº§ï¼šPyramidTextEncoder ===
        # æ›¿ä»£åŸæœ‰çš„ 3 å±‚æ–‡æœ¬è‡ªæ³¨æ„åŠ›æ¨¡å—
        if HAS_MAMBA:
            self.text_encoder_v2 = PyramidTextEncoder(dim=self.text_width)
            if self.logger:
                self.debug_logger.info("âœ… Initialized PyramidTextEncoder (CNN+Mamba) for text processing")
        else:
            self.text_encoder_v2 = None
            if self.logger:
                self.debug_logger.warning("âš ï¸  Mamba not found. Falling back to legacy Attention layers for text encoder.")
            
            # Legacy Fallback
            self.text_attn_layers = nn.ModuleList([
                nn.MultiheadAttention(embed_dim=self.text_width, num_heads=4, dropout=0.1) for _ in range(3)
            ])
            self.text_attn_norm_layers = nn.ModuleList([
                nn.LayerNorm(self.text_width) for _ in range(3)
            ])

        # åˆå§‹åŒ–èåˆæ¨¡å—
        self.fusion = get_fusion_module(fusion_config) if fusion_config else None
        self.feat_dim = fusion_config.get("output_dim", 256) if fusion_config else 256

        # åˆå§‹åŒ–å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
        self.scale = nn.Parameter(torch.ones(1), requires_grad=True)

        # æ—¥å¿—è®°å½•åˆå§‹åŒ–ä¿¡æ¯
        if self.logger:
            self.debug_logger.info(
                f"Initialized model with scale: {self.scale.item():.4f}, fusion: {fusion_config.get('type', 'None')}")

        # æ–‡æœ¬åˆ†è¯ç»“æœç¼“å­˜
        self.text_cache = {}
    
    def _count_classifier_params(self):
        """è®¡ç®—åˆ†ç±»åˆ†æ”¯çš„å‚æ•°é‡"""
        params = sum(p.numel() for p in self.id_bn.parameters())
        params += sum(p.numel() for p in self.id_classifier.parameters())
        return params

    def encode_image(self, image):
        """
        ç¼–ç å›¾åƒï¼Œæå–å›¾åƒç‰¹å¾å¹¶è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿ç”¨ ViT/Vim æ•´ä¸ªåºåˆ—ã€‚

        Args:
            image (torch.Tensor): è¾“å…¥å›¾åƒï¼Œå½¢çŠ¶ä¸º [batch_size, channels, height, width] æˆ–æ›´é«˜ç»´ã€‚

        Returns:
            torch.Tensor: æ ‡å‡†åŒ–åçš„å›¾åƒåµŒå…¥ï¼Œå½¢çŠ¶ä¸º [batch_size, 256]ã€‚
        """
        if image is None:
            return None
        device = next(self.parameters()).device
        if image.dim() == 5:
            image = image.squeeze(-1)
        image = image.to(device)
        
        # è·å–å›¾åƒç‰¹å¾
        if self.vision_backbone_type == 'vim':
            # Vim Numerical Stability Fix: Force float32
            with torch.cuda.amp.autocast(enabled=False):
                image_embeds_raw = self.visual_encoder(image.float())
                # Vim returns [batch_size, seq_len, 384]
            # Cast back to match mixed precision context if needed, or keep float32
            image_embeds_raw = image_embeds_raw.to(image.dtype)
        else:
            # ViT è¿”å› BaseModelOutputï¼Œå– last_hidden_state [batch_size, seq_len, 768]
            image_outputs = self.visual_encoder(image)
            image_embeds_raw = image_outputs.last_hidden_state
            
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦ (å¦‚æœæ˜¯ Vim: 384->768; å¦‚æœæ˜¯ ViT: Identity)
        image_embeds_raw = self.visual_proj(image_embeds_raw)
        
        # åç»­å¤„ç†ä¿æŒä¸å˜ (è§£è€¦ -> æ£€ç´¢MLP -> å½’ä¸€åŒ–)
        id_embeds, _, _, _ = self.disentangle(image_embeds_raw)  # [batch_size, hidden_size]
        image_embeds = self.shared_mlp(id_embeds)
        image_embeds = self.image_mlp(image_embeds)
        image_embeds = torch.nn.functional.normalize(image_embeds, dim=-1, eps=1e-8)
        return image_embeds

    def encode_text(self, instruction):
        """
        ç¼–ç æ–‡æœ¬ï¼Œæå–æ–‡æœ¬ç‰¹å¾å¹¶è¿›è¡Œæ ‡å‡†åŒ–ã€‚
        Adapted for CLIP:
        1. Max length 77
        2. Projection 512 -> 768
        """
        if instruction is None:
            return None
        device = next(self.parameters()).device
        if isinstance(instruction, (list, tuple)):
            texts = list(instruction)
        else:
            texts = [instruction]

        # æ£€æŸ¥ç¼“å­˜ä»¥å¤ç”¨åˆ†è¯ç»“æœ
        cache_key = tuple(texts)
        if cache_key in self.text_cache:
            tokenized = self.text_cache[cache_key]
        else:
            # CLIP Limit is 77
            tokenized = self.tokenizer(
                texts,
                padding='max_length',
                max_length=77,  # CLIP specific
                truncation=True,
                return_tensors="pt",
                return_attention_mask=True
            )
            self.text_cache[cache_key] = tokenized

        input_ids = tokenized['input_ids'].to(device)
        attention_mask = tokenized['attention_mask'].to(device)

        # CLIPç¼–ç 
        # CLIP output: last_hidden_state=[B, 77, 512], pooler_output=[B, 512]
        # æˆ‘ä»¬ä½¿ç”¨ last_hidden_state ä»¥ä¿ç•™åºåˆ—ä¿¡æ¯ç”¨äºåç»­ Attention
        text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask)
        text_embeds = text_outputs.last_hidden_state 
        
        # === ç»´åº¦é€‚é…: 512 -> 768 ===
        text_embeds = self.text_proj(text_embeds) # [B, 77, 768]

        # === æ–‡æœ¬ç‰¹å¾æå– (Pyramid vs Legacy) ===
        if self.text_encoder_v2 is not None:
            # ä½¿ç”¨ PyramidTextEncoder
            # output dict: {'feat_attr', 'feat_id', 'feat_id_bn'}
            out_v2 = self.text_encoder_v2(text_embeds)
            
            # å¯¹äº encode_text (é€šå¸¸ç”¨äºæ£€ç´¢)ï¼Œæˆ‘ä»¬ä½¿ç”¨ ID ç‰¹å¾
            # æ³¨æ„ï¼šPyramid Encoder è¿”å›çš„ feat_id å·²ç»æ˜¯ [B, D] ä¸”ç»è¿‡ LayerNorm
            text_embeds = out_v2['feat_id']
            
            # ä¸ºäº†å…¼å®¹åç»­çš„ MLP æŠ•å½± (shared_mlp -> text_mlp)
            # åŸå§‹é€»è¾‘æ˜¯: text_embeds (Attn output) -> shared_mlp -> text_mlp
            # è¿™é‡Œçš„ text_embeds ç›¸å½“äº Attn output
        else:
            # Legacy Fallback: 3 å±‚è‡ªæ³¨æ„åŠ›å¤„ç†
            # æ³¨æ„: CLIP attention mask æ˜¯ 1 (attend), 0 (ignore)
            # nn.MultiheadAttention key_padding_mask éœ€è¦ True (ignore), False (attend)
            # æ‰€ä»¥ä½¿ç”¨ ~attention_mask.bool()
            
            text_embeds = text_embeds.transpose(0, 1)  # [seq_len, batch_size, hidden_size]
            for attn, norm in zip(self.text_attn_layers, self.text_attn_norm_layers):
                attn_output, _ = attn(
                    query=text_embeds,
                    key=text_embeds,
                    value=text_embeds,
                    key_padding_mask=~attention_mask.bool()
                )
                text_embeds = attn_output + text_embeds  # æ®‹å·®è¿æ¥
                text_embeds = norm(text_embeds)
            text_embeds = text_embeds.transpose(0, 1)  # [batch_size, seq_len, hidden_size]

            # å‡å€¼æ± åŒ–ï¼Œç»“åˆ attention_mask å¿½ç•¥å¡«å…… token
            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]
            text_embeds = torch.sum(text_embeds * attention_mask, dim=1) / torch.sum(attention_mask, dim=1)
            # å½¢çŠ¶: [batch_size, hidden_size]

        # é™ç»´å’Œæ ‡å‡†åŒ– (ç»Ÿä¸€æŠ•å½±æ¥å£)
        text_embeds = self.shared_mlp(text_embeds)
        text_embeds = self.text_mlp(text_embeds)
        # ä½¿ç”¨æ›´ç¨³å®šçš„å½’ä¸€åŒ–ï¼Œæ·»åŠ epsé¿å…é™¤é›¶
        text_embeds = torch.nn.functional.normalize(text_embeds, dim=-1, eps=1e-8)

        if not isinstance(instruction, list):
            text_embeds = text_embeds.squeeze(0)
        return text_embeds

    def forward(self, image=None, cloth_instruction=None, id_instruction=None, return_attention=False):
        """
        å‰å‘ä¼ æ’­ï¼Œå¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºå¤šæ¨¡æ€ç‰¹å¾å’Œåˆ†ç±»ç»“æœã€‚
        
        === é‡æ„åçš„æµç¨‹ï¼ˆåˆ†æ”¯è§£è€¦ï¼‰===
        1. ViTç¼–ç  â†’ image_embeds [B, 197, 768]
        2. G-S3è§£è€¦ â†’ id_embeds, cloth_embeds [B, 768]
        3. åˆ†æ”¯1ï¼ˆåˆ†ç±»ï¼‰ï¼šid_embeds â†’ id_for_classification â†’ id_logits
        4. åˆ†æ”¯2ï¼ˆæ£€ç´¢ï¼‰ï¼šid_embeds â†’ shared_mlp â†’ image_mlp â†’ image_embeds
        5. åˆ†æ”¯3ï¼ˆclothï¼‰ï¼šcloth_embeds â†’ shared_mlp â†’ image_mlp â†’ cloth_image_embeds

        Args:
            image (torch.Tensor, optional): è¾“å…¥å›¾åƒï¼Œå½¢çŠ¶ä¸º [batch_size, channels, height, width]ã€‚
            cloth_instruction (str or list, optional): æœè£…æè¿°æ–‡æœ¬ã€‚
            id_instruction (str or list, optional): èº«ä»½æè¿°æ–‡æœ¬ã€‚
            return_attention (bool): æ˜¯å¦è¿”å›æ³¨æ„åŠ›å›¾ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ã€‚

        Returns:
            tuple: (image_embeds, id_text_embeds, fused_embeds, id_logits, id_embeds,
                    cloth_embeds, cloth_text_embeds, cloth_image_embeds, gate, gate_weights,
                    id_cls_features)  # æ–°å¢ï¼šåˆ†ç±»åˆ†æ”¯çš„ä¸­é—´ç‰¹å¾
                   æˆ–åŒ…å«æ³¨æ„åŠ›å›¾çš„æ‰©å±•å…ƒç»„
        """
        device = next(self.parameters()).device
        id_logits, id_embeds, cloth_embeds, gate = None, None, None, None
        id_attn_map, cloth_attn_map = None, None
        id_cls_features = None  # æ–°å¢ï¼šåˆ†ç±»åˆ†æ”¯çš„ä¸­é—´ç‰¹å¾
        
        if image is not None:
            if image.dim() == 5:
                image = image.squeeze(-1)
            image = image.to(device)
            
            # === è·å–å›¾åƒåŸå§‹ç‰¹å¾ ===
            if self.vision_backbone_type == 'vim':
                # Vim Numerical Stability Fix: Force float32
                with torch.cuda.amp.autocast(enabled=False):
                    image_embeds_raw = self.visual_encoder(image.float())
            else:
                image_outputs = self.visual_encoder(image)
                image_embeds_raw = image_outputs.last_hidden_state  # [B, 197, 768]
            
            # æ£€æŸ¥ç¼–ç å™¨è¾“å‡ºNaN
            if torch.isnan(image_embeds_raw).any():
                if self.logger:
                    self.debug_logger.error("âš ï¸  CRITICAL: Visual encoder output contains NaN!")
                # è¿”å›é›¶å¼ é‡é¿å…å´©æºƒ
                image_embeds_raw = torch.zeros_like(image_embeds_raw)
            
            # === ç»´åº¦å¯¹é½ (Vim 384 -> 768, ViT 768 -> 768) ===
            image_embeds_raw = self.visual_proj(image_embeds_raw)
            
            # æ£€æŸ¥æŠ•å½±åNaN
            if torch.isnan(image_embeds_raw).any():
                if self.logger:
                    self.debug_logger.error("âš ï¸  CRITICAL: Visual projection output contains NaN!")
                image_embeds_raw = torch.zeros_like(image_embeds_raw)
            
            # ============================================================
            # æ­¥éª¤1ï¼šå¯¹ç§°G-S3è§£è€¦ï¼Œå¾—åˆ°idå’Œattrç‰¹å¾
            # ============================================================
            if return_attention:
                id_embeds, cloth_embeds, gate_stats, original_feat, id_attn_map, cloth_attn_map = self.disentangle(
                    image_embeds_raw, return_attention=True)
            else:
                id_embeds, cloth_embeds, gate_stats, original_feat = self.disentangle(
                    image_embeds_raw, return_attention=False)
            
            # NaNæ£€æŸ¥
            if torch.isnan(id_embeds).any():
                if self.logger:
                    self.debug_logger.error("âš ï¸  CRITICAL: id_embeds contains NaN after disentangle!")
                id_embeds = torch.zeros_like(id_embeds)
            if torch.isnan(cloth_embeds).any():
                if self.logger:
                    self.debug_logger.error("âš ï¸  CRITICAL: cloth_embeds contains NaN after disentangle!")
                cloth_embeds = torch.zeros_like(cloth_embeds)
            
            # gate_statsç°åœ¨æ˜¯ä¸€ä¸ªdictï¼Œè®°å½•åˆ°æ—¥å¿—
            if self.logger and hasattr(self, '_log_counter_gate'):
                self._log_counter_gate = getattr(self, '_log_counter_gate', 0) + 1
                if self._log_counter_gate % 200 == 0:
                    self.debug_logger.debug(
                        f"Gate stats: ID[mean={gate_stats['gate_id_mean']:.4f}, std={gate_stats['gate_id_std']:.4f}], "
                        f"Attr[mean={gate_stats['gate_attr_mean']:.4f}, std={gate_stats['gate_attr_std']:.4f}], "
                        f"Diversity={gate_stats['diversity']:.4f}"
                    )
            
            # å­˜å‚¨ä¸­é—´ç‰¹å¾ç”¨äºè°ƒè¯•ï¼ˆä»…åœ¨ debug æ¨¡å¼ä¸‹ï¼‰
            if hasattr(self, '_debug_mode') and self._debug_mode:
                self._debug_features = {
                    'image_embeds_raw': image_embeds_raw,
                    'id_embeds': id_embeds,
                    'cloth_embeds': cloth_embeds,
                    'gate_stats': gate_stats
                }
                if hasattr(self.disentangle, '_debug_info'):
                    self._debug_features.update(self.disentangle._debug_info)
            
            # ============================================================
            # æ­¥éª¤2ï¼šåˆ†æ”¯1 - åˆ†ç±»åˆ†æ”¯ï¼ˆBNNeck éšå¼ç›‘ç£ï¼‰
            # ============================================================
            # 1. é€šè¿‡ BNNeck è¿›è¡Œç‰¹å¾è§„èŒƒåŒ–
            # id_embeds [B, 768] â†’ id_cls_features [B, 768]
            id_cls_features = self.id_bn(id_embeds)
            
            # 2. è®¡ç®—åˆ†ç±» Logits
            # id_cls_features [B, 768] â†’ id_logits [B, num_classes]
            id_logits = self.id_classifier(id_cls_features)
            
            # æ³¨æ„ï¼šæ£€ç´¢åˆ†æ”¯ä¾ç„¶ä½¿ç”¨åŸå§‹ id_embedsï¼Œè¿™ä½¿å¾— id_embeds åŒæ—¶å—åˆ° 
            # BNNeck(åˆ†ç±») å’Œ MLP(æ£€ç´¢) çš„åŒé‡æ¢¯åº¦çº¦æŸï¼Œä¿ƒè¿›ç‰¹å¾é²æ£’æ€§
            
            # ============================================================
            # æ­¥éª¤3ï¼šåˆ†æ”¯2 - æ£€ç´¢åˆ†æ”¯ï¼ˆç”¨äºinfo_nceï¼‰
            # ============================================================
            # id_embeds [B, 768] â†’ shared_mlp [B, 512] â†’ image_mlp [B, 256]
            image_embeds = self.shared_mlp(id_embeds)
            image_embeds = self.image_mlp(image_embeds)
            image_embeds = torch.nn.functional.normalize(image_embeds, dim=-1, eps=1e-8)
            
            # ============================================================
            # æ­¥éª¤4ï¼šåˆ†æ”¯3 - clothæ£€ç´¢åˆ†æ”¯ï¼ˆç”¨äºcloth_semanticï¼‰
            # ============================================================
            # cloth_embeds [B, 768] â†’ shared_mlp [B, 512] â†’ image_mlp [B, 256]
            cloth_image_embeds = self.shared_mlp(cloth_embeds)
            cloth_image_embeds = self.image_mlp(cloth_image_embeds)
            cloth_image_embeds = torch.nn.functional.normalize(cloth_image_embeds, dim=-1, eps=1e-8)
        else:
            image_embeds = None
            cloth_image_embeds = None
            gate_stats = None

        # ============================================================
        # æ­¥éª¤5ï¼šæ–‡æœ¬ç¼–ç å’Œèåˆ (Updated for Pyramid Encoder)
        # ============================================================
        
        # ç­–ç•¥ï¼šå¦‚æœä½¿ç”¨ PyramidTextEncoderï¼Œæˆ‘ä»¬å€¾å‘äºä½¿ç”¨ä¸€æ®µå®Œæ•´çš„æ–‡æœ¬ (id_instruction) 
        # æ¥åŒæ—¶æå– cloth å’Œ id ç‰¹å¾ï¼Œè€Œä¸æ˜¯åˆ†åˆ«ç¼–ç ä¸¤ä¸ªç‰‡æ®µã€‚
        # ä½†ä¸ºäº†å…¼å®¹ç°æœ‰æ¥å£ï¼Œæˆ‘ä»¬ä¿ç•™ cloth_instruction çš„è¾“å…¥ï¼Œä½†ä¸»è¦é€»è¾‘åŸºäº id_instruction
        
        # ç¡®å®šä¸»æ–‡æœ¬è¾“å…¥ (Main Text Source)
        # å¦‚æœæä¾›äº† id_instructionï¼Œæˆ‘ä»¬å‡è®¾å®ƒæ˜¯åŒ…å«ä¸°å¯Œä¿¡æ¯çš„å®Œæ•´æè¿°ï¼ˆæˆ–IDéƒ¨åˆ†ï¼‰
        main_instruction = id_instruction if id_instruction is not None else cloth_instruction
        
        # åˆå§‹åŒ–å˜é‡
        feat_attr_raw, feat_id_raw = None, None
        
        if self.text_encoder_v2 is not None and main_instruction is not None:
            # === æ–°é€»è¾‘: å•æµè¾“å…¥ï¼ŒåŒå¤´è¾“å‡º ===
            
            # 1. è·å– CLIP åŸå§‹ Embeddings [B, 77, 768] (æ— æ± åŒ–)
            if isinstance(main_instruction, (list, tuple)):
                texts = list(main_instruction)
            else:
                texts = [main_instruction]
                
            cache_key = tuple(texts)
            if cache_key in self.text_cache:
                tokenized = self.text_cache[cache_key]
            else:
                tokenized = self.tokenizer(texts, padding='max_length', max_length=77, truncation=True, return_tensors="pt", return_attention_mask=True)
                self.text_cache[cache_key] = tokenized
            
            input_ids = tokenized['input_ids'].to(device)
            attention_mask = tokenized['attention_mask'].to(device)
            
            text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask)
            text_seq_embeds = text_outputs.last_hidden_state # [B, 77, 512]
            text_seq_embeds = self.text_proj(text_seq_embeds) # [B, 77, 768]
            
            # 2. Pyramid Encoder å¤„ç†
            out_v2 = self.text_encoder_v2(text_seq_embeds)
            
            # 3. è·å–è§£è€¦ç‰¹å¾ [B, 768]
            feat_attr_raw = out_v2['feat_attr']
            feat_id_raw = out_v2['feat_id']
            # feat_id_bn = out_v2['feat_id_bn'] # ç”¨äºåˆ†ç±»æŸå¤±
            
            # 4. æŠ•å½±åˆ°å…¬å…±ç©ºé—´ (768 -> 512 -> 256) ä»¥åŒ¹é…å›¾åƒç‰¹å¾
            #    cloth_text_embeds (Attr)
            cloth_text_embeds = self.shared_mlp(feat_attr_raw)
            cloth_text_embeds = self.text_mlp(cloth_text_embeds)
            cloth_text_embeds = torch.nn.functional.normalize(cloth_text_embeds, dim=-1, eps=1e-8)
            
            #    id_text_embeds (ID)
            id_text_embeds = self.shared_mlp(feat_id_raw)
            id_text_embeds = self.text_mlp(id_text_embeds)
            id_text_embeds = torch.nn.functional.normalize(id_text_embeds, dim=-1, eps=1e-8)
            
        else:
            # === æ—§é€»è¾‘: åŒæµåˆ†åˆ«ç¼–ç  ===
            # Fallback to separate encoding
            cloth_text_embeds = self.encode_text(cloth_instruction)
            id_text_embeds = self.encode_text(id_instruction)
        
        fused_embeds, gate_weights = None, None
        
        # === èåˆæ¨¡å—è°ƒç”¨ (æ”¯æŒ Fusion V2) ===
        if self.fusion and image_embeds is not None and id_text_embeds is not None:
            if isinstance(self.fusion, SamgRcsmFusion):
                # Fusion V2 éœ€è¦: img_id, img_attr, txt_id, txt_attr, energy_ratio
                # å›¾åƒç‰¹å¾: id_embeds, cloth_embeds (æ¥è‡ª disentangle)
                # æ–‡æœ¬ç‰¹å¾: feat_id_raw, feat_attr_raw (æ¥è‡ª Pyramid)
                # èƒ½é‡æ¯”ç‡: gate_stats['energy_ratio']
                
                energy_ratio = gate_stats.get('energy_ratio') if gate_stats else None
                
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½å¯ç”¨
                if feat_id_raw is not None and feat_attr_raw is not None and energy_ratio is not None:
                    fused_embeds, gate_weights = self.fusion(
                        img_id=id_embeds, 
                        img_attr=cloth_embeds,
                        txt_id=feat_id_raw, 
                        txt_attr=feat_attr_raw,
                        energy_ratio=energy_ratio
                    )
                    fused_embeds = self.scale * torch.nn.functional.normalize(fused_embeds, dim=-1, eps=1e-8)
                else:
                    # å¦‚æœç¼ºå°‘æŸäº›è¾“å…¥ï¼ˆå¦‚Legacy Text Encoderå¯¼è‡´æ— rawç‰¹å¾ï¼‰ï¼Œå›é€€æˆ–è·³è¿‡
                     if self.logger:
                        self.debug_logger.warning("FusionV2 active but missing inputs (raw text feats or energy).")
            else:
                # Legacy Fusion
                fused_embeds, gate_weights = self.fusion(image_embeds, id_text_embeds)
                fused_embeds = self.scale * torch.nn.functional.normalize(fused_embeds, dim=-1, eps=1e-8)
        else:
            # Fusionæ¨¡å—æœªæ¿€æ´»æ—¶ï¼Œä½¿ç”¨image_embedsä½œä¸ºfallback
            # ç¡®ä¿fused_embedså§‹ç»ˆæœ‰å€¼å‚ä¸æŸå¤±è®¡ç®—
            if image_embeds is not None:
                fused_embeds = image_embeds
        
        # ============================================================
        # è¿”å›å€¼ï¼ˆæ–°å¢ï¼šoriginal_featç”¨äºé‡æ„ç›‘ç£ï¼‰
        # ============================================================
        # æ³¨æ„ï¼šgate_statsæ˜¯dictï¼ŒåŒ…å«é—¨æ§ç»Ÿè®¡ä¿¡æ¯
        base_outputs = (image_embeds, id_text_embeds, fused_embeds, id_logits, id_embeds,
                       cloth_embeds, cloth_text_embeds, cloth_image_embeds, gate_stats, gate_weights,
                       id_cls_features, original_feat)  # æ–°å¢original_feat
        
        if return_attention:
            return base_outputs + (id_attn_map, cloth_attn_map)
        else:
            return base_outputs

    def load_param(self, trained_path):
        """
        åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚

        Args:
            trained_path (str): é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚

        Returns:
            T2IReIDModel: åŠ è½½å‚æ•°åçš„æ¨¡å‹ã€‚
        """
        trained_path = Path(trained_path)
        checkpoint = torch.load(trained_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
        self = copy_state_dict(state_dict, self)
        if self.logger:
            self.debug_logger.info(f"Loaded checkpoint from {trained_path}, scale: {self.scale.item():.4f}")
        return self