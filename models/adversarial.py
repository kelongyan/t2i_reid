# models/adversarial.py
"""
Adversarial Decoupling Module
å¯¹æŠ—å¼è§£è€¦ï¼šé€šè¿‡åˆ¤åˆ«å™¨å¼ºåˆ¶IDç‰¹å¾æ— æ³•é¢„æµ‹æœè£…å±æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function


class GradientReversalFunction(Function):
    """
    æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer)
    
    å‰å‘ä¼ æ’­ï¼šy = x
    åå‘ä¼ æ’­ï¼šdy/dx = -lambda * grad_output
    
    ç”¨äºå¯¹æŠ—è®­ç»ƒï¼Œè®©ç‰¹å¾æå–å™¨"æ¬ºéª—"åˆ¤åˆ«å™¨
    """
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.lambda_ * grad_output, None


class GradientReversalLayer(nn.Module):
    """æ¢¯åº¦åè½¬å±‚åŒ…è£…å™¨"""
    def __init__(self, lambda_=1.0):
        super().__init__()
        self.lambda_ = lambda_
    
    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)
    
    def set_lambda(self, lambda_):
        """åŠ¨æ€è°ƒæ•´åè½¬å¼ºåº¦"""
        self.lambda_ = lambda_


class AttributeDiscriminator(nn.Module):
    """
    å±æ€§åˆ¤åˆ«å™¨
    
    ç›®æ ‡ï¼šåˆ¤æ–­ç‰¹å¾ä¸­æ˜¯å¦åŒ…å«æœè£…å±æ€§ä¿¡æ¯
    
    è®­ç»ƒç­–ç•¥ï¼š
    - Discriminator Loss: æœ€å¤§åŒ–åˆ†ç±»å‡†ç¡®ç‡ï¼ˆè®©åˆ¤åˆ«å™¨å­¦ä¼šè¯†åˆ«å±æ€§ï¼‰
    - Feature Extractor Loss: æœ€å°åŒ–åˆ†ç±»å‡†ç¡®ç‡ï¼ˆé€šè¿‡GRLè®©ç‰¹å¾æ— æ³•è¢«è¯†åˆ«ï¼‰
    
    Args:
        dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        num_attributes: å±æ€§ç±»åˆ«æ•°ï¼ˆåŠ¨æ€è®¡ç®—ï¼Œæˆ–ä½¿ç”¨è™šæ‹Ÿæ ‡ç­¾ï¼‰
        hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
        dropout: Dropoutæ¯”ä¾‹
    """
    def __init__(self, dim=768, num_attributes=128, hidden_dims=[512, 256], dropout=0.3):
        super().__init__()
        self.dim = dim
        self.num_attributes = num_attributes
        
        # æ¢¯åº¦åè½¬å±‚
        self.grl = GradientReversalLayer(lambda_=1.0)
        
        # åˆ¤åˆ«å™¨ç½‘ç»œ (Multi-layer MLP)
        layers = []
        in_dim = dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout)
            ])
            in_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(in_dim, num_attributes))
        
        self.discriminator = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """ğŸ”¥ æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼Œé™ä½åˆå§‹æŸå¤±"""
        for m in self.discriminator.modules():
            if isinstance(m, nn.Linear):
                # ğŸ”¥ ä½¿ç”¨æ›´å°çš„gainï¼Œé™ä½åˆ¤åˆ«å™¨åˆå§‹èƒ½åŠ›
                # è®©å¯¹æŠ—è®­ç»ƒä»æ›´å¹³è¡¡çš„çŠ¶æ€å¼€å§‹
                nn.init.xavier_normal_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, features, reverse_grad=True):
        """
        Args:
            features: [B, D] è¾“å…¥ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯IDç‰¹å¾ï¼‰
            reverse_grad: æ˜¯å¦åè½¬æ¢¯åº¦ï¼ˆè®­ç»ƒç‰¹å¾æå–å™¨æ—¶Trueï¼Œè®­ç»ƒåˆ¤åˆ«å™¨æ—¶Falseï¼‰
        
        Returns:
            logits: [B, num_attributes] å±æ€§åˆ†ç±»logits
        """
        if reverse_grad:
            features = self.grl(features)
        
        logits = self.discriminator(features)
        return logits
    
    def set_lambda(self, lambda_):
        """åŠ¨æ€è°ƒæ•´æ¢¯åº¦åè½¬å¼ºåº¦"""
        self.grl.set_lambda(lambda_)


class DomainDiscriminator(nn.Module):
    """
    åŸŸåˆ¤åˆ«å™¨ (å¯é€‰)
    
    åˆ¤æ–­ç‰¹å¾æ¥è‡ªIDåˆ†æ”¯è¿˜æ˜¯Attråˆ†æ”¯
    ç”¨äºå¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯å­¦ä¹ ä¸åŒçš„è¡¨å¾
    """
    def __init__(self, dim=768, hidden_dim=512, dropout=0.3):
        super().__init__()
        self.grl = GradientReversalLayer(lambda_=1.0)
        
        self.discriminator = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(256, 2)  # Binary: ID or Attr
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.discriminator.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=0.5)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, features, reverse_grad=True):
        """
        Args:
            features: [B, D]
            reverse_grad: Trueè¡¨ç¤ºè®­ç»ƒç‰¹å¾æå–å™¨ï¼ŒFalseè¡¨ç¤ºè®­ç»ƒåˆ¤åˆ«å™¨
        
        Returns:
            logits: [B, 2] (0=IDåˆ†æ”¯, 1=Attråˆ†æ”¯)
        """
        if reverse_grad:
            features = self.grl(features)
        
        logits = self.discriminator(features)
        return logits
    
    def set_lambda(self, lambda_):
        self.grl.set_lambda(lambda_)


def compute_attribute_pseudo_labels(cloth_embeds, num_clusters=128):
    """
    ğŸ”¥ æ”¹è¿›çš„ä¼ªæ ‡ç­¾ç”Ÿæˆæ–¹æ³•

    æ”¹è¿›ï¼š
    1. ä½¿ç”¨å¤šä¸ªç»´åº¦çš„åŠ æƒç»„åˆï¼ˆè€Œéç®€å•å“ˆå¸Œï¼‰
    2. æ·»åŠ éšæœºæ‰°åŠ¨ï¼Œé¿å…ä¼ªæ ‡ç­¾è¿‡äºå›ºå®š
    3. ç¡®ä¿æ¯ä¸ªbatchæœ‰è¶³å¤Ÿçš„ç±»åˆ«å¤šæ ·æ€§

    Args:
        cloth_embeds: [B, D] æœè£…ç‰¹å¾
        num_clusters: èšç±»æ•°é‡ï¼ˆä¼ªå±æ€§ç±»åˆ«æ•°ï¼‰

    Returns:
        pseudo_labels: [B] ä¼ªæ ‡ç­¾
    """
    with torch.no_grad():
        # å½’ä¸€åŒ–
        cloth_embeds_norm = F.normalize(cloth_embeds, dim=-1, eps=1e-8)

        # ğŸ”¥ æ”¹è¿›1ï¼šä½¿ç”¨æ›´å¤šç»´åº¦ï¼Œå¢åŠ å¤šæ ·æ€§
        n_dims = min(16, cloth_embeds_norm.shape[1])  # ä½¿ç”¨å‰16ä¸ªç»´åº¦

        # ğŸ”¥ æ”¹è¿›2ï¼šåŠ æƒç»„åˆï¼Œè€Œéç®€å•çš„äºŒè¿›åˆ¶
        # ä½¿ç”¨è´¨æ•°ä½œä¸ºæƒé‡ï¼Œå‡å°‘ç¢°æ’
        weights = torch.tensor([
            1, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53
        ], device=cloth_embeds.device)[:n_dims]

        # å°†ç‰¹å¾å€¼ç¦»æ•£åŒ–ä¸º-1, 0, 1ä¸‰ä¸ªçº§åˆ«
        discretized = torch.sign(cloth_embeds_norm[:, :n_dims])

        # åŠ æƒæ±‚å’Œ
        pseudo_labels = (discretized * weights).sum(dim=1)

        # ğŸ”¥ æ”¹è¿›3ï¼šå–æ¨¡ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        pseudo_labels = pseudo_labels % num_clusters

        # ğŸ”¥ ä¿®å¤ï¼šæå‰è½¬æ¢ä¸º long ç±»å‹ï¼Œé¿å…åç»­ç±»å‹ä¸åŒ¹é…
        pseudo_labels = pseudo_labels.long()

        # ğŸ”¥ æ”¹è¿›4ï¼šæ·»åŠ å¾®å°çš„éšæœºæ‰°åŠ¨ï¼Œå¢åŠ è®­ç»ƒåŠ¨æ€æ€§
        # ä»…åœ¨è®­ç»ƒæ—¶æ·»åŠ ï¼ˆä½¿ç”¨0.01çš„æ¦‚ç‡ç¿»è½¬5%çš„æ ‡ç­¾ï¼‰
        if torch.rand(1).item() < 0.01:
            flip_mask = torch.rand(pseudo_labels.shape[0], device=pseudo_labels.device) < 0.05
            if flip_mask.any():
                # éšæœºç¿»è½¬æ ‡ç­¾
                pseudo_labels[flip_mask] = torch.randint(
                    0, num_clusters, (flip_mask.sum().item(),), device=pseudo_labels.device
                )

    return pseudo_labels  # å·²ç»æ˜¯ long ç±»å‹


class AdversarialDecoupler(nn.Module):
    """
    å¯¹æŠ—å¼è§£è€¦æ¨¡å—ï¼ˆæ•´åˆï¼‰
    
    åŒ…å«ï¼š
    1. Attribute Discriminator: å¼ºåˆ¶IDç‰¹å¾ä¸åŒ…å«æœè£…ä¿¡æ¯
    2. Domain Discriminator (å¯é€‰): å¼ºåˆ¶ID/Attrç‰¹å¾æ¥è‡ªä¸åŒåˆ†å¸ƒ
    """
    def __init__(self, dim=768, num_attributes=128, use_domain_disc=False, logger=None):
        super().__init__()
        self.logger = logger
        self.use_domain_disc = use_domain_disc
        
        # å±æ€§åˆ¤åˆ«å™¨
        self.attr_disc = AttributeDiscriminator(
            dim=dim, 
            num_attributes=num_attributes,
            hidden_dims=[512, 256],
            dropout=0.3
        )
        
        # åŸŸåˆ¤åˆ«å™¨ï¼ˆå¯é€‰ï¼‰
        if use_domain_disc:
            self.domain_disc = DomainDiscriminator(dim=dim, hidden_dim=512, dropout=0.3)
        
        # ğŸ”¥ æ¢¯åº¦åè½¬å¼ºåº¦è°ƒåº¦å™¨ï¼ˆæ›´å¹³ç¼“çš„å¢é•¿æ›²çº¿ï¼‰
        # ä»0.0ç¼“æ…¢å¢é•¿åˆ°1.0ï¼Œé¿å…æ—©æœŸå¯¹æŠ—è¿‡å¼º
        # ä½¿ç”¨sigmoidå‡½æ•°ï¼Œåœ¨è®­ç»ƒä¸­æœŸè¾¾åˆ°0.5
        self.lambda_schedule = lambda p: 1.0 / (1.0 + torch.exp(torch.tensor(-5.0 * (p - 0.5))))
    
    def update_lambda(self, progress):
        """
        æ›´æ–°æ¢¯åº¦åè½¬å¼ºåº¦
        
        Args:
            progress: è®­ç»ƒè¿›åº¦ [0, 1]
        """
        lambda_ = self.lambda_schedule(progress).item()
        self.attr_disc.set_lambda(lambda_)
        if self.use_domain_disc:
            self.domain_disc.set_lambda(lambda_)
        
        if self.logger and hasattr(self, '_log_counter'):
            self._log_counter = getattr(self, '_log_counter', 0) + 1
            if self._log_counter % 500 == 0:
                self.logger.debug_logger.debug(f"[Adversarial] Lambda updated: {lambda_:.4f}")
    
    def forward(self, id_feat, cloth_feat, training_phase='feature'):
        """
        Args:
            id_feat: [B, D] IDç‰¹å¾
            cloth_feat: [B, D] æœè£…ç‰¹å¾
            training_phase: 'feature' or 'discriminator'
        
        Returns:
            losses: dict of adversarial losses
        """
        losses = {}
        
        # ç”Ÿæˆæœè£…ä¼ªæ ‡ç­¾
        pseudo_labels = compute_attribute_pseudo_labels(cloth_feat, num_clusters=self.attr_disc.num_attributes)
        
        # 1. å±æ€§åˆ¤åˆ«å™¨æŸå¤±
        if training_phase == 'feature':
            # è®­ç»ƒç‰¹å¾æå–å™¨ï¼šè®©IDç‰¹å¾"æ¬ºéª—"åˆ¤åˆ«å™¨ï¼ˆæ¢¯åº¦åè½¬ï¼‰
            attr_logits = self.attr_disc(id_feat, reverse_grad=True)
            # äº¤å‰ç†µæŸå¤±ï¼ˆä½†æ¢¯åº¦è¢«åè½¬ï¼‰
            loss_attr_adv = F.cross_entropy(attr_logits, pseudo_labels)
            losses['adversarial_attr'] = loss_attr_adv
        else:
            # è®­ç»ƒåˆ¤åˆ«å™¨ï¼šè®©åˆ¤åˆ«å™¨æ­£ç¡®é¢„æµ‹æœè£…å±æ€§ï¼ˆæ— æ¢¯åº¦åè½¬ï¼‰
            attr_logits = self.attr_disc(cloth_feat, reverse_grad=False)
            loss_attr_disc = F.cross_entropy(attr_logits, pseudo_labels)
            losses['discriminator_attr'] = loss_attr_disc
        
        # 2. åŸŸåˆ¤åˆ«å™¨æŸå¤±ï¼ˆå¯é€‰ï¼‰
        if self.use_domain_disc:
            if training_phase == 'feature':
                # è®©åˆ¤åˆ«å™¨æ— æ³•åŒºåˆ†ID/Attrç‰¹å¾
                domain_logits_id = self.domain_disc(id_feat, reverse_grad=True)
                domain_logits_attr = self.domain_disc(cloth_feat, reverse_grad=True)
                
                # ç›®æ ‡ï¼šè®©åˆ¤åˆ«å™¨è¾“å‡ºæ¥è¿‘0.5ï¼ˆæ— æ³•åˆ¤æ–­ï¼‰
                domain_labels = torch.cat([
                    torch.zeros(id_feat.size(0), dtype=torch.long, device=id_feat.device),
                    torch.ones(cloth_feat.size(0), dtype=torch.long, device=cloth_feat.device)
                ])
                domain_logits = torch.cat([domain_logits_id, domain_logits_attr], dim=0)
                loss_domain_adv = F.cross_entropy(domain_logits, domain_labels)
                losses['adversarial_domain'] = loss_domain_adv
            else:
                # è®­ç»ƒåˆ¤åˆ«å™¨ï¼šæ­£ç¡®åŒºåˆ†ID/Attrç‰¹å¾
                domain_logits_id = self.domain_disc(id_feat, reverse_grad=False)
                domain_logits_attr = self.domain_disc(cloth_feat, reverse_grad=False)
                
                domain_labels = torch.cat([
                    torch.zeros(id_feat.size(0), dtype=torch.long, device=id_feat.device),
                    torch.ones(cloth_feat.size(0), dtype=torch.long, device=cloth_feat.device)
                ])
                domain_logits = torch.cat([domain_logits_id, domain_logits_attr], dim=0)
                loss_domain_disc = F.cross_entropy(domain_logits, domain_labels)
                losses['discriminator_domain'] = loss_domain_disc
        
        return losses
