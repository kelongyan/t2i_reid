"""
AH-Net Module (Optimized)
å®ç°ä¸å¯¹ç§°å¼‚æ„ç½‘ç»œçš„æ ¸å¿ƒäº¤äº’é€»è¾‘
- å‡çº§: é™æ€ Query -> åŠ¨æ€å®ä¾‹æ„ŸçŸ¥ Query
- å‡çº§: å•å¤´ Attention -> å¤šå¤´ Attention (8 Heads)
- æ–°å¢: Query æ­£äº¤æ€§æ­£åˆ™åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .ahnet_streams import IDStructureStream, AttributeTextureStream

class DynamicQueryGenerator(nn.Module):
    """
    åŠ¨æ€ Query ç”Ÿæˆå™¨
    å°†ç‰¹å¾å›¾å‹ç¼©å¹¶æ˜ å°„ä¸º Query å‘é‡ï¼Œèµ‹äºˆæ¨¡å‹"å®ä¾‹æ„ŸçŸ¥"èƒ½åŠ›ã€‚
    """
    def __init__(self, dim, hidden_dim=None):
        super().__init__()
        hidden_dim = hidden_dim or dim // 2
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.mlp = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim),
            nn.LayerNorm(dim)
        )
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        """ğŸ”¥ æ›´å®‰å…¨çš„æƒé‡åˆå§‹åŒ–"""
        if isinstance(m, nn.Linear):
            # ä½¿ç”¨æ›´å°çš„æ ‡å‡†å·®ï¼Œé˜²æ­¢NaNæ¢¯åº¦
            nn.init.xavier_normal_(m.weight, gain=0.05)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)

    def forward(self, x):
        """
        Args:
            x: [B, D, H, W]
        Returns:
            query: [B, 1, D]
        """
        B, D, H, W = x.shape
        x_flat = self.pool(x).flatten(1) # [B, D]
        query = self.mlp(x_flat)         # [B, D]
        return query.unsqueeze(1)        # [B, 1, D]


class MultiHeadAttention2D(nn.Module):
    """
    é’ˆå¯¹ 2D ç‰¹å¾å›¾ä¼˜åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    ğŸ”¥ æ”¹è¿›ï¼šæ›´å®‰å…¨çš„æƒé‡åˆå§‹åŒ–
    """
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Conv2d(dim, dim, 1) # Use 1x1 Conv for spatial features
        self.v_proj = nn.Conv2d(dim, dim, 1)
        
        self.out_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)
        
        # ğŸ”¥ åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """ğŸ”¥ æ›´å®‰å…¨çš„æƒé‡åˆå§‹åŒ–"""
        # Q, K, VæŠ•å½±ï¼šä½¿ç”¨æ›´å°çš„æ ‡å‡†å·®
        for m in [self.q_proj, self.out_proj]:
            nn.init.xavier_normal_(m.weight, gain=0.05)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        
        # K, Vçš„1x1å·ç§¯
        for m in [self.k_proj, self.v_proj]:
            nn.init.xavier_normal_(m.weight, gain=0.05)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, query, feature_map):
        """
        Args:
            query: [B, 1, D] (Dynamic Query)
            feature_map: [B, D, H, W] (Key/Value Source)
        Returns:
            context: [B, D]
            attn_map: [B, 1, H, W] (Averaged over heads for visualization)
        """
        B, _, D = query.shape
        _, _, H, W = feature_map.shape
        
        # 1. Projections
        # Q: [B, 1, D] -> [B, 1, Heads, Dim_Head] -> [B, Heads, 1, Dim_Head]
        q = self.q_proj(query).view(B, 1, self.num_heads, -1).permute(0, 2, 1, 3)
        
        # K, V: [B, D, H, W] -> [B, Heads, Dim_Head, H*W]
        k = self.k_proj(feature_map).flatten(2).view(B, self.num_heads, -1, H*W) # [B, H, D_h, N]
        v = self.v_proj(feature_map).flatten(2).view(B, self.num_heads, -1, H*W) # [B, H, D_h, N]
        
        # 2. Attention
        # Scores: Q * K^T -> [B, Heads, 1, N]
        attn = (q @ k) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn) # [B, Heads, 1, N]
        
        # 3. Context
        # Context: Attn * V^T -> [B, Heads, 1, Dim_Head] -> [B, 1, D]
        context = (attn @ v.transpose(-1, -2)).permute(0, 2, 1, 3).reshape(B, 1, D)
        context = self.out_proj(context)
        context = self.norm(context + query) # Residual + Norm
        context = context.squeeze(1) # [B, D]
        
        # 4. Attention Map for Visualization / Loss
        # Reshape [B, Heads, 1, H*W] -> [B, Heads, H, W]
        attn_map_heads = attn.view(B, self.num_heads, H, W)
        
        # Average over heads for downstream "Spatial Conflict" calculation
        # Or keep heads? AH-Net original logic uses simple overlap. 
        # Mean is a safe proxy for "Global Attention Intensity".
        attn_map_avg = attn_map_heads.mean(dim=1, keepdim=True) # [B, 1, H, W]
        
        return context, attn_map_avg


class AHNetModule(nn.Module):
    """
    AH-Net: Asymmetric Heterogeneous Network Module (Extreme Performance Ver.)
    
    æ¶æ„å‡çº§ï¼š
    1. è¾“å…¥å¤„ç†: Seq -> Grid
    2. åŒæµåˆ†æ”¯: ID Stream (Mamba) & Attr Stream (CNN)
    3. äº¤äº’æœºåˆ¶: 
       - Dynamic Query Generation (Instance Aware)
       - Multi-Head Attention (High Capacity)
    4. äº’æ–¥è§£è€¦: Conflict Score + Orthogonality Regularization
    """
    def __init__(self, dim=384, img_size=(384, 128), patch_size=16, 
                 d_state=16, d_conv=4, expand=2, logger=None):
        super().__init__()
        self.dim = dim
        self.logger = logger
        
        # è®¡ç®—ç½‘æ ¼å°ºå¯¸
        self.grid_h = img_size[0] // patch_size
        self.grid_w = img_size[1] // patch_size
        
        if logger:
            logger.debug_logger.info(f"ğŸš€ AH-Net (Extreme): Grid=({self.grid_h}, {self.grid_w}), Dim={dim}, Heads=8")
        
        # === 1. ä¸å¯¹ç§°åŒæµ ===
        self.id_stream = IDStructureStream(
            dim=dim, d_state=d_state, d_conv=d_conv, expand=expand, logger=logger
        )
        self.attr_stream = AttributeTextureStream(
            dim=dim, grid_size=(self.grid_h, self.grid_w), logger=logger
        )
        
        # === 2. åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆå™¨ (Dynamic Query) ===
        self.id_query_gen = DynamicQueryGenerator(dim)
        self.attr_query_gen = DynamicQueryGenerator(dim)
        
        # === 3. å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention) ===
        self.id_attn = MultiHeadAttention2D(dim, num_heads=8)
        self.attr_attn = MultiHeadAttention2D(dim, num_heads=8)
        
        # === 4. ç‰¹å¾è§£ç å™¨ (ç”¨äºé‡æ„ Loss) ===
        self.decoder = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(dim, dim)
        )
        
        # ğŸ”¥ åˆå§‹åŒ–æƒé‡ï¼Œé˜²æ­¢NaNæ¢¯åº¦
        self._init_weights()

    def _compute_conflict_score(self, map_id, map_attr):
        """
        è®¡ç®—å†²çªåˆ†æ•°ã€‚
        è¾“å…¥ä¸ºå·²å¹³å‡çš„å¤šå¤´æ³¨æ„åŠ›å›¾ [B, 1, H, W]
        """
        overlap = map_id * map_attr  # [B, 1, H, W]
        conflict = overlap.sum(dim=(2, 3))  # [B, 1]
        pixel_count = map_id.shape[2] * map_id.shape[3]
        conflict_score = conflict.squeeze(1) / pixel_count  # [B]
        return conflict_score

    def forward(self, x_grid, return_attention=False):
        """
        Args:
            x_grid: [B, D, H, W] è¾“å…¥ç‰¹å¾ç½‘æ ¼
        Returns:
            v_id: [B, D]
            v_attr: [B, D]
            aux_info: dict
        """
        # ğŸ”¥ æ·»åŠ è¾“å…¥éªŒè¯
        assert x_grid.dim() == 4, f"Expected 4D tensor [B, D, H, W], got {x_grid.dim()}D"
        B, D, H, W = x_grid.shape
        assert D == self.dim, f"Expected dim={self.dim}, got {D}"
        
        # === 1. åŒæµå¤„ç† ===
        f_id_map = self.id_stream(x_grid) # [B, D, H/2, W/2]
        f_attr_map = self.attr_stream(x_grid) # [B, D, H, W]
        
        # === 2. åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆ ===
        # æ ¹æ®å„è‡ªæµçš„ç‰¹å¾ç”Ÿæˆ"æƒ³çœ‹ä»€ä¹ˆ"çš„ Query
        q_id = self.id_query_gen(f_id_map)     # [B, 1, D]
        q_attr = self.attr_query_gen(f_attr_map) # [B, 1, D]
        
        # === 3. å¤šå¤´æ³¨æ„åŠ›äº¤äº’ ===
        v_id, map_id = self.id_attn(q_id, f_id_map)
        v_attr, map_attr = self.attr_attn(q_attr, f_attr_map)
        
        # === 4. åå¤„ç†ä¸äº’æ–¥ ===
        # ä¸Šé‡‡æ · ID Map ä½¿å¾—å°ºå¯¸åŒ¹é…
        map_id_up = F.interpolate(map_id, size=(H, W), mode='bilinear', align_corners=False)
        
        # è®¡ç®—ç©ºé—´å†²çªåˆ†æ•°
        conflict_score = self._compute_conflict_score(map_id_up, map_attr)
        
        # è®¡ç®— Query æ­£äº¤æ€§ (ç”¨äº Loss æƒ©ç½š)
        # Cosine Similarity between Q_id and Q_attr
        q_id_norm = F.normalize(q_id.squeeze(1), p=2, dim=1)
        q_attr_norm = F.normalize(q_attr.squeeze(1), p=2, dim=1)
        ortho_reg = (q_id_norm * q_attr_norm).sum(dim=1).abs().mean()

        # === 5. é‡æ„ ===
        # ğŸ”¥ ä¿®å¤ Bug #4: ç§»é™¤v_idçš„detach(),è®©é‡æ„æŸå¤±åŒæ—¶ä¼˜åŒ–IDå’ŒAttråˆ†æ”¯
        recon_input = v_id + v_attr
        recon_feat = self.decoder(recon_input)
        original_global = x_grid.mean(dim=(2, 3))

        # ğŸ”¥ è°ƒè¯•æ—¥å¿—
        if self.logger and hasattr(self, '_log_counter'):
            self._log_counter = getattr(self, '_log_counter', 0) + 1
            if self._log_counter % 200 == 0:
                self.logger.debug_logger.debug(
                    f"[AH-Net Extreme] Conflict: {conflict_score.mean():.4f} | Ortho Reg: {ortho_reg.item():.4f}"
                )

        aux_info = {
            'map_id': map_id_up,
            'map_attr': map_attr,
            'conflict_score': conflict_score,
            'recon_feat': recon_feat,
            'target_feat': original_global,
            'ortho_reg': ortho_reg, # æ–°å¢ï¼šæ­£äº¤æ­£åˆ™é¡¹
            'v_id': v_id,
            'v_attr': v_attr
        }
        
        return v_id, v_attr, aux_info
    
    def _init_weights(self):
        """ğŸ”¥ æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼Œé˜²æ­¢NaNæ¢¯åº¦"""
        # åˆå§‹åŒ–è§£ç å™¨
        for m in self.decoder.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

# Alias
FSHDModule = AHNetModule